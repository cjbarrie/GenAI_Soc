{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs for Simulation II: Experiments\n",
    "\n",
    "**Learning objectives:**\n",
    "- Use LLMs to predict experimental outcomes before data collection\n",
    "- Implement zero-shot experimental forecasting (Hewitt et al. approach)\n",
    "- Understand fine-tuned behavioral models (Centaur approach)\n",
    "- Apply both methods to real experimental designs\n",
    "- Critically evaluate when simulation is appropriate\n",
    "\n",
    "**How to run this notebook:**\n",
    "- **Google Colab** (recommended for Part 2): Fine-tuning requires GPU\n",
    "- **Local/Colab** (Part 1): Zero-shot prediction works anywhere\n",
    "- **API key needed**: OpenAI for Part 1\n",
    "\n",
    "**Key papers:**\n",
    "- **Hewitt, Ashokkumar et al. (2024)**: Predicting Results of Social Science Experiments\n",
    "- **Binz & Schulz et al. (2025)**: Centaur: Foundation Model of Human Cognition\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: Two Approaches to Experimental Simulation\n",
    "\n",
    "This chapter demonstrates two complementary approaches to simulating experimental outcomes:\n",
    "\n",
    "### Approach 1: Zero-shot prediction (Hewitt et al.)\n",
    "- Use a **general-purpose LLM** (no fine-tuning)\n",
    "- Prompt with demographic profiles + experimental conditions\n",
    "- Generate simulated responses to predict treatment effects\n",
    "- **Advantage**: Fast, no training data needed\n",
    "- **Limitation**: Relies on LLM's prior knowledge\n",
    "\n",
    "### Approach 2: Fine-tuned behavioral model (Centaur)\n",
    "- **Fine-tune on real human behavioral data**\n",
    "- Train on trial-by-trial choices across experiments\n",
    "- Model learns general principles of human decision-making\n",
    "- **Advantage**: More accurate predictions within domain\n",
    "- **Limitation**: Requires large behavioral corpus\n",
    "\n",
    "**When to use which:**\n",
    "- **Zero-shot**: Quick forecasts before data collection, exploring many scenarios\n",
    "- **Fine-tuned**: When you have behavioral data, need precision, studying specific paradigms\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Zero-shot Experimental Prediction\n",
    "\n",
    "We'll replicate the **Hewitt et al. (2024)** approach using a real preregistered experiment:\n",
    "\n",
    "**Connors (2020): \"Everyone's Doing It: Affective Polarization is Inflated by Social Pressure\"**\n",
    "\n",
    "**Research question**: Does perceived privacy affect how much affective polarization people report?\n",
    "\n",
    "**Design**:\n",
    "- **N = 3,333** participants\n",
    "- **4 conditions**: Public, Private, Friends, Control\n",
    "- **Outcome**: Affective polarization (feeling thermometers toward out-party)\n",
    "- **Hypothesis**: Privacy increases → reported polarization decreases\n",
    "\n",
    "**Actual results**:\n",
    "- **No main effect** of privacy conditions\n",
    "- Evidence that social pressure inflates polarization measures\n",
    "- Effects depend on individual differences in social conformity\n",
    "\n",
    "Let's see if an LLM can predict these findings!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q openai pandas numpy matplotlib seaborn scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import getpass\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Define the Experimental Setup\n",
    "\n",
    "We need to translate the Connors study into prompts for the LLM:\n",
    "\n",
    "1. **Study context**: Background about the survey\n",
    "2. **Treatment conditions**: Four privacy manipulations\n",
    "3. **Outcome measure**: Feeling thermometer (0-100)\n",
    "4. **Demographic profiles**: Representative U.S. sample\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study context\n",
    "STUDY_CONTEXT = \"\"\"\n",
    "You are participating in a research study about political attitudes in America.\n",
    "You will answer questions about how you feel toward different political groups.\n",
    "\"\"\"\n",
    "\n",
    "# Treatment conditions (exactly as in Connors 2020)\n",
    "CONDITIONS = {\n",
    "    'control': \"\",  # No reminder\n",
    "    \n",
    "    'public': \"\"\"\n",
    "Just a reminder, the results based on your responses may be published.\n",
    "\"\"\",\n",
    "    \n",
    "    'private': \"\"\"\n",
    "Just a reminder, your responses are completely private.\n",
    "\"\"\",\n",
    "    \n",
    "    'friends': \"\"\"\n",
    "Imagine your friends will read your responses to the following questions.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Outcome measure - feeling thermometer\n",
    "# We'll ask about the out-party (opposite of respondent's party)\n",
    "OUTCOME_TEMPLATE = \"\"\"\n",
    "On a scale from 0 to 100, where 0 means very cold/unfavorable and \n",
    "100 means very warm/favorable, how do you feel toward {target_party}?\n",
    "\n",
    "Respond with ONLY a number from 0 to 100.\n",
    "\"\"\"\n",
    "\n",
    "print(\"✓ Experimental setup defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Create Demographic Profiles\n",
    "\n",
    "Following **Hewitt et al. (2024)**, we'll create realistic demographic profiles representing the U.S. population.\n",
    "\n",
    "Key dimensions:\n",
    "- **Political party**: Democrat, Republican, Independent\n",
    "- **Age**: Range of age groups\n",
    "- **Gender**: Male, Female\n",
    "- **Education**: High school, Some college, College degree, Graduate degree\n",
    "- **Region**: Different U.S. regions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demographic_profiles(n_profiles=100):\n",
    "    \"\"\"\n",
    "    Generate representative demographic profiles\n",
    "    \n",
    "    Following typical U.S. population distributions\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    profiles = []\n",
    "    \n",
    "    # Demographic distributions (approximate U.S. proportions)\n",
    "    parties = ['Democrat', 'Republican', 'Independent']\n",
    "    party_probs = [0.35, 0.30, 0.35]\n",
    "    \n",
    "    ages = ['25-34', '35-44', '45-54', '55-64', '65+']\n",
    "    age_probs = [0.20, 0.20, 0.20, 0.20, 0.20]\n",
    "    \n",
    "    genders = ['male', 'female']\n",
    "    gender_probs = [0.49, 0.51]\n",
    "    \n",
    "    educations = ['high school diploma', 'some college', 'college degree', 'graduate degree']\n",
    "    edu_probs = [0.25, 0.30, 0.30, 0.15]\n",
    "    \n",
    "    regions = ['Northeast', 'South', 'Midwest', 'West']\n",
    "    region_probs = [0.17, 0.38, 0.21, 0.24]\n",
    "    \n",
    "    for i in range(n_profiles):\n",
    "        party = np.random.choice(parties, p=party_probs)\n",
    "        age = np.random.choice(ages, p=age_probs)\n",
    "        gender = np.random.choice(genders, p=gender_probs)\n",
    "        education = np.random.choice(educations, p=edu_probs)\n",
    "        region = np.random.choice(regions, p=region_probs)\n",
    "        \n",
    "        # Determine out-party (target for feeling thermometer)\n",
    "        if party == 'Democrat':\n",
    "            out_party = 'Republicans'\n",
    "        elif party == 'Republican':\n",
    "            out_party = 'Democrats'\n",
    "        else:  # Independent\n",
    "            out_party = np.random.choice(['Democrats', 'Republicans'])\n",
    "        \n",
    "        profile = {\n",
    "            'id': i,\n",
    "            'party': party,\n",
    "            'age': age,\n",
    "            'gender': gender,\n",
    "            'education': education,\n",
    "            'region': region,\n",
    "            'out_party': out_party,\n",
    "            'description': f\"a {age}-year-old {gender} {party} from the {region} with a {education}\"\n",
    "        }\n",
    "        \n",
    "        profiles.append(profile)\n",
    "    \n",
    "    return pd.DataFrame(profiles)\n",
    "\n",
    "# Generate profiles\n",
    "profiles_df = create_demographic_profiles(n_profiles=100)\n",
    "\n",
    "print(f\"Generated {len(profiles_df)} demographic profiles\")\n",
    "print(\"\\nExample profiles:\")\n",
    "print(profiles_df[['description', 'out_party']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Simulate Experimental Responses\n",
    "\n",
    "Now we'll use GPT-4 to simulate how each demographic profile would respond in each condition.\n",
    "\n",
    "**Key parameters:**\n",
    "- **temperature = 1.0**: Allows realistic variation in responses\n",
    "- **Multiple runs per profile**: Capture within-person variability\n",
    "- **Extract numeric responses**: Parse feeling thermometer scores\n",
    "\n",
    "**This takes ~5-10 minutes to run all simulations.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_response(demographic_desc, out_party, condition_text, \n",
    "                     model=\"gpt-4o\", temperature=1.0, max_retries=3):\n",
    "    \"\"\"\n",
    "    Simulate a single response from LLM\n",
    "    \n",
    "    Args:\n",
    "        demographic_desc: String describing the respondent\n",
    "        out_party: Target party for feeling thermometer\n",
    "        condition_text: Treatment condition reminder\n",
    "        model: Which OpenAI model to use\n",
    "        temperature: Sampling temperature\n",
    "        max_retries: Number of attempts if parsing fails\n",
    "    \n",
    "    Returns:\n",
    "        int: Feeling thermometer score (0-100), or None if failed\n",
    "    \"\"\"\n",
    "    outcome_question = OUTCOME_TEMPLATE.format(target_party=out_party)\n",
    "    \n",
    "    prompt = f\"\"\"You are {demographic_desc}.\n",
    "\n",
    "{STUDY_CONTEXT}\n",
    "\n",
    "{condition_text}\n",
    "\n",
    "{outcome_question}\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=10\n",
    "            )\n",
    "            \n",
    "            # Extract numeric response\n",
    "            content = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Try to parse as integer\n",
    "            score = int(''.join(filter(str.isdigit, content)))\n",
    "            \n",
    "            # Validate range\n",
    "            if 0 <= score <= 100:\n",
    "                return score\n",
    "        \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"Failed to parse response after {max_retries} attempts: {e}\")\n",
    "                return None\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✓ Simulation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_simulation(profiles_df, conditions, n_responses_per_profile=2):\n",
    "    \"\"\"\n",
    "    Run full experimental simulation across all conditions and profiles\n",
    "    \n",
    "    Args:\n",
    "        profiles_df: DataFrame of demographic profiles\n",
    "        conditions: Dict of condition names and texts\n",
    "        n_responses_per_profile: Number of responses to simulate per profile\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: All simulated responses\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_simulations = len(profiles_df) * len(conditions) * n_responses_per_profile\n",
    "    completed = 0\n",
    "    \n",
    "    print(f\"Starting simulation: {total_simulations} total responses to generate...\")\n",
    "    print(f\"Estimated time: ~{total_simulations * 2 / 60:.1f} minutes\\n\")\n",
    "    \n",
    "    for _, profile in profiles_df.iterrows():\n",
    "        for condition_name, condition_text in conditions.items():\n",
    "            for response_num in range(n_responses_per_profile):\n",
    "                score = simulate_response(\n",
    "                    demographic_desc=profile['description'],\n",
    "                    out_party=profile['out_party'],\n",
    "                    condition_text=condition_text\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'profile_id': profile['id'],\n",
    "                    'party': profile['party'],\n",
    "                    'age': profile['age'],\n",
    "                    'gender': profile['gender'],\n",
    "                    'education': profile['education'],\n",
    "                    'region': profile['region'],\n",
    "                    'out_party': profile['out_party'],\n",
    "                    'condition': condition_name,\n",
    "                    'response_num': response_num,\n",
    "                    'feeling_thermometer': score\n",
    "                })\n",
    "                \n",
    "                completed += 1\n",
    "                if completed % 50 == 0:\n",
    "                    print(f\"Progress: {completed}/{total_simulations} ({100*completed/total_simulations:.1f}%)\")\n",
    "                \n",
    "                # Rate limiting\n",
    "                time.sleep(0.5)\n",
    "    \n",
    "    print(\"\\n✓ Simulation complete!\")\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run the simulation (this takes ~10 minutes with 100 profiles, 4 conditions, 2 responses each)\n",
    "# For faster testing, reduce n_profiles in create_demographic_profiles() above\n",
    "simulated_data = run_experiment_simulation(\n",
    "    profiles_df, \n",
    "    CONDITIONS, \n",
    "    n_responses_per_profile=2\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(simulated_data)} simulated responses\")\n",
    "print(f\"Missing responses: {simulated_data['feeling_thermometer'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Analyze Predicted Treatment Effects\n",
    "\n",
    "Now we'll analyze the LLM's predictions to see if it forecasts the same patterns as the actual study.\n",
    "\n",
    "**Key analyses:**\n",
    "1. **Main effect of privacy**: Do private conditions reduce polarization?\n",
    "2. **Comparison across conditions**: Which shows lowest polarization?\n",
    "3. **Effect sizes**: How large are the predicted differences?\n",
    "\n",
    "**Actual Connors (2020) findings:**\n",
    "- No simple main effect of privacy conditions\n",
    "- Effects moderated by individual differences\n",
    "- Friends and private conditions showed some effects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing responses\n",
    "clean_data = simulated_data.dropna(subset=['feeling_thermometer'])\n",
    "\n",
    "# Compute means by condition\n",
    "condition_means = clean_data.groupby('condition')['feeling_thermometer'].agg(['mean', 'std', 'count'])\n",
    "condition_means = condition_means.round(2)\n",
    "\n",
    "print(\"Predicted Feeling Thermometer Scores by Condition\")\n",
    "print(\"=\"*60)\n",
    "print(condition_means)\n",
    "print(\"\\nNote: Lower scores = more polarization (colder toward out-party)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predicted effects\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Create violin plot\n",
    "sns.violinplot(data=clean_data, x='condition', y='feeling_thermometer', \n",
    "               order=['control', 'public', 'private', 'friends'],\n",
    "               palette='Set2', ax=ax)\n",
    "\n",
    "# Add mean markers\n",
    "means = clean_data.groupby('condition')['feeling_thermometer'].mean()\n",
    "for i, condition in enumerate(['control', 'public', 'private', 'friends']):\n",
    "    ax.plot(i, means[condition], 'D', color='red', markersize=10, \n",
    "            label='Mean' if i == 0 else '')\n",
    "\n",
    "ax.set_xlabel('Condition', fontsize=12)\n",
    "ax.set_ylabel('Feeling Thermometer (0-100)', fontsize=12)\n",
    "ax.set_title('LLM-Predicted Treatment Effects on Affective Polarization', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests: Compare each treatment to control\n",
    "control_scores = clean_data[clean_data['condition'] == 'control']['feeling_thermometer']\n",
    "\n",
    "print(\"Statistical Tests: Treatment vs. Control\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for condition in ['public', 'private', 'friends']:\n",
    "    treatment_scores = clean_data[clean_data['condition'] == condition]['feeling_thermometer']\n",
    "    \n",
    "    # t-test\n",
    "    t_stat, p_value = stats.ttest_ind(treatment_scores, control_scores)\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((treatment_scores.std()**2 + control_scores.std()**2) / 2)\n",
    "    cohens_d = (treatment_scores.mean() - control_scores.mean()) / pooled_std\n",
    "    \n",
    "    print(f\"\\n{condition.upper()} vs. Control:\")\n",
    "    print(f\"  Mean difference: {treatment_scores.mean() - control_scores.mean():.2f} points\")\n",
    "    print(f\"  t-statistic: {t_stat:.3f}\")\n",
    "    print(f\"  p-value: {p_value:.4f} {'*' if p_value < 0.05 else '(n.s.)'}\")\n",
    "    print(f\"  Cohen's d: {cohens_d:.3f}\")\n",
    "\n",
    "print(\"\\n* = statistically significant at p < 0.05\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Compare to Actual Results\n",
    "\n",
    "**Actual Connors (2020) findings:**\n",
    "- **No main treatment effects** for any condition\n",
    "- Effects varied based on individual differences in social conformity\n",
    "- Private and friends conditions showed some reduction in polarization\n",
    "- **Conclusion**: Social pressure inflates polarization measures\n",
    "\n",
    "**LLM predictions:**\n",
    "- See results above\n",
    "- Compare effect directions and magnitudes\n",
    "\n",
    "**Questions to consider:**\n",
    "1. Did the LLM predict the right **direction** of effects?\n",
    "2. Are the **magnitudes** realistic?\n",
    "3. Did it capture the **null main effect**?\n",
    "4. What does this tell us about using LLMs for experimental forecasting?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflections on Zero-shot Prediction\n",
    "\n",
    "**What we learned:**\n",
    "1. LLMs can generate plausible experimental predictions\n",
    "2. Easy to test many demographic subgroups\n",
    "3. Fast and cheap compared to real data collection\n",
    "\n",
    "**Limitations:**\n",
    "1. May miss subtle moderators and individual differences\n",
    "2. Effect sizes may be off\n",
    "3. Can't discover truly novel phenomena\n",
    "4. Relies on LLM's training data biases\n",
    "\n",
    "**Best use cases:**\n",
    "- **Pre-testing** experimental designs before expensive fieldwork\n",
    "- **Power analysis**: Estimate sample sizes needed\n",
    "- **Exploring heterogeneity**: Which subgroups to focus on?\n",
    "- **Theory development**: Generate hypotheses to test\n",
    "\n",
    "**Always validate with real humans!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Fine-tuned Behavioral Models (Mini Centaur)\n",
    "\n",
    "Now we'll explore the **Centaur approach**: fine-tuning an LLM on real behavioral data.\n",
    "\n",
    "**Key difference from Part 1:**\n",
    "- Part 1 (zero-shot): LLM uses its general knowledge\n",
    "- Part 2 (fine-tuned): LLM learns from actual human choices\n",
    "\n",
    "**Example: Asch Conformity Experiment**\n",
    "\n",
    "Classic social psychology study (1951):\n",
    "- Participants judge which line matches a target line\n",
    "- **Confederates** (actors) give obviously wrong answers\n",
    "- **Question**: Will participants conform to group pressure?\n",
    "- **Result**: ~75% of people conformed at least once\n",
    "\n",
    "We'll use a **mini Centaur model** fine-tuned on behavioral data to predict conformity!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for Mini Centaur\n",
    "\n",
    "**Important**: This section requires GPU and specific libraries.\n",
    "\n",
    "**Recommended**: Run this in Google Colab with GPU enabled.\n",
    "\n",
    "The model uses:\n",
    "- **Llama 3.1 8B** base model\n",
    "- **QLoRA fine-tuning** (efficient parameter updates)\n",
    "- Pre-trained on behavioral experiments (\"Minitaur\")\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (only in Colab)\n",
    "# This may take a few minutes\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q unsloth transformers accelerate bitsandbytes\n",
    "    print(\"✓ Installed packages for Colab\")\n",
    "else:\n",
    "    print(\"⚠ Not in Colab - you may need to install packages manually\")\n",
    "    print(\"  Run: pip install unsloth transformers accelerate bitsandbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for fine-tuned model\n",
    "try:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import transformers\n",
    "    import torch\n",
    "    \n",
    "    print(\"✓ Successfully imported model libraries\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"\\n⚠ Warning: No GPU detected. Model loading will be slow.\")\n",
    "        print(\"  In Colab: Runtime → Change runtime type → GPU\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing libraries: {e}\")\n",
    "    print(\"\\nPlease install required packages or run in Colab with GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load the Mini Centaur Model\n",
    "\n",
    "We'll load a pre-trained model that has been fine-tuned on behavioral experiments.\n",
    "\n",
    "**Model details:**\n",
    "- Base: Llama 3.1 8B\n",
    "- Fine-tuned using QLoRA on behavioral data\n",
    "- Trained on trial-by-trial human choices\n",
    "- Can predict responses in multiple experimental paradigms\n",
    "\n",
    "**This takes ~2-3 minutes to download and load.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mini Centaur model\n",
    "print(\"Loading mini Centaur model...\")\n",
    "print(\"This may take 2-3 minutes on first run.\\n\")\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"marcelbinz/Llama-3.1-Minitaur-8B-adapter\",\n",
    "        max_seq_length=32768,  # Long context for full trial histories\n",
    "        dtype=None,  # Auto-detect optimal dtype\n",
    "        load_in_4bit=True,  # Use 4-bit quantization to save memory\n",
    "    )\n",
    "    \n",
    "    # Set model to inference mode\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    print(f\"  Model: {model.config._name_or_path}\")\n",
    "    print(f\"  Max sequence length: {model.config.max_position_embeddings}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  • Make sure you're using a GPU runtime in Colab\")\n",
    "    print(\"  • Check your internet connection (model downloads from HuggingFace)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference pipeline\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    trust_remote_code=True,\n",
    "    pad_token_id=0,\n",
    "    do_sample=True,  # Sample to get variation\n",
    "    temperature=1.0,  # Standard temperature for behavioral simulation\n",
    "    max_new_tokens=1,  # Only generate next choice token\n",
    ")\n",
    "\n",
    "print(\"✓ Inference pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Asch Conformity Experiment\n",
    "\n",
    "**Classic study** (Asch, 1951):\n",
    "\n",
    "1. Participants see a **target line** and three **comparison lines** (A, B, C)\n",
    "2. Task: Choose which comparison line matches the target length\n",
    "3. **Twist**: 6-7 confederates give wrong answers before the real participant\n",
    "4. **Question**: Will the participant give the correct answer or conform?\n",
    "\n",
    "**Example trial:**\n",
    "```\n",
    "Target: medium length\n",
    "A: medium (CORRECT)\n",
    "B: long (WRONG)\n",
    "C: short (WRONG)\n",
    "\n",
    "Confederates all say: \"B\"\n",
    "What does participant say?\n",
    "```\n",
    "\n",
    "**Historical results:**\n",
    "- ~75% of participants conformed at least once\n",
    "- ~32% of all trials showed conformity\n",
    "- Stronger conformity with more confederates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format of Behavioral Data for Centaur\n",
    "\n",
    "Centaur is trained on prompts that include:\n",
    "\n",
    "1. **Task instructions** (what the experiment is about)\n",
    "2. **Full trial history** (all previous choices by this participant)\n",
    "3. **Current trial** (the situation now)\n",
    "4. **Question** (what should they choose next?)\n",
    "\n",
    "**Key insight**: The model learns patterns from how humans respond across many trials, not just isolated choices.\n",
    "\n",
    "Let's see an example:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Asch conformity prompt (following Centaur format)\n",
    "asch_prompt = \"\"\"\n",
    "You will see a target line and three comparison lines labeled A, B, and C.\n",
    "In each trial, press the key for the line that matches the target in length.\n",
    "\n",
    "Target: medium. A: medium, B: long, C: short. Others say: B B B B B B. You press <<A>>.\n",
    "Target: short. A: long, B: short, C: medium. Others say: B B B B B B. You press <<B>>.\n",
    "Target: long. A: long, B: short, C: medium. Others say: C C C C C C. You press <<A>>.\n",
    "Target: medium. A: medium, B: long, C: short. Others say: B B B B B B. You press <<B>>.\n",
    "Target: short. A: short, B: medium, C: long. Others say: C C C C C C. You press <<\n",
    "\"\"\".strip()\n",
    "\n",
    "print(\"Example Prompt for Centaur:\")\n",
    "print(\"=\"*70)\n",
    "print(asch_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generate Predictions with Mini Centaur\n",
    "\n",
    "Now let's see what the model predicts:\n",
    "\n",
    "**The model will:**\n",
    "1. Read the full trial history\n",
    "2. Notice the pattern of conformity/resistance\n",
    "3. Consider the social pressure (\"Others say: C C C C C C\")\n",
    "4. Generate the next choice token (A, B, or C)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction\n",
    "print(\"Generating model prediction...\\n\")\n",
    "\n",
    "output = pipe(asch_prompt)[0]['generated_text']\n",
    "predicted_choice = output[len(asch_prompt):].strip()\n",
    "\n",
    "print(f\"Predicted choice: {predicted_choice}\")\n",
    "\n",
    "# Parse the choice\n",
    "if 'A' in predicted_choice:\n",
    "    choice_letter = 'A'\n",
    "    interpretation = \"INDEPENDENT (correct answer)\"\n",
    "elif 'C' in predicted_choice:\n",
    "    choice_letter = 'C'\n",
    "    interpretation = \"CONFORMITY (followed group)\"\n",
    "else:\n",
    "    choice_letter = predicted_choice\n",
    "    interpretation = \"UNCLEAR\"\n",
    "\n",
    "print(f\"Interpretation: {interpretation}\")\n",
    "print(f\"Correct: A | Group: C | Model: {choice_letter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Multiple Trials\n",
    "\n",
    "Let's simulate a full participant's session:\n",
    "- Multiple trials with varying conformity pressure\n",
    "- Track conformity rate over time\n",
    "- Compare to human behavioral patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_asch_trial(trial_history, target, correct_option, options, group_says):\n",
    "    \"\"\"\n",
    "    Run a single Asch conformity trial\n",
    "    \n",
    "    Args:\n",
    "        trial_history: String of previous trials\n",
    "        target: Description of target line\n",
    "        correct_option: Which option (A/B/C) is correct\n",
    "        options: Dict mapping A/B/C to descriptions\n",
    "        group_says: What option the confederates choose\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (chosen_option, conformed)\n",
    "    \"\"\"\n",
    "    # Build trial prompt\n",
    "    trial_text = f\"Target: {target}. A: {options['A']}, B: {options['B']}, C: {options['C']}. Others say: {' '.join([group_says]*6)}. You press <<\"\n",
    "    \n",
    "    # Full prompt with history\n",
    "    full_prompt = trial_history + \"\\n\" + trial_text if trial_history else trial_text\n",
    "    \n",
    "    # Generate prediction\n",
    "    output = pipe(full_prompt)[0]['generated_text']\n",
    "    prediction = output[len(full_prompt):].strip()\n",
    "    \n",
    "    # Parse choice\n",
    "    if 'A' in prediction:\n",
    "        choice = 'A'\n",
    "    elif 'B' in prediction:\n",
    "        choice = 'B'\n",
    "    elif 'C' in prediction:\n",
    "        choice = 'C'\n",
    "    else:\n",
    "        choice = 'INVALID'\n",
    "    \n",
    "    # Check if conformed\n",
    "    conformed = (choice == group_says and choice != correct_option)\n",
    "    \n",
    "    # Add to history\n",
    "    response_text = trial_text + choice + \">>.\"  \n",
    "    \n",
    "    return choice, conformed, response_text\n",
    "\n",
    "print(\"✓ Trial simulation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a simulated participant session\n",
    "instructions = \"\"\"You will see a target line and three comparison lines labeled A, B, and C.\n",
    "In each trial, press the key for the line that matches the target in length.\n",
    "\"\"\"\n",
    "\n",
    "# Define trial sequence (mix of conformity pressure and control trials)\n",
    "trials = [\n",
    "    # Trial 1: Control (group gives correct answer)\n",
    "    {'target': 'medium', 'correct': 'A', 'options': {'A': 'medium', 'B': 'long', 'C': 'short'}, 'group': 'A'},\n",
    "    \n",
    "    # Trial 2: Conformity pressure (group gives wrong answer)\n",
    "    {'target': 'short', 'correct': 'B', 'options': {'A': 'long', 'B': 'short', 'C': 'medium'}, 'group': 'C'},\n",
    "    \n",
    "    # Trial 3: Conformity pressure\n",
    "    {'target': 'long', 'correct': 'A', 'options': {'A': 'long', 'B': 'short', 'C': 'medium'}, 'group': 'C'},\n",
    "    \n",
    "    # Trial 4: Control\n",
    "    {'target': 'short', 'correct': 'C', 'options': {'A': 'medium', 'B': 'long', 'C': 'short'}, 'group': 'C'},\n",
    "    \n",
    "    # Trial 5: Strong conformity pressure\n",
    "    {'target': 'medium', 'correct': 'A', 'options': {'A': 'medium', 'B': 'long', 'C': 'short'}, 'group': 'B'},\n",
    "    \n",
    "    # Trial 6: Conformity pressure\n",
    "    {'target': 'long', 'correct': 'B', 'options': {'A': 'short', 'B': 'long', 'C': 'medium'}, 'group': 'A'},\n",
    "]\n",
    "\n",
    "# Run simulation\n",
    "print(\"Running Asch Conformity Simulation\")\n",
    "print(\"=\"*70)\n",
    "print()\n",
    "\n",
    "history = instructions\n",
    "results = []\n",
    "\n",
    "for i, trial in enumerate(trials, 1):\n",
    "    choice, conformed, trial_text = run_asch_trial(\n",
    "        history,\n",
    "        trial['target'],\n",
    "        trial['correct'],\n",
    "        trial['options'],\n",
    "        trial['group']\n",
    "    )\n",
    "    \n",
    "    # Record result\n",
    "    is_pressure_trial = (trial['group'] != trial['correct'])\n",
    "    results.append({\n",
    "        'trial': i,\n",
    "        'correct': trial['correct'],\n",
    "        'group_says': trial['group'],\n",
    "        'model_chose': choice,\n",
    "        'conformed': conformed,\n",
    "        'pressure_trial': is_pressure_trial\n",
    "    })\n",
    "    \n",
    "    # Display\n",
    "    pressure_str = \"[PRESSURE]\" if is_pressure_trial else \"[CONTROL]\"\n",
    "    conform_str = \"→ CONFORMED\" if conformed else \"→ independent\"\n",
    "    \n",
    "    print(f\"Trial {i} {pressure_str}:\")\n",
    "    print(f\"  Correct: {trial['correct']} | Group: {trial['group']} | Model: {choice} {conform_str}\")\n",
    "    print()\n",
    "    \n",
    "    # Update history\n",
    "    history += \"\\n\" + trial_text\n",
    "    \n",
    "    time.sleep(0.5)  # Slight delay between trials\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(results)\n",
    "pressure_trials = results_df[results_df['pressure_trial']]\n",
    "conformity_rate = pressure_trials['conformed'].mean()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Total trials: {len(results_df)}\")\n",
    "print(f\"  Conformity pressure trials: {len(pressure_trials)}\")\n",
    "print(f\"  Conformity rate: {conformity_rate*100:.1f}%\")\n",
    "print(f\"\\n  Historical human conformity rate: ~32%\")\n",
    "print(f\"  Model's conformity rate: {conformity_rate*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparing Approaches: Zero-shot vs. Fine-tuned\n",
    "\n",
    "We've now seen both approaches:\n",
    "\n",
    "### Part 1: Zero-shot (Hewitt et al.)\n",
    "- **Input**: Demographic description + experimental setup\n",
    "- **Model**: General-purpose GPT-4\n",
    "- **Output**: Predicted survey response\n",
    "- **Strength**: Fast, flexible, no training needed\n",
    "- **Weakness**: May miss subtle psychological mechanisms\n",
    "\n",
    "### Part 2: Fine-tuned (Centaur)\n",
    "- **Input**: Full trial history in task-specific format\n",
    "- **Model**: Llama 3.1 fine-tuned on behavioral data\n",
    "- **Output**: Next choice given history\n",
    "- **Strength**: Learns from real human behavior patterns\n",
    "- **Weakness**: Requires behavioral corpus, domain-specific\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison\n",
    "comparison_data = pd.DataFrame([\n",
    "    {'Approach': 'Zero-shot (GPT-4)', 'Speed': 'Fast', 'Flexibility': 'High', 'Accuracy': 'Medium', 'Training': 'None'},\n",
    "    {'Approach': 'Fine-tuned (Centaur)', 'Speed': 'Medium', 'Flexibility': 'Low', 'Accuracy': 'High', 'Training': 'Required'}\n",
    "])\n",
    "\n",
    "print(\"Comparison of Approaches\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_data.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Best Practices\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **LLMs can predict experimental outcomes** with varying accuracy\n",
    "2. **Two approaches**: Zero-shot (fast, flexible) vs. Fine-tuned (precise, domain-specific)\n",
    "3. **Best use**: Pre-testing designs, exploring heterogeneity, power analysis\n",
    "4. **Always validate** with real human data before publishing\n",
    "\n",
    "**Best practices:**\n",
    "\n",
    "1. **Use simulation for exploration**, not substitution\n",
    "2. **Document everything**: Prompts, models, parameters, seeds\n",
    "3. **Test sensitivity**: Try multiple prompts and models\n",
    "4. **Compare to theory**: Do predictions make psychological sense?\n",
    "5. **Report limitations**: Be transparent about simulation boundaries\n",
    "\n",
    "**Ethical considerations:**\n",
    "\n",
    "- Don't claim simulated results are real human data\n",
    "- Clearly label predictions vs. actual findings\n",
    "- Be cautious with novel phenomena (LLMs may be wrong)\n",
    "- Consider training data biases (WEIRD populations)\n",
    "- Always get real human data for final claims\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "**Try this yourself:**\n",
    "\n",
    "1. **Modify the Connors study**:\n",
    "   - Add different privacy conditions\n",
    "   - Test other demographic moderators\n",
    "   - Try different outcome measures\n",
    "\n",
    "2. **Explore other experiments**:\n",
    "   - Find preregistered studies at [tessexperiments.org](https://tessexperiments.org)\n",
    "   - Simulate before reading results\n",
    "   - Compare LLM predictions to actual findings\n",
    "\n",
    "3. **Fine-tune your own model**:\n",
    "   - Collect trial-by-trial behavioral data\n",
    "   - Format as Centaur-style prompts\n",
    "   - Use QLoRA to fine-tune efficiently\n",
    "\n",
    "**Resources:**\n",
    "\n",
    "- **Centaur model**: [marcelbinz/Llama-3.1-Minitaur-8B-adapter](https://huggingface.co/marcelbinz/Llama-3.1-Minitaur-8B-adapter)\n",
    "- **Interactive demo**: [https://huggingface.co/spaces/marcelbinz/Centaur](https://huggingface.co/spaces/marcelbinz/Centaur)\n",
    "- **TESS experiments**: [https://tessexperiments.org](https://tessexperiments.org)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>LLMs for Annotation I &#8212; GenAI for Sociology</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css?v=6644e6bb" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'week04_llms';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Qualitative Coding with LLMs" href="week05_qualitative.html" />
    <link rel="prev" title="GenAI for Sociology" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">GenAI for Sociology</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    GenAI for Sociology
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">LLMs for Annotation I</a></li>
<li class="toctree-l1"><a class="reference internal" href="week05_qualitative.html">LLMs for Annotation II</a></li>
<li class="toctree-l1"><a class="reference internal" href="week06_annotation.html">LLMs for Annotation III</a></li>
<li class="toctree-l1"><a class="reference internal" href="week07_silicon_sampling.html">LLMs for Synthetic Data I: Simulating Survey Respondents</a></li>
<li class="toctree-l1"><a class="reference internal" href="week08_interactive_experiments.html">LLMs for Synthetic Data II: Interactive Experiments and Persuasion</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/cjbarrie/GenAI_Soc/blob/main/genai_book/week04_llms.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/cjbarrie/GenAI_Soc" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/cjbarrie/GenAI_Soc/issues/new?title=Issue%20on%20page%20%2Fweek04_llms.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/week04_llms.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>LLMs for Annotation I</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-text-generation-with-gpt-2">Part 1: Text Generation with GPT-2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-gpt-2-model">Load GPT-2 model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-next-token-prediction">Understanding next-token prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-longer-text">Generate longer text</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-temperature">Effect of temperature</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-detecting-bias-in-language-models">Part 2: Detecting Bias in Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#occupation-bias-test">Occupation bias test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#race-and-occupation-bias">Race and occupation bias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-full-continuations">Generate full continuations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-using-openai-api">Part 3: Using OpenAI API</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-openai-api">Setup OpenAI API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-generation">Simple text generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-annotation">Simple text annotation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#annotating-multiple-texts">Annotating multiple texts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-api-vs-open-source-model">Compare API vs open-source model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4-testing-for-bias-in-api-models">Part 4: Testing for Bias in API Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-5-open-source-alternatives-to-openai-api">Part 5: Open-Source Alternatives to OpenAI API</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-using-ollama">Option 1: Using Ollama</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-with-ollama">Text Generation with Ollama</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-annotation-with-ollama">Text Annotation with Ollama</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#json-mode-with-ollama">JSON Mode with Ollama</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-using-hugging-face-transformers">Option 2: Using Hugging Face Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-with-hugging-face">Text Generation with Hugging Face</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-annotation-with-hugging-face">Text Annotation with Hugging Face</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-running-locations">Notes on Running Locations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="llms-for-annotation-i">
<h1>LLMs for Annotation I<a class="headerlink" href="#llms-for-annotation-i" title="Link to this heading">#</a></h1>
<p><strong>Learning objectives:</strong></p>
<ul class="simple">
<li><p>Understand how generative LLMs work (GPT-style models)</p></li>
<li><p>Generate text with GPT-2 and control outputs with temperature/sampling</p></li>
<li><p>Use OpenAI API for generation and simple annotation</p></li>
<li><p>Detect and analyze bias in model outputs</p></li>
<li><p>Compare open-source vs. API models</p></li>
</ul>
<p><strong>How to run this notebook:</strong></p>
<ul class="simple">
<li><p><strong>Google Colab</strong> (recommended): Works for all parts, GPU helpful for GPT-2</p></li>
<li><p><strong>OpenAI API key needed</strong>: For Part 3 (API examples)</p></li>
<li><p><strong>Local</strong>: Works, but GPT-2 generation slow on CPU</p></li>
</ul>
<hr class="docutils" />
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install packages</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-q<span class="w"> </span>transformers<span class="w"> </span>torch<span class="w"> </span>openai

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>

<span class="c1"># Check device</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What this code does:</strong></p>
<p>This setup cell installs and imports the necessary libraries:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">transformers</span></code>: Hugging Face library for loading pre-trained models (GPT-2)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code>: PyTorch deep learning framework (required for running models)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">openai</span></code>: Official OpenAI Python client for API access</p></li>
</ul>
<p><strong>Device selection:</strong> The code automatically detects if you have a GPU available (CUDA). GPUs are much faster for running large models like GPT-2. If no GPU is available, it falls back to CPU (slower but functional).</p>
<p><strong>How to use:</strong></p>
<ul class="simple">
<li><p>In Google Colab: Select Runtime → Change runtime type → Hardware accelerator → GPU (T4)</p></li>
<li><p>Locally: Works on CPU, but generation will be slower</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="part-1-text-generation-with-gpt-2">
<h2>Part 1: Text Generation with GPT-2<a class="headerlink" href="#part-1-text-generation-with-gpt-2" title="Link to this heading">#</a></h2>
<p>GPT-2 is a <strong>causal language model</strong> - it predicts the next word given previous words.</p>
<p><strong>How it works:</strong></p>
<ol class="arabic simple">
<li><p>Takes a prompt (starting text)</p></li>
<li><p>Predicts next token probabilities</p></li>
<li><p>Samples a token</p></li>
<li><p>Repeats until done</p></li>
</ol>
<section id="load-gpt-2-model">
<h3>Load GPT-2 model<a class="headerlink" href="#load-gpt-2-model" title="Link to this heading">#</a></h3>
<p><strong>What this code does:</strong></p>
<p>This loads the GPT-2 model from Hugging Face’s model hub:</p>
<ul class="simple">
<li><p><strong>Model choice:</strong> <code class="docutils literal notranslate"><span class="pre">gpt2-medium</span></code> has 355M parameters - a good balance between quality and speed. Other options:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt2</span></code> (124M) - faster but lower quality</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt2-large</span></code> (774M) - better quality but slower</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt2-xl</span></code> (1.5B) - best quality but very slow without GPU</p></li>
</ul>
</li>
<li><p><strong>Tokenizer:</strong> Converts text to numbers (tokens) that the model understands</p></li>
<li><p><strong>Model:</strong> The neural network that generates text</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">.to(device)</span></code>:</strong> Moves the model to GPU if available</p></li>
</ul>
<p><strong>Expected output:</strong> You’ll see a progress bar as the model downloads (~1.4GB for gpt2-medium). Subsequent runs will use the cached version.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use GPT-2 medium (355M parameters, good balance of quality/speed)</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;gpt2-medium&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Loaded </span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Parameters: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What this code does:</strong></p>
<p>This demonstrates the core mechanism of how LLMs work - predicting the next token:</p>
<ol class="arabic simple">
<li><p><strong>Tokenization:</strong> Convert the prompt into token IDs</p></li>
<li><p><strong>Forward pass:</strong> Run the model to get logits (raw prediction scores) for every possible next token</p></li>
<li><p><strong>Select last token:</strong> <code class="docutils literal notranslate"><span class="pre">logits[:,</span> <span class="pre">-1,</span> <span class="pre">:]</span></code> gets the predictions for what comes after the prompt</p></li>
<li><p><strong>Greedy decoding:</strong> <code class="docutils literal notranslate"><span class="pre">argmax</span></code> picks the single most likely token</p></li>
</ol>
<p><strong>Key insight:</strong> The model outputs a probability distribution over its entire vocabulary (~50k tokens). We can use this to see alternative possibilities, not just the top choice.</p>
<p><strong>Expected output:</strong> For “The capital of France is”, the model will predict “ Paris” with high confidence.</p>
<p><strong>What this code does:</strong></p>
<p>Instead of just the single most likely token, this shows the top 5 alternatives with their probabilities:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.softmax</span></code>:</strong> Converts raw logits into proper probabilities (0-1, summing to 1)</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">torch.topk</span></code>:</strong> Gets the k highest values and their indices</p></li>
<li><p><strong>Decoding:</strong> Converts token IDs back to readable text</p></li>
</ul>
<p><strong>Why this matters:</strong> Understanding the probability distribution helps us see model confidence and alternative possibilities. A model that assigns 0.95 probability to one token is very confident; a model with 0.25 spread across several tokens is uncertain.</p>
<p><strong>Expected output:</strong> You’ll see “ Paris” with highest probability, followed by other French cities or alternative phrasings.</p>
</section>
<section id="understanding-next-token-prediction">
<h3>Understanding next-token prediction<a class="headerlink" href="#understanding-next-token-prediction" title="Link to this heading">#</a></h3>
<p><strong>What this function does:</strong></p>
<p>This is our main text generation function with important controls:</p>
<p><strong>Parameters explained:</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">max_new_tokens</span></code>:</strong> How many tokens to generate (roughly 1 token = 0.75 words)</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">temperature</span></code>:</strong> Controls randomness</p>
<ul>
<li><p>0.1-0.5: Very focused, deterministic (good for factual tasks)</p></li>
<li><p>0.7-1.0: Balanced (good default)</p></li>
<li><p>1.5+: Very random, creative, sometimes incoherent</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">top_p</span></code> (nucleus sampling):</strong> Only sample from top X% of probability mass</p>
<ul>
<li><p>0.9 = ignore the bottom 10% least likely tokens</p></li>
<li><p>Helps avoid nonsensical rare tokens</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">seed</span></code>:</strong> For reproducibility - same seed = same output</p></li>
</ul>
<p><strong>Key steps:</strong></p>
<ol class="arabic simple">
<li><p>Set random seed for reproducibility</p></li>
<li><p>Tokenize the prompt</p></li>
<li><p>Call <code class="docutils literal notranslate"><span class="pre">model.generate()</span></code> with sampling parameters</p></li>
<li><p>Extract only the newly generated tokens (skip the prompt)</p></li>
<li><p>Decode back to text</p></li>
</ol>
<p><strong>How to use:</strong> Call with different prompts and adjust temperature to control creativity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simple example: predict one next token</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;The capital of France is&quot;</span>

<span class="c1"># Tokenize</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Get predictions</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>  <span class="c1"># [batch, sequence_length, vocab_size]</span>
    
    <span class="c1"># Get last token&#39;s predictions (what comes next)</span>
    <span class="n">next_token_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    
    <span class="c1"># Greedy decoding: pick most likely token</span>
    <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">next_token_logits</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">next_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">next_token_id</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Most likely next token: &#39;</span><span class="si">{</span><span class="n">next_token</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Look at top-5 most likely next tokens</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">next_token_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">top_probs</span><span class="p">,</span> <span class="n">top_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Top 5 predictions for: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">top_probs</span><span class="p">,</span> <span class="n">top_indices</span><span class="p">):</span>
    <span class="n">token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  &#39;</span><span class="si">{</span><span class="n">token</span><span class="si">}</span><span class="s2">&#39; - </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="generate-longer-text">
<h3>Generate longer text<a class="headerlink" href="#generate-longer-text" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">top_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate text from a prompt</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        prompt: Starting text</span>
<span class="sd">        max_new_tokens: How many tokens to generate</span>
<span class="sd">        temperature: Randomness (lower = more deterministic)</span>
<span class="sd">        top_p: Nucleus sampling (0.9 = use top 90% probability mass)</span>
<span class="sd">        seed: Random seed for reproducibility</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>          <span class="c1"># use sampling (not greedy)</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="n">top_p</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>
    
    <span class="c1"># Decode and return only new tokens (skip prompt)</span>
    <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:]</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">generated_text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Test</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Write a two-sentence sci-fi plot:&quot;</span>
<span class="n">generated</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated: </span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What this code does:</strong></p>
<p>This demonstrates systematic bias testing by comparing model predictions for gendered prompts:</p>
<p><strong>The function <code class="docutils literal notranslate"><span class="pre">get_top_next_tokens</span></code>:</strong></p>
<ul class="simple">
<li><p>Takes a prompt and returns the k most likely next tokens</p></li>
<li><p>Applies temperature if specified</p></li>
<li><p>Returns both tokens and their probabilities</p></li>
</ul>
<p><strong>Why this matters for bias detection:</strong></p>
<ul class="simple">
<li><p>If “The man worked as a” → mostly predicts high-status occupations</p></li>
<li><p>But “The woman worked as a” → predicts lower-status or stereotypical occupations</p></li>
<li><p>This reveals gender bias learned from training data</p></li>
</ul>
<p><strong>Expected observation:</strong> You’ll likely see stereotypical patterns like:</p>
<ul class="simple">
<li><p>“man” → lawyer, engineer, doctor, CEO</p></li>
<li><p>“woman” → teacher, nurse, secretary, assistant</p></li>
</ul>
<p><strong>Sociological significance:</strong> These biases reflect and may perpetuate real-world inequality. Always test models for demographic biases before using them for annotation.</p>
</section>
<section id="effect-of-temperature">
<h3>Effect of temperature<a class="headerlink" href="#effect-of-temperature" title="Link to this heading">#</a></h3>
<p><strong>Temperature</strong> controls randomness:</p>
<ul class="simple">
<li><p>Low (0.1-0.5): More focused, repetitive</p></li>
<li><p>Medium (0.7-1.0): Balanced</p></li>
<li><p>High (1.5+): More random, creative, incoherent</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Sociology is the study of&quot;</span>
<span class="n">temperatures</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="k">for</span> <span class="n">temp</span> <span class="ow">in</span> <span class="n">temperatures</span><span class="p">:</span>
    <span class="n">generated</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="n">temp</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Temperature = </span><span class="si">{</span><span class="n">temp</span><span class="si">}</span><span class="s2">:&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="part-2-detecting-bias-in-language-models">
<h2>Part 2: Detecting Bias in Language Models<a class="headerlink" href="#part-2-detecting-bias-in-language-models" title="Link to this heading">#</a></h2>
<p>Let’s examine how models complete prompts differently based on demographic cues.</p>
<section id="occupation-bias-test">
<h3>Occupation bias test<a class="headerlink" href="#occupation-bias-test" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_top_next_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get top-k most likely next tokens and their probabilities</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        
        <span class="c1"># Apply temperature</span>
        <span class="k">if</span> <span class="n">temperature</span> <span class="o">!=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span>
        
        <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">top_probs</span><span class="p">,</span> <span class="n">top_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">prob</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">top_probs</span><span class="p">,</span> <span class="n">top_indices</span><span class="p">):</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">token</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">prob</span><span class="p">)))</span>
    
    <span class="k">return</span> <span class="n">results</span>

<span class="c1"># Compare prompts with demographic cues</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The man worked as a&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The woman worked as a&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Top 10 next-token predictions:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Prompt: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="n">top_tokens</span> <span class="o">=</span> <span class="n">get_top_next_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">top_tokens</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">token</span><span class="si">:</span><span class="s2">20s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="race-and-occupation-bias">
<h3>Race and occupation bias<a class="headerlink" href="#race-and-occupation-bias" title="Link to this heading">#</a></h3>
<p><strong>What this code does:</strong></p>
<p>Sets up the OpenAI API client for using GPT-4 or GPT-3.5 models:</p>
<p><strong>How to get an API key:</strong></p>
<ol class="arabic simple">
<li><p>Go to <a class="reference external" href="https://platform.openai.com/">https://platform.openai.com/</a></p></li>
<li><p>Sign up / log in</p></li>
<li><p>Navigate to API keys</p></li>
<li><p>Create a new secret key</p></li>
<li><p>Copy it immediately (you won’t see it again)</p></li>
</ol>
<p><strong>Cost considerations:</strong></p>
<ul class="simple">
<li><p>GPT-4o-mini: ~<span class="math notranslate nohighlight">\(0.15 per 1M input tokens, ~\)</span>0.60 per 1M output tokens (cheap)</p></li>
<li><p>GPT-4o: ~<span class="math notranslate nohighlight">\(2.50 per 1M input tokens, ~\)</span>10 per 1M output tokens (expensive but better)</p></li>
<li><p>Set usage limits in your OpenAI account to avoid surprises</p></li>
</ul>
<p><strong>Security:</strong> The code uses <code class="docutils literal notranslate"><span class="pre">getpass</span></code> so your API key isn’t visible in the notebook output. Never commit API keys to git repositories.</p>
<p><strong>Expected output:</strong> “✓ OpenAI client initialized” - you’re ready to make API calls.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test with racial cues</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The Black man worked as a&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The white man worked as a&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Occupation predictions by race:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Prompt: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="n">top_tokens</span> <span class="o">=</span> <span class="n">get_top_next_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">top_tokens</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">token</span><span class="si">:</span><span class="s2">20s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What this code does:</strong></p>
<p>This demonstrates the basic structure of an OpenAI API call:</p>
<p><strong>Message structure:</strong></p>
<ul class="simple">
<li><p><strong>System message:</strong> Sets the model’s behavior/persona (“You are a creative writer”)</p></li>
<li><p><strong>User message:</strong> Your actual prompt/question</p></li>
</ul>
<p><strong>Key parameters:</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">model</span></code>:</strong> Which model to use</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4o-mini</span></code>: Cheapest, fast, good quality</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-4o</span></code>: More expensive, better reasoning</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpt-3.5-turbo</span></code>: Older, cheaper, lower quality</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">temperature</span></code>:</strong> 0.0 (deterministic) to 2.0 (very random)</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">max_tokens</span></code>:</strong> Maximum length of response</p></li>
</ul>
<p><strong>Response structure:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">response.choices[0].message.content</span></code> contains the generated text</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response.usage</span></code> contains token counts (for cost tracking)</p></li>
</ul>
<p><strong>How to use:</strong> Modify the system and user messages for your task. For annotation tasks, use lower temperature (0.1-0.3) for consistency.</p>
</section>
<section id="generate-full-continuations">
<h3>Generate full continuations<a class="headerlink" href="#generate-full-continuations" title="Link to this heading">#</a></h3>
<p><strong>What this code does:</strong></p>
<p>This demonstrates a simple text annotation task with the OpenAI API:</p>
<p><strong>The task:</strong></p>
<ul class="simple">
<li><p>Classify text into one of three categories: protest, discrimination, or solidarity</p></li>
<li><p>Simple prompt asking for a label</p></li>
<li><p>Returns free text response</p></li>
</ul>
<p><strong>Key features:</strong></p>
<ul class="simple">
<li><p><strong>System message:</strong> Sets context (“You are a careful social science annotator”)</p></li>
<li><p><strong>User message:</strong> Contains the actual classification task</p></li>
<li><p><strong>Temperature 0.1:</strong> Low for consistency (same text → same label)</p></li>
</ul>
<p><strong>Output format:</strong></p>
<ul class="simple">
<li><p>Returns simple text (e.g., “protest”)</p></li>
<li><p>Easy to understand but needs parsing for batch processing</p></li>
<li><p>For structured outputs (JSON), see Week 6</p></li>
</ul>
<p><strong>When to use this approach:</strong></p>
<ul class="simple">
<li><p>Quick exploration and prototyping</p></li>
<li><p>Small-scale annotation (10-100 texts)</p></li>
<li><p>When you’ll manually review each annotation</p></li>
</ul>
<p><strong>Limitations:</strong></p>
<ul class="simple">
<li><p>Model might be verbose (“The label is protest because…”)</p></li>
<li><p>No guaranteed structure</p></li>
<li><p>Harder to parse programmatically</p></li>
</ul>
<p><strong>Next notebook (Week 6):</strong> Learn how to get structured outputs (JSON) for reliable batch annotation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate multiple continuations for comparison</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The doctor said he&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The nurse said she&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Generated continuations:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Prompt: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">generated</span> <span class="o">=</span> <span class="n">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="o">+</span><span class="n">i</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  [</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">] </span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What this code does:</strong></p>
<p>Demonstrates how to annotate multiple texts in a loop:</p>
<p><strong>The <code class="docutils literal notranslate"><span class="pre">annotate_text_simple</span></code> function:</strong></p>
<ul class="simple">
<li><p>Takes a text and returns just the label</p></li>
<li><p>System message includes “Return only the label, nothing else” to reduce verbosity</p></li>
<li><p>Uses temperature 0.1 for consistency</p></li>
</ul>
<p><strong>Batch processing:</strong></p>
<ul class="simple">
<li><p>Loop through list of texts</p></li>
<li><p>Annotate each one</p></li>
<li><p>Store results in a list of dictionaries</p></li>
<li><p>Convert to pandas DataFrame for easy viewing</p></li>
</ul>
<p><strong>Important: <code class="docutils literal notranslate"><span class="pre">time.sleep(0.2)</span></code></strong></p>
<ul class="simple">
<li><p>Adds 200ms delay between API calls</p></li>
<li><p>Prevents hitting rate limits</p></li>
<li><p>Free tier: 3 requests/minute</p></li>
<li><p>Paid tier: Much higher limits</p></li>
</ul>
<p><strong>Limitations of this simple approach:</strong></p>
<ul class="simple">
<li><p>Returns free text (might be “Protest” or “protest” or “This is a protest”)</p></li>
<li><p>No confidence scores</p></li>
<li><p>No rationale/explanation</p></li>
<li><p>Harder to validate programmatically</p></li>
</ul>
<p><strong>For production annotation:</strong></p>
<ul class="simple">
<li><p>Use structured outputs (JSON or function calling)</p></li>
<li><p>Add error handling (try/except)</p></li>
<li><p>Log all requests and responses</p></li>
<li><p>See Week 6 for production-ready approaches</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="part-3-using-openai-api">
<h2>Part 3: Using OpenAI API<a class="headerlink" href="#part-3-using-openai-api" title="Link to this heading">#</a></h2>
<p>Now let’s use GPT-4 (or GPT-3.5) via the OpenAI API for better quality responses.</p>
<section id="setup-openai-api">
<h3>Setup OpenAI API<a class="headerlink" href="#setup-openai-api" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">getpass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># Set API key</span>
<span class="k">if</span> <span class="s2">&quot;OPENAI_API_KEY&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;OPENAI_API_KEY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">getpass</span><span class="o">.</span><span class="n">getpass</span><span class="p">(</span><span class="s2">&quot;Enter your OpenAI API key: &quot;</span><span class="p">)</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✓ OpenAI client initialized&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What this code does:</strong></p>
<p>This demonstrates <strong>function calling</strong> (also called “tools”) - the most structured approach to getting typed outputs:</p>
<p><strong>How it works:</strong></p>
<ol class="arabic simple">
<li><p><strong>Define a schema:</strong> Specify exactly what fields you want, their types, and constraints</p></li>
<li><p><strong>Send as <code class="docutils literal notranslate"><span class="pre">tools</span></code> parameter:</strong> Model knows to call this function</p></li>
<li><p><strong>Extract arguments:</strong> Parse the function call from response</p></li>
</ol>
<p><strong>Key advantages over JSON mode:</strong></p>
<ul class="simple">
<li><p><strong>Type validation:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;type&quot;:</span> <span class="pre">&quot;number&quot;</span></code> ensures numeric values</p></li>
<li><p><strong>Enum constraints:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;enum&quot;:</span> <span class="pre">[...]</span></code> restricts to specific values</p></li>
<li><p><strong>Required fields:</strong> <code class="docutils literal notranslate"><span class="pre">&quot;required&quot;:</span> <span class="pre">[...]</span></code> ensures all fields present</p></li>
<li><p><strong>Nested structures:</strong> Can define complex object hierarchies</p></li>
</ul>
<p><strong>When to use function calling vs JSON mode:</strong></p>
<ul class="simple">
<li><p><strong>Function calling:</strong> When you need strong typing and validation (recommended for production)</p></li>
<li><p><strong>JSON mode:</strong> When you want flexibility or rapid prototyping (faster to set up)</p></li>
</ul>
<p><strong>How to use:</strong> Define one function per annotation type. The model will automatically format its response to match your schema.</p>
<p><strong>Cost:</strong> Same as regular API calls - charged per token.</p>
</section>
<section id="simple-text-generation">
<h3>Simple text generation<a class="headerlink" href="#simple-text-generation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate text with GPT</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a creative writer.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a two-sentence sci-fi plot hook.&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>  <span class="c1"># or &quot;gpt-4&quot; for higher quality</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span>
<span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated plot:</span><span class="se">\n</span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="simple-text-annotation">
<h3>Simple text annotation<a class="headerlink" href="#simple-text-annotation" title="Link to this heading">#</a></h3>
<p>Let’s use the API to annotate text with simple prompts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Annotate text with simple prompt</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Thousands gathered in front of parliament.&quot;</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a careful social science annotator.&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
        <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;&#39;&#39;Label this text as one of: protest, discrimination, or solidarity.</span>

<span class="s1">Text: &quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&quot;</span>

<span class="s1">Label: &#39;&#39;&#39;</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text: </span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Label: </span><span class="si">{</span><span class="n">result</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="annotating-multiple-texts">
<h3>Annotating multiple texts<a class="headerlink" href="#annotating-multiple-texts" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>

<span class="k">def</span><span class="w"> </span><span class="nf">annotate_text_simple</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Annotate a single text - returns simple label</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a careful social science annotator. Return only the label, nothing else.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;&#39;&#39;Label this text as one of: protest, discrimination, or solidarity.</span>

<span class="s1">Text: &quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&quot;</span>

<span class="s1">Label:&#39;&#39;&#39;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Batch annotate several texts</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Thousands gathered in front of parliament.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Volunteers cleaned the park and cooked for neighbors.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;He yelled slurs at a woman on the tram.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Police blocked the march after clashes.&quot;</span>
<span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">annotate_text_simple</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">label</span><span class="p">})</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>  <span class="c1"># Rate limiting</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Annotation results:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="compare-api-vs-open-source-model">
<h3>Compare API vs open-source model<a class="headerlink" href="#compare-api-vs-open-source-model" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test the same prompt with both models</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;The capital of France is&quot;</span>

<span class="c1"># GPT-2 prediction</span>
<span class="n">gpt2_tokens</span> <span class="o">=</span> <span class="n">get_top_next_tokens</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># GPT-4 prediction</span>
<span class="n">gpt4_response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are predicting the next word only.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
<span class="n">gpt4_next</span> <span class="o">=</span> <span class="n">gpt4_response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPT-2 (open-source) top predictions:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">token</span><span class="p">,</span> <span class="n">prob</span> <span class="ow">in</span> <span class="n">gpt2_tokens</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">token</span><span class="si">:</span><span class="s2">15s</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">GPT-4 (API) prediction:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  </span><span class="si">{</span><span class="n">gpt4_next</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="part-4-testing-for-bias-in-api-models">
<h2>Part 4: Testing for Bias in API Models<a class="headerlink" href="#part-4-testing-for-bias-in-api-models" title="Link to this heading">#</a></h2>
<p>Let’s test for similar biases in GPT-4.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test occupation completion</span>
<span class="n">prompts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;The man worked as a&quot;</span><span class="p">,</span>
    <span class="s2">&quot;The woman worked as a&quot;</span><span class="p">,</span>
<span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPT-4 occupation completions:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">70</span><span class="p">)</span>

<span class="k">for</span> <span class="n">prompt</span> <span class="ow">in</span> <span class="n">prompts</span><span class="p">:</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Complete the sentence naturally. Return only the completion.&quot;</span><span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
        <span class="p">],</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">10</span>
    <span class="p">)</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: &#39;</span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Completion: </span><span class="si">{</span><span class="n">completion</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="part-5-open-source-alternatives-to-openai-api">
<h2>Part 5: Open-Source Alternatives to OpenAI API<a class="headerlink" href="#part-5-open-source-alternatives-to-openai-api" title="Link to this heading">#</a></h2>
<p>The examples above use OpenAI’s paid API. Here we show how to accomplish the same tasks using <strong>free, open-source alternatives</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Ollama</strong> - Run models locally with an OpenAI-compatible API</p></li>
<li><p><strong>Hugging Face</strong> - Direct model access with more control</p></li>
</ol>
<p>Both approaches keep your data private and cost nothing to run.</p>
<section id="option-1-using-ollama">
<h3>Option 1: Using Ollama<a class="headerlink" href="#option-1-using-ollama" title="Link to this heading">#</a></h3>
<p><strong>Ollama</strong> provides an OpenAI-compatible API for running models locally.</p>
<p><strong>Setup:</strong></p>
<ol class="arabic simple">
<li><p>Install Ollama: <a class="reference external" href="https://ollama.com/download">https://ollama.com/download</a></p></li>
<li><p>Pull a model: <code class="docutils literal notranslate"><span class="pre">ollama</span> <span class="pre">pull</span> <span class="pre">llama3.2</span></code> (or browse models at <a class="reference external" href="https://ollama.com/library">https://ollama.com/library</a>)</p></li>
<li><p>Ollama runs a local API server automatically</p></li>
</ol>
<p><strong>Advantages:</strong></p>
<ul class="simple">
<li><p>✓ OpenAI-compatible API (minimal code changes)</p></li>
<li><p>✓ Easy to install and use</p></li>
<li><p>✓ Supports many open models (Llama, Mistral, Phi, Gemma, etc.)</p></li>
<li><p>✓ Automatic model management</p></li>
<li><p>✓ Free and runs locally</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Ollama client</span>
<span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-q<span class="w"> </span>ollama

<span class="kn">from</span><span class="w"> </span><span class="nn">ollama</span><span class="w"> </span><span class="kn">import</span> <span class="n">Client</span> <span class="k">as</span> <span class="n">OllamaClient</span>

<span class="c1"># Connect to local Ollama server</span>
<span class="n">ollama_client</span> <span class="o">=</span> <span class="n">OllamaClient</span><span class="p">(</span><span class="n">host</span><span class="o">=</span><span class="s1">&#39;http://localhost:11434&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;✓ Ollama client initialized&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Available models:&quot;</span><span class="p">)</span>
<span class="n">models</span> <span class="o">=</span> <span class="n">ollama_client</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">[</span><span class="s1">&#39;models&#39;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">model</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="text-generation-with-ollama">
<h4>Text Generation with Ollama<a class="headerlink" href="#text-generation-with-ollama" title="Link to this heading">#</a></h4>
<p>The API is nearly identical to OpenAI’s - just change the client and model name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Text generation with Ollama (same API as OpenAI!)</span>
<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a creative writer.&quot;</span><span class="p">},</span>
    <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a two-sentence sci-fi plot hook.&quot;</span><span class="p">}</span>
<span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">ollama_client</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">,</span>  <span class="c1"># Use your installed model</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">options</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
        <span class="s2">&quot;num_predict&quot;</span><span class="p">:</span> <span class="mi">100</span>  <span class="c1"># equivalent to max_tokens</span>
    <span class="p">}</span>
<span class="p">)</span>

<span class="n">generated</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated plot (Ollama):</span><span class="se">\n</span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="text-annotation-with-ollama">
<h4>Text Annotation with Ollama<a class="headerlink" href="#text-annotation-with-ollama" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">annotate_text_ollama</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Annotate a single text using Ollama</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a careful social science annotator. Return only the label, nothing else.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;&#39;&#39;Label this text as one of: protest, discrimination, or solidarity.</span>

<span class="s1">Text: &quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&quot;</span>

<span class="s1">Label:&#39;&#39;&#39;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">ollama_client</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
        <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;temperature&quot;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">}</span>
    <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Batch annotate with Ollama</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Thousands gathered in front of parliament.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Volunteers cleaned the park and cooked for neighbors.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;He yelled slurs at a woman on the tram.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Police blocked the march after clashes.&quot;</span>
<span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">annotate_text_ollama</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">label</span><span class="p">})</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Annotation results (Ollama):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="json-mode-with-ollama">
<h4>JSON Mode with Ollama<a class="headerlink" href="#json-mode-with-ollama" title="Link to this heading">#</a></h4>
<p>Ollama also supports structured JSON outputs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Approach 1: Simple JSON mode</span>
<span class="k">def</span><span class="w"> </span><span class="nf">annotate_with_json_ollama</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">):</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a careful social science annotator. Return valid JSON only.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;&#39;&#39;Label this text as one of: protest, discrimination, or solidarity.</span>

<span class="s1">Text: &quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&quot;</span>

<span class="s1">Return JSON with keys: label, confidence (0-1), reasoning.&#39;&#39;&#39;</span><span class="p">}</span>
    <span class="p">]</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">ollama_client</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
        <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;json&quot;</span>  <span class="c1"># Basic JSON mode</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">])</span>

<span class="c1"># Approach 2: Pydantic structured outputs (stronger typing)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pydantic</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaseModel</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Annotation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">label</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">confidence</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">reasoning</span><span class="p">:</span> <span class="nb">str</span>

<span class="k">def</span><span class="w"> </span><span class="nf">annotate_with_pydantic_ollama</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2&quot;</span><span class="p">):</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a careful social science annotator.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;&#39;&#39;Label this text as one of: protest, discrimination, or solidarity.</span>

<span class="s1">Text: &quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&quot;&#39;&#39;&#39;</span><span class="p">}</span>
    <span class="p">]</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">ollama_client</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
        <span class="nb">format</span><span class="o">=</span><span class="n">Annotation</span><span class="o">.</span><span class="n">model_json_schema</span><span class="p">()</span>  <span class="c1"># Pydantic schema</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">Annotation</span><span class="o">.</span><span class="n">model_validate_json</span><span class="p">(</span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">])</span>

<span class="c1"># Example with both approaches</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Thousands gathered in front of parliament.&quot;</span>

<span class="c1"># Using simple JSON mode</span>
<span class="n">result1</span> <span class="o">=</span> <span class="n">annotate_with_json_ollama</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Simple JSON mode:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Label: </span><span class="si">{</span><span class="n">result1</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Confidence: </span><span class="si">{</span><span class="n">result1</span><span class="p">[</span><span class="s1">&#39;confidence&#39;</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Using Pydantic structured outputs</span>
<span class="n">result2</span> <span class="o">=</span> <span class="n">annotate_with_pydantic_ollama</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pydantic structured outputs:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Label: </span><span class="si">{</span><span class="n">result2</span><span class="o">.</span><span class="n">label</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Confidence: </span><span class="si">{</span><span class="n">result2</span><span class="o">.</span><span class="n">confidence</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="option-2-using-hugging-face-transformers">
<h3>Option 2: Using Hugging Face Transformers<a class="headerlink" href="#option-2-using-hugging-face-transformers" title="Link to this heading">#</a></h3>
<p><strong>Hugging Face</strong> provides direct access to model internals.</p>
<p><strong>Setup:</strong></p>
<ul class="simple">
<li><p>Models run locally</p></li>
<li><p>Requires more VRAM for larger models</p></li>
<li><p>Can use quantized models to reduce memory usage</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load a chat-optimized model from Hugging Face</span>
<span class="c1"># Example: Microsoft Phi-3</span>
<span class="c1"># Note: On Google Colab, you&#39;ll need a GPU runtime for reasonable speed</span>
<span class="n">hf_model_name</span> <span class="o">=</span> <span class="s2">&quot;microsoft/Phi-3-mini-4k-instruct&quot;</span>

<span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">hf_model_name</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">hf_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">hf_model_name</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span>  <span class="c1"># Use half precision to save memory</span>
    <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;✓ Loaded </span><span class="si">{</span><span class="n">hf_model_name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="text-generation-with-hugging-face">
<h4>Text Generation with Hugging Face<a class="headerlink" href="#text-generation-with-hugging-face" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">generate_text_hf</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate text using Hugging Face model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Format prompt as chat message</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a creative writer.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
    <span class="p">]</span>
    
    <span class="c1"># Apply chat template</span>
    <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">messages</span><span class="p">,</span> 
        <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
        <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">hf_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>
    
    <span class="c1"># Decode and extract only new tokens</span>
    <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:]</span>
    <span class="n">generated_text</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">generated_text</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Test generation</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Write a two-sentence sci-fi plot hook.&quot;</span>
<span class="n">generated</span> <span class="o">=</span> <span class="n">generate_text_hf</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prompt: </span><span class="si">{</span><span class="n">prompt</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated (Hugging Face): </span><span class="si">{</span><span class="n">generated</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="text-annotation-with-hugging-face">
<h4>Text Annotation with Hugging Face<a class="headerlink" href="#text-annotation-with-hugging-face" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">annotate_text_hf</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Annotate a single text using Hugging Face model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a careful social science annotator. Return only the label, nothing else.&quot;</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;&#39;&#39;Label this text as one of: protest, discrimination, or solidarity.</span>

<span class="s1">Text: &quot;</span><span class="si">{</span><span class="n">text</span><span class="si">}</span><span class="s1">&quot;</span>

<span class="s1">Label:&#39;&#39;&#39;</span>
        <span class="p">}</span>
    <span class="p">]</span>
    
    <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span>
        <span class="n">messages</span><span class="p">,</span> 
        <span class="n">tokenize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
        <span class="n">add_generation_prompt</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">hf_model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
        <span class="o">**</span><span class="n">inputs</span><span class="p">,</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="n">max_new_tokens</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">do_sample</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">pad_token_id</span><span class="o">=</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span>
    <span class="p">)</span>
    
    <span class="n">generated_ids</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:]</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">generated_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">label</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

<span class="c1"># Batch annotate with Hugging Face</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;Thousands gathered in front of parliament.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Volunteers cleaned the park and cooked for neighbors.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;He yelled slurs at a woman on the tram.&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Police blocked the march after clashes.&quot;</span>
<span class="p">]</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
    <span class="n">label</span> <span class="o">=</span> <span class="n">annotate_text_hf</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="n">text</span><span class="p">,</span> <span class="s2">&quot;label&quot;</span><span class="p">:</span> <span class="n">label</span><span class="p">})</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Annotation results (Hugging Face):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="notes-on-running-locations">
<h3>Notes on Running Locations<a class="headerlink" href="#notes-on-running-locations" title="Link to this heading">#</a></h3>
<p><strong>Ollama:</strong></p>
<ul class="simple">
<li><p>Requires local installation (cannot run on Google Colab)</p></li>
<li><p>Download from <a class="reference external" href="https://ollama.com/download">https://ollama.com/download</a></p></li>
<li><p>Runs models on your own machine</p></li>
</ul>
<p><strong>Hugging Face Transformers:</strong></p>
<ul class="simple">
<li><p>Works on Google Colab (requires GPU runtime for reasonable speed)</p></li>
<li><p>Works locally</p></li>
<li><p>Downloads models automatically on first use</p></li>
</ul>
<p><strong>OpenAI API:</strong></p>
<ul class="simple">
<li><p>Works anywhere with internet connection</p></li>
<li><p>No local installation of models required</p></li>
<li><p>Requires API key and incurs costs per token</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<p><strong>What we learned:</strong></p>
<ol class="arabic simple">
<li><p>✓ How <strong>causal language models</strong> generate text (next-token prediction)</p></li>
<li><p>✓ How <strong>temperature</strong> and <strong>sampling</strong> control randomness</p></li>
<li><p>✓ How to detect <strong>bias</strong> in model outputs</p></li>
<li><p>✓ How to use <strong>OpenAI API</strong> for generation and annotation</p></li>
<li><p>✓ How to get <strong>structured outputs</strong> with JSON mode and function calling</p></li>
</ol>
<p><strong>Key insights:</strong></p>
<ul class="simple">
<li><p>LLMs learn from their training data, including <strong>biases</strong></p></li>
<li><p><strong>Temperature</strong> is crucial: low for factual tasks, higher for creative tasks</p></li>
<li><p><strong>API models</strong> (GPT-4) are more capable but less transparent than open models</p></li>
<li><p><strong>Structured outputs</strong> (JSON, function calling) are essential for reliable annotation</p></li>
</ul>
<p><strong>Open-source vs. API comparison:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Open-source (GPT-2)</p></th>
<th class="head"><p>API (GPT-4)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Cost</p></td>
<td><p>Free</p></td>
<td><p>Pay per token</p></td>
</tr>
<tr class="row-odd"><td><p>Quality</p></td>
<td><p>Lower</p></td>
<td><p>Higher</p></td>
</tr>
<tr class="row-even"><td><p>Privacy</p></td>
<td><p>Local data</p></td>
<td><p>Sent to provider</p></td>
</tr>
<tr class="row-odd"><td><p>Reproducibility</p></td>
<td><p>Fixed weights</p></td>
<td><p>Version drift</p></td>
</tr>
<tr class="row-even"><td><p>Speed</p></td>
<td><p>Depends on hardware</p></td>
<td><p>Fast, optimized</p></td>
</tr>
<tr class="row-odd"><td><p>Control</p></td>
<td><p>Full access</p></td>
<td><p>Black box</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Next steps:</strong></p>
<ul class="simple">
<li><p>Week 5: Use LLMs for <strong>qualitative coding</strong> and thematic analysis</p></li>
<li><p>Week 6: Advanced <strong>prompting strategies</strong> and structured outputs</p></li>
</ul>
<p><strong>Ethical considerations:</strong></p>
<ul class="simple">
<li><p>Always test for demographic biases before deploying</p></li>
<li><p>Document model versions and settings</p></li>
<li><p>Validate against human annotations</p></li>
<li><p>Be transparent about using AI in research</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">GenAI for Sociology</p>
      </div>
    </a>
    <a class="right-next"
       href="week05_qualitative.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Qualitative Coding with LLMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setup">Setup</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-text-generation-with-gpt-2">Part 1: Text Generation with GPT-2</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-gpt-2-model">Load GPT-2 model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-next-token-prediction">Understanding next-token prediction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-longer-text">Generate longer text</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-temperature">Effect of temperature</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-2-detecting-bias-in-language-models">Part 2: Detecting Bias in Language Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#occupation-bias-test">Occupation bias test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#race-and-occupation-bias">Race and occupation bias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-full-continuations">Generate full continuations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-using-openai-api">Part 3: Using OpenAI API</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#setup-openai-api">Setup OpenAI API</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-generation">Simple text generation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-text-annotation">Simple text annotation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#annotating-multiple-texts">Annotating multiple texts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compare-api-vs-open-source-model">Compare API vs open-source model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-4-testing-for-bias-in-api-models">Part 4: Testing for Bias in API Models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-5-open-source-alternatives-to-openai-api">Part 5: Open-Source Alternatives to OpenAI API</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-1-using-ollama">Option 1: Using Ollama</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-with-ollama">Text Generation with Ollama</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-annotation-with-ollama">Text Annotation with Ollama</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#json-mode-with-ollama">JSON Mode with Ollama</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#option-2-using-hugging-face-transformers">Option 2: Using Hugging Face Transformers</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-generation-with-hugging-face">Text Generation with Hugging Face</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#text-annotation-with-hugging-face">Text Annotation with Hugging Face</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#notes-on-running-locations">Notes on Running Locations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Christopher Barrie
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
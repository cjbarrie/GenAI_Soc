{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LLMs for Annotation I\n\n**Learning objectives:**\n- Understand how generative LLMs work (GPT-style models)\n- Generate text with GPT-2 and control outputs with temperature/sampling\n- Use OpenAI API for generation and simple annotation\n- Detect and analyze bias in model outputs\n- Compare open-source vs. API models\n\n**How to run this notebook:**\n- **Google Colab** (recommended): Works for all parts, GPU helpful for GPT-2\n- **OpenAI API key needed**: For Part 3 (API examples)\n- **Local**: Works, but GPT-2 generation slow on CPU\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q transformers torch openai\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\u2713 Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis setup cell installs and imports the necessary libraries:\n- `transformers`: Hugging Face library for loading pre-trained models (GPT-2)\n- `torch`: PyTorch deep learning framework (required for running models)\n- `openai`: Official OpenAI Python client for API access\n\n**Device selection:** The code automatically detects if you have a GPU available (CUDA). GPUs are much faster for running large models like GPT-2. If no GPU is available, it falls back to CPU (slower but functional).\n\n**How to use:**\n- In Google Colab: Select Runtime \u2192 Change runtime type \u2192 Hardware accelerator \u2192 GPU (T4)\n- Locally: Works on CPU, but generation will be slower",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Text Generation with GPT-2\n",
    "\n",
    "GPT-2 is a **causal language model** - it predicts the next word given previous words.\n",
    "\n",
    "**How it works:**\n",
    "1. Takes a prompt (starting text)\n",
    "2. Predicts next token probabilities\n",
    "3. Samples a token\n",
    "4. Repeats until done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GPT-2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis loads the GPT-2 model from Hugging Face's model hub:\n\n- **Model choice:** `gpt2-medium` has 355M parameters - a good balance between quality and speed. Other options:\n  - `gpt2` (124M) - faster but lower quality\n  - `gpt2-large` (774M) - better quality but slower\n  - `gpt2-xl` (1.5B) - best quality but very slow without GPU\n\n- **Tokenizer:** Converts text to numbers (tokens) that the model understands\n- **Model:** The neural network that generates text\n- **`.to(device)`:** Moves the model to GPU if available\n\n**Expected output:** You'll see a progress bar as the model downloads (~1.4GB for gpt2-medium). Subsequent runs will use the cached version.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPT-2 medium (355M parameters, good balance of quality/speed)\n",
    "model_name = \"gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "print(f\"\u2713 Loaded {model_name}\")\n",
    "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis demonstrates the core mechanism of how LLMs work - predicting the next token:\n\n1. **Tokenization:** Convert the prompt into token IDs\n2. **Forward pass:** Run the model to get logits (raw prediction scores) for every possible next token\n3. **Select last token:** `logits[:, -1, :]` gets the predictions for what comes after the prompt\n4. **Greedy decoding:** `argmax` picks the single most likely token\n\n**Key insight:** The model outputs a probability distribution over its entire vocabulary (~50k tokens). We can use this to see alternative possibilities, not just the top choice.\n\n**Expected output:** For \"The capital of France is\", the model will predict \" Paris\" with high confidence.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nInstead of just the single most likely token, this shows the top 5 alternatives with their probabilities:\n\n- **`torch.softmax`:** Converts raw logits into proper probabilities (0-1, summing to 1)\n- **`torch.topk`:** Gets the k highest values and their indices\n- **Decoding:** Converts token IDs back to readable text\n\n**Why this matters:** Understanding the probability distribution helps us see model confidence and alternative possibilities. A model that assigns 0.95 probability to one token is very confident; a model with 0.25 spread across several tokens is uncertain.\n\n**Expected output:** You'll see \" Paris\" with highest probability, followed by other French cities or alternative phrasings.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding next-token prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this function does:**\n\nThis is our main text generation function with important controls:\n\n**Parameters explained:**\n- **`max_new_tokens`:** How many tokens to generate (roughly 1 token = 0.75 words)\n- **`temperature`:** Controls randomness\n  - 0.1-0.5: Very focused, deterministic (good for factual tasks)\n  - 0.7-1.0: Balanced (good default)\n  - 1.5+: Very random, creative, sometimes incoherent\n- **`top_p` (nucleus sampling):** Only sample from top X% of probability mass\n  - 0.9 = ignore the bottom 10% least likely tokens\n  - Helps avoid nonsensical rare tokens\n- **`seed`:** For reproducibility - same seed = same output\n\n**Key steps:**\n1. Set random seed for reproducibility\n2. Tokenize the prompt\n3. Call `model.generate()` with sampling parameters\n4. Extract only the newly generated tokens (skip the prompt)\n5. Decode back to text\n\n**How to use:** Call with different prompts and adjust temperature to control creativity.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example: predict one next token\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits  # [batch, sequence_length, vocab_size]\n",
    "    \n",
    "    # Get last token's predictions (what comes next)\n",
    "    next_token_logits = logits[:, -1, :]\n",
    "    \n",
    "    # Greedy decoding: pick most likely token\n",
    "    next_token_id = next_token_logits.argmax(dim=-1)\n",
    "    next_token = tokenizer.decode(next_token_id[0])\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Most likely next token: '{next_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at top-5 most likely next tokens\n",
    "probs = torch.softmax(next_token_logits, dim=-1)\n",
    "top_probs, top_indices = torch.topk(probs[0], k=5)\n",
    "\n",
    "print(f\"\\nTop 5 predictions for: '{prompt}'\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    token = tokenizer.decode([idx])\n",
    "    print(f\"  '{token}' - {prob:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate longer text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_new_tokens=50, temperature=1.0, top_p=0.9, seed=42):\n",
    "    \"\"\"\n",
    "    Generate text from a prompt\n",
    "    \n",
    "    Args:\n",
    "        prompt: Starting text\n",
    "        max_new_tokens: How many tokens to generate\n",
    "        temperature: Randomness (lower = more deterministic)\n",
    "        top_p: Nucleus sampling (0.9 = use top 90% probability mass)\n",
    "        seed: Random seed for reproducibility\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,          # use sampling (not greedy)\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and return only new tokens (skip prompt)\n",
    "    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text.strip()\n",
    "\n",
    "# Test\n",
    "prompt = \"Write a two-sentence sci-fi plot:\"\n",
    "generated = generate_text(prompt, max_new_tokens=50)\n",
    "\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis demonstrates systematic bias testing by comparing model predictions for gendered prompts:\n\n**The function `get_top_next_tokens`:**\n- Takes a prompt and returns the k most likely next tokens\n- Applies temperature if specified\n- Returns both tokens and their probabilities\n\n**Why this matters for bias detection:**\n- If \"The man worked as a\" \u2192 mostly predicts high-status occupations\n- But \"The woman worked as a\" \u2192 predicts lower-status or stereotypical occupations\n- This reveals gender bias learned from training data\n\n**Expected observation:** You'll likely see stereotypical patterns like:\n- \"man\" \u2192 lawyer, engineer, doctor, CEO\n- \"woman\" \u2192 teacher, nurse, secretary, assistant\n\n**Sociological significance:** These biases reflect and may perpetuate real-world inequality. Always test models for demographic biases before using them for annotation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of temperature\n",
    "\n",
    "**Temperature** controls randomness:\n",
    "- Low (0.1-0.5): More focused, repetitive\n",
    "- Medium (0.7-1.0): Balanced\n",
    "- High (1.5+): More random, creative, incoherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Sociology is the study of\"\n",
    "temperatures = [0.3, 0.7, 1.2]\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in temperatures:\n",
    "    generated = generate_text(prompt, max_new_tokens=30, temperature=temp)\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    print(f\"  {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Detecting Bias in Language Models\n",
    "\n",
    "Let's examine how models complete prompts differently based on demographic cues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupation bias test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_next_tokens(prompt, k=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Get top-k most likely next tokens and their probabilities\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        if temperature != 1.0:\n",
    "            logits = logits / temperature\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        top_probs, top_indices = torch.topk(probs[0], k=k)\n",
    "    \n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx])\n",
    "        results.append((token, float(prob)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare prompts with demographic cues\n",
    "prompts = [\n",
    "    \"The man worked as a\",\n",
    "    \"The woman worked as a\",\n",
    "]\n",
    "\n",
    "print(\"Top 10 next-token predictions:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    top_tokens = get_top_next_tokens(prompt, k=10)\n",
    "    for token, prob in top_tokens:\n",
    "        print(f\"  {token:20s} {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race and occupation bias"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nSets up the OpenAI API client for using GPT-4 or GPT-3.5 models:\n\n**How to get an API key:**\n1. Go to https://platform.openai.com/\n2. Sign up / log in\n3. Navigate to API keys\n4. Create a new secret key\n5. Copy it immediately (you won't see it again)\n\n**Cost considerations:**\n- GPT-4o-mini: ~$0.15 per 1M input tokens, ~$0.60 per 1M output tokens (cheap)\n- GPT-4o: ~$2.50 per 1M input tokens, ~$10 per 1M output tokens (expensive but better)\n- Set usage limits in your OpenAI account to avoid surprises\n\n**Security:** The code uses `getpass` so your API key isn't visible in the notebook output. Never commit API keys to git repositories.\n\n**Expected output:** \"\u2713 OpenAI client initialized\" - you're ready to make API calls.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with racial cues\n",
    "prompts = [\n",
    "    \"The Black man worked as a\",\n",
    "    \"The white man worked as a\",\n",
    "]\n",
    "\n",
    "print(\"Occupation predictions by race:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    top_tokens = get_top_next_tokens(prompt, k=8)\n",
    "    for token, prob in top_tokens:\n",
    "        print(f\"  {token:20s} {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis demonstrates the basic structure of an OpenAI API call:\n\n**Message structure:**\n- **System message:** Sets the model's behavior/persona (\"You are a creative writer\")\n- **User message:** Your actual prompt/question\n\n**Key parameters:**\n- **`model`:** Which model to use\n  - `gpt-4o-mini`: Cheapest, fast, good quality\n  - `gpt-4o`: More expensive, better reasoning\n  - `gpt-3.5-turbo`: Older, cheaper, lower quality\n- **`temperature`:** 0.0 (deterministic) to 2.0 (very random)\n- **`max_tokens`:** Maximum length of response\n\n**Response structure:**\n- `response.choices[0].message.content` contains the generated text\n- `response.usage` contains token counts (for cost tracking)\n\n**How to use:** Modify the system and user messages for your task. For annotation tasks, use lower temperature (0.1-0.3) for consistency.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate full continuations"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis demonstrates a simple text annotation task with the OpenAI API:\n\n**The task:**\n- Classify text into one of three categories: protest, discrimination, or solidarity\n- Simple prompt asking for a label\n- Returns free text response\n\n**Key features:**\n- **System message:** Sets context (\"You are a careful social science annotator\")\n- **User message:** Contains the actual classification task\n- **Temperature 0.1:** Low for consistency (same text \u2192 same label)\n\n**Output format:**\n- Returns simple text (e.g., \"protest\")\n- Easy to understand but needs parsing for batch processing\n- For structured outputs (JSON), see Week 6\n\n**When to use this approach:**\n- Quick exploration and prototyping\n- Small-scale annotation (10-100 texts)\n- When you'll manually review each annotation\n\n**Limitations:**\n- Model might be verbose (\"The label is protest because...\")\n- No guaranteed structure\n- Harder to parse programmatically\n\n**Next notebook (Week 6):** Learn how to get structured outputs (JSON) for reliable batch annotation.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple continuations for comparison\n",
    "prompts = [\n",
    "    \"The doctor said he\",\n",
    "    \"The nurse said she\",\n",
    "]\n",
    "\n",
    "print(\"Generated continuations:\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    for i in range(3):\n",
    "        generated = generate_text(prompt, max_new_tokens=20, seed=42+i)\n",
    "        print(f\"  [{i+1}] {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nDemonstrates how to annotate multiple texts in a loop:\n\n**The `annotate_text_simple` function:**\n- Takes a text and returns just the label\n- System message includes \"Return only the label, nothing else\" to reduce verbosity\n- Uses temperature 0.1 for consistency\n\n**Batch processing:**\n- Loop through list of texts\n- Annotate each one\n- Store results in a list of dictionaries\n- Convert to pandas DataFrame for easy viewing\n\n**Important: `time.sleep(0.2)`**\n- Adds 200ms delay between API calls\n- Prevents hitting rate limits\n- Free tier: 3 requests/minute\n- Paid tier: Much higher limits\n\n**Limitations of this simple approach:**\n- Returns free text (might be \"Protest\" or \"protest\" or \"This is a protest\")\n- No confidence scores\n- No rationale/explanation\n- Harder to validate programmatically\n\n**For production annotation:**\n- Use structured outputs (JSON or function calling)\n- Add error handling (try/except)\n- Log all requests and responses\n- See Week 6 for production-ready approaches",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 3: Using OpenAI API\n\nNow let's use GPT-4 (or GPT-3.5) via the OpenAI API for better quality responses."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "print(\"\u2713 OpenAI client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis demonstrates **function calling** (also called \"tools\") - the most structured approach to getting typed outputs:\n\n**How it works:**\n1. **Define a schema:** Specify exactly what fields you want, their types, and constraints\n2. **Send as `tools` parameter:** Model knows to call this function\n3. **Extract arguments:** Parse the function call from response\n\n**Key advantages over JSON mode:**\n- **Type validation:** `\"type\": \"number\"` ensures numeric values\n- **Enum constraints:** `\"enum\": [...]` restricts to specific values\n- **Required fields:** `\"required\": [...]` ensures all fields present\n- **Nested structures:** Can define complex object hierarchies\n\n**When to use function calling vs JSON mode:**\n- **Function calling:** When you need strong typing and validation (recommended for production)\n- **JSON mode:** When you want flexibility or rapid prototyping (faster to set up)\n\n**How to use:** Define one function per annotation type. The model will automatically format its response to match your schema.\n\n**Cost:** Same as regular API calls - charged per token.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with GPT\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a two-sentence sci-fi plot hook.\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",  # or \"gpt-4\" for higher quality\n",
    "    messages=messages,\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "generated = response.choices[0].message.content\n",
    "print(f\"Generated plot:\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Simple text annotation\n\nLet's use the API to annotate text with simple prompts."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Annotate text with simple prompt\ntext = \"Thousands gathered in front of parliament.\"\n\nmessages = [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a careful social science annotator.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": f'''Label this text as one of: protest, discrimination, or solidarity.\n\nText: \"{text}\"\n\nLabel: '''\n    }\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=messages,\n    temperature=0.1\n)\n\nresult = response.choices[0].message.content\nprint(f\"Text: {text}\\n\")\nprint(f\"Label: {result}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Annotating multiple texts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport time\n\ndef annotate_text_simple(text, model=\"gpt-4o-mini\"):\n    \"\"\"\n    Annotate a single text - returns simple label\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a careful social science annotator. Return only the label, nothing else.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f'''Label this text as one of: protest, discrimination, or solidarity.\n\nText: \"{text}\"\n\nLabel:'''\n        }\n    ]\n    \n    response = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=0.1\n    )\n    \n    return response.choices[0].message.content.strip()\n\n# Batch annotate several texts\ntexts = [\n    \"Thousands gathered in front of parliament.\",\n    \"Volunteers cleaned the park and cooked for neighbors.\",\n    \"He yelled slurs at a woman on the tram.\",\n    \"Police blocked the march after clashes.\"\n]\n\nresults = []\nfor text in texts:\n    label = annotate_text_simple(text)\n    results.append({\"text\": text, \"label\": label})\n    time.sleep(0.2)  # Rate limiting\n\ndf = pd.DataFrame(results)\nprint(\"\\nAnnotation results:\")\nprint(df.to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare API vs open-source model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the same prompt with both models\n",
    "prompt = \"The capital of France is\"\n",
    "\n",
    "# GPT-2 prediction\n",
    "gpt2_tokens = get_top_next_tokens(prompt, k=5)\n",
    "\n",
    "# GPT-4 prediction\n",
    "gpt4_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are predicting the next word only.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.1,\n",
    "    max_tokens=5\n",
    ")\n",
    "gpt4_next = gpt4_response.choices[0].message.content\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"GPT-2 (open-source) top predictions:\")\n",
    "for token, prob in gpt2_tokens:\n",
    "    print(f\"  {token:15s} {prob:.4f}\")\n",
    "\n",
    "print(f\"\\nGPT-4 (API) prediction:\")\n",
    "print(f\"  {gpt4_next}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 4: Testing for Bias in API Models\n\nLet's test for similar biases in GPT-4."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Test occupation completion\nprompts = [\n    \"The man worked as a\",\n    \"The woman worked as a\",\n]\n\nprint(\"GPT-4 occupation completions:\\n\")\nprint(\"=\" * 70)\n\nfor prompt in prompts:\n    response = client.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Complete the sentence naturally. Return only the completion.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n        temperature=0.7,\n        max_tokens=10\n    )\n    completion = response.choices[0].message.content\n    print(f\"Prompt: '{prompt}'\")\n    print(f\"  Completion: {completion}\\n\")",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Part 5: Open-Source Alternatives to OpenAI API\n\nThe examples above use OpenAI's paid API. Here we show how to accomplish the same tasks using **free, open-source alternatives**:\n\n1. **Ollama** - Run models locally with an OpenAI-compatible API\n2. **Hugging Face** - Direct model access with more control\n\nBoth approaches keep your data private and cost nothing to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Using Ollama\n\n**Ollama** provides an OpenAI-compatible API for running models locally.\n\n**Setup:**\n1. Install Ollama: https://ollama.com/download\n2. Pull a model: `ollama pull llama3.2` (or browse models at https://ollama.com/library)\n3. Ollama runs a local API server automatically\n\n**Advantages:**\n- \u2713 OpenAI-compatible API (minimal code changes)\n- \u2713 Easy to install and use\n- \u2713 Supports many open models (Llama, Mistral, Phi, Gemma, etc.)\n- \u2713 Automatic model management\n- \u2713 Free and runs locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama client\n!pip install -q ollama\n\nfrom ollama import Client as OllamaClient\n\n# Connect to local Ollama server\nollama_client = OllamaClient(host='http://localhost:11434')\n\nprint(\"\u2713 Ollama client initialized\")\nprint(\"\\nAvailable models:\")\nmodels = ollama_client.list()\nfor model in models['models']:\n    print(f\"  - {model['name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation with Ollama\n\nThe API is nearly identical to OpenAI's - just change the client and model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with Ollama (same API as OpenAI!)\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n    {\"role\": \"user\", \"content\": \"Write a two-sentence sci-fi plot hook.\"}\n]\n\nresponse = ollama_client.chat(\n    model=\"llama3.2\",  # Use your installed model\n    messages=messages,\n    options={\n        \"temperature\": 0.7,\n        \"num_predict\": 100  # equivalent to max_tokens\n    }\n)\n\ngenerated = response['message']['content']\nprint(f\"Generated plot (Ollama):\\n{generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Annotation with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text_ollama(text, model=\"llama3.2\"):\n    \"\"\"\n    Annotate a single text using Ollama\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a careful social science annotator. Return only the label, nothing else.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f'''Label this text as one of: protest, discrimination, or solidarity.\n\nText: \"{text}\"\n\nLabel:'''\n        }\n    ]\n    \n    response = ollama_client.chat(\n        model=model,\n        messages=messages,\n        options={\"temperature\": 0.1}\n    )\n    \n    return response['message']['content'].strip()\n\n# Batch annotate with Ollama\ntexts = [\n    \"Thousands gathered in front of parliament.\",\n    \"Volunteers cleaned the park and cooked for neighbors.\",\n    \"He yelled slurs at a woman on the tram.\",\n    \"Police blocked the march after clashes.\"\n]\n\nresults = []\nfor text in texts:\n    label = annotate_text_ollama(text)\n    results.append({\"text\": text, \"label\": label})\n\ndf = pd.DataFrame(results)\nprint(\"\\nAnnotation results (Ollama):\")\nprint(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JSON Mode with Ollama\n\nOllama also supports structured JSON outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Simple JSON mode\n",
    "def annotate_with_json_ollama(text, model=\"llama3.2\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a careful social science annotator. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": f'''Label this text as one of: protest, discrimination, or solidarity.\n",
    "\n",
    "Text: \"{text}\"\n",
    "\n",
    "Return JSON with keys: label, confidence (0-1), reasoning.'''}\n",
    "    ]\n",
    "    \n",
    "    response = ollama_client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        format=\"json\"  # Basic JSON mode\n",
    "    )\n",
    "    return json.loads(response['message']['content'])\n",
    "\n",
    "# Approach 2: Pydantic structured outputs (stronger typing)\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Annotation(BaseModel):\n",
    "    label: str\n",
    "    confidence: float\n",
    "    reasoning: str\n",
    "\n",
    "def annotate_with_pydantic_ollama(text, model=\"llama3.2\"):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a careful social science annotator.\"},\n",
    "        {\"role\": \"user\", \"content\": f'''Label this text as one of: protest, discrimination, or solidarity.\n",
    "\n",
    "Text: \"{text}\"'''}\n",
    "    ]\n",
    "    \n",
    "    response = ollama_client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        format=Annotation.model_json_schema()  # Pydantic schema\n",
    "    )\n",
    "    return Annotation.model_validate_json(response['message']['content'])\n",
    "\n",
    "# Example with both approaches\n",
    "text = \"Thousands gathered in front of parliament.\"\n",
    "\n",
    "# Using simple JSON mode\n",
    "result1 = annotate_with_json_ollama(text)\n",
    "print(f\"Simple JSON mode:\")\n",
    "print(f\"  Label: {result1['label']}\")\n",
    "print(f\"  Confidence: {result1['confidence']}\\n\")\n",
    "\n",
    "# Using Pydantic structured outputs\n",
    "result2 = annotate_with_pydantic_ollama(text)\n",
    "print(f\"Pydantic structured outputs:\")\n",
    "print(f\"  Label: {result2.label}\")\n",
    "print(f\"  Confidence: {result2.confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Using Hugging Face Transformers\n\n**Hugging Face** provides direct access to model internals.\n\n**Setup:**\n- Models run locally\n- Requires more VRAM for larger models\n- Can use quantized models to reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a chat-optimized model from Hugging Face\n# Example: Microsoft Phi-3\n# Note: On Google Colab, you'll need a GPU runtime for reasonable speed\nhf_model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n\nhf_tokenizer = AutoTokenizer.from_pretrained(hf_model_name, trust_remote_code=True)\nhf_model = AutoModelForCausalLM.from_pretrained(\n    hf_model_name,\n    torch_dtype=torch.float16,  # Use half precision to save memory\n    trust_remote_code=True\n).to(device)\n\nprint(f\"\u2713 Loaded {hf_model_name}\")\nprint(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_hf(prompt, max_new_tokens=100, temperature=0.7):\n    \"\"\"\n    Generate text using Hugging Face model\n    \"\"\"\n    # Format prompt as chat message\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n        {\"role\": \"user\", \"content\": prompt}\n    ]\n    \n    # Apply chat template\n    formatted_prompt = hf_tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n    \n    inputs = hf_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n    \n    outputs = hf_model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        do_sample=True,\n        pad_token_id=hf_tokenizer.eos_token_id\n    )\n    \n    # Decode and extract only new tokens\n    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n    generated_text = hf_tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    return generated_text.strip()\n\n# Test generation\nprompt = \"Write a two-sentence sci-fi plot hook.\"\ngenerated = generate_text_hf(prompt, max_new_tokens=50)\n\nprint(f\"Prompt: {prompt}\\n\")\nprint(f\"Generated (Hugging Face): {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Annotation with Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text_hf(text, max_new_tokens=10):\n    \"\"\"\n    Annotate a single text using Hugging Face model\n    \"\"\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a careful social science annotator. Return only the label, nothing else.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f'''Label this text as one of: protest, discrimination, or solidarity.\n\nText: \"{text}\"\n\nLabel:'''\n        }\n    ]\n    \n    formatted_prompt = hf_tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n    \n    inputs = hf_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n    \n    outputs = hf_model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        temperature=0.1,\n        do_sample=True,\n        pad_token_id=hf_tokenizer.eos_token_id\n    )\n    \n    generated_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n    label = hf_tokenizer.decode(generated_ids, skip_special_tokens=True)\n    \n    return label.strip()\n\n# Batch annotate with Hugging Face\ntexts = [\n    \"Thousands gathered in front of parliament.\",\n    \"Volunteers cleaned the park and cooked for neighbors.\",\n    \"He yelled slurs at a woman on the tram.\",\n    \"Police blocked the march after clashes.\"\n]\n\nresults = []\nfor text in texts:\n    label = annotate_text_hf(text)\n    results.append({\"text\": text, \"label\": label})\n\ndf = pd.DataFrame(results)\nprint(\"\\nAnnotation results (Hugging Face):\")\nprint(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Running Locations\n\n**Ollama:**\n- Requires local installation (cannot run on Google Colab)\n- Download from https://ollama.com/download\n- Runs models on your own machine\n\n**Hugging Face Transformers:**\n- Works on Google Colab (requires GPU runtime for reasonable speed)\n- Works locally\n- Downloads models automatically on first use\n\n**OpenAI API:**\n- Works anywhere with internet connection\n- No local installation of models required\n- Requires API key and incurs costs per token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**What we learned:**\n",
    "1. \u2713 How **causal language models** generate text (next-token prediction)\n",
    "2. \u2713 How **temperature** and **sampling** control randomness\n",
    "3. \u2713 How to detect **bias** in model outputs\n",
    "4. \u2713 How to use **OpenAI API** for generation and annotation\n",
    "5. \u2713 How to get **structured outputs** with JSON mode and function calling\n",
    "\n",
    "**Key insights:**\n",
    "- LLMs learn from their training data, including **biases**\n",
    "- **Temperature** is crucial: low for factual tasks, higher for creative tasks\n",
    "- **API models** (GPT-4) are more capable but less transparent than open models\n",
    "- **Structured outputs** (JSON, function calling) are essential for reliable annotation\n",
    "\n",
    "**Open-source vs. API comparison:**\n",
    "\n",
    "| Aspect | Open-source (GPT-2) | API (GPT-4) |\n",
    "|--------|---------------------|-------------|\n",
    "| Cost | Free | Pay per token |\n",
    "| Quality | Lower | Higher |\n",
    "| Privacy | Local data | Sent to provider |\n",
    "| Reproducibility | Fixed weights | Version drift |\n",
    "| Speed | Depends on hardware | Fast, optimized |\n",
    "| Control | Full access | Black box |\n",
    "\n",
    "**Next steps:**\n",
    "- Week 5: Use LLMs for **qualitative coding** and thematic analysis\n",
    "- Week 6: Advanced **prompting strategies** and structured outputs\n",
    "\n",
    "**Ethical considerations:**\n",
    "- Always test for demographic biases before deploying\n",
    "- Document model versions and settings\n",
    "- Validate against human annotations\n",
    "- Be transparent about using AI in research"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Structured Output Annotation Examples\n",
    "\n",
    "This notebook demonstrates different approaches to getting structured outputs from LLMs for text annotation tasks.\n",
    "\n",
    "**Topics covered:**\n",
    "1. Basic prompting patterns\n",
    "2. Four approaches to structured outputs\n",
    "3. Robust JSON extraction\n",
    "4. Batch annotation\n",
    "5. Local models with Ollama (optional)\n",
    "6. Mixture of experts (ensemble)\n",
    "7. Validation and logging\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install required packages and set up API keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -q openai pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import hashlib\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key\n",
    "import getpass\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key: ')\n",
    "\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "print(\"✓ API key configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 1: Basic Prompting Patterns\n",
    "\n",
    "Simple approaches to formatting prompts for text annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample political texts\n",
    "texts = [\n",
    "    \"We must invest in renewable energy now!\",\n",
    "    \"Cut taxes and reduce business regulations\",\n",
    "    \"Healthcare is a human right for all citizens\",\n",
    "    \"Maintain current spending levels and balanced budget\"\n",
    "]\n",
    "\n",
    "print(\"Sample texts:\")\n",
    "for i, text in enumerate(texts, 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1A: Simple f-string Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = texts[0]\n",
    "\n",
    "# Simple f-string approach\n",
    "prompt = f\"\"\"Classify the political stance of this text as:\n",
    "- Progressive\n",
    "- Conservative\n",
    "- Centrist\n",
    "\n",
    "Text: {text}\n",
    "Stance:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Response: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1B: Reusable Template for Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template approach for consistency\n",
    "STANCE_TEMPLATE = \"\"\"Classify the political stance of this text as:\n",
    "- Progressive\n",
    "- Conservative\n",
    "- Centrist\n",
    "\n",
    "Text: {text}\n",
    "Stance:\"\"\"\n",
    "\n",
    "results = []\n",
    "for text in texts:\n",
    "    prompt = STANCE_TEMPLATE.format(text=text)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    result = response.choices[0].message.content.strip()\n",
    "    results.append({\"text\": text, \"stance\": result})\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Stance: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1C: Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot template with examples\n",
    "FEW_SHOT_TEMPLATE = \"\"\"Classify political stance as Progressive, Conservative, or Centrist.\n",
    "\n",
    "Examples:\n",
    "Text: \"Cut taxes and reduce regulations\" -> Conservative\n",
    "Text: \"Expand healthcare access for all\" -> Progressive\n",
    "Text: \"Maintain current spending levels\" -> Centrist\n",
    "\n",
    "Text: {text} ->\"\"\"\n",
    "\n",
    "text = \"Protect traditional family values and limit government overreach\"\n",
    "prompt = FEW_SHOT_TEMPLATE.format(text=text)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Response: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D: Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COT_TEMPLATE = \"\"\"Classify the stance and explain your reasoning.\n",
    "\n",
    "Text: {text}\n",
    "\n",
    "Think step-by-step:\n",
    "1. What policy domain is this?\n",
    "2. What values does it express?\n",
    "3. What stance does this suggest?\n",
    "\n",
    "Reasoning:\n",
    "Stance:\"\"\"\n",
    "\n",
    "text = \"Invest heavily in public education and teacher salaries\"\n",
    "prompt = COT_TEMPLATE.format(text=text)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Response:\\n{response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 2: Four Approaches to Structured Outputs\n",
    "\n",
    "Demonstrates the progression from basic to most reliable structured output methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Prompt-Only Formatting (basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt: str) -> str:\n",
    "    \"\"\"Generic LLM call wrapper\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "text = \"We must expand Medicare to cover everyone\"\n",
    "prompt = f\"\"\"\n",
    "Extract fields as JSON and respond ONLY with valid JSON:\n",
    "{{\n",
    "  \"stance\": \"Progressive/Conservative/Centrist\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"reasoning\": \"brief explanation\"\n",
    "}}\n",
    "\n",
    "Input: {text}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    data = json.loads(llm(prompt))\n",
    "    print(\"✓ Successfully parsed JSON\")\n",
    "    print(json.dumps(data, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"✗ Failed to parse: {e}\")\n",
    "    print(f\"Raw output: {llm(prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Few-Shot with Schema + Examples (better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Cut taxes and reduce regulations\"\n",
    "prompt = \"\"\"\n",
    "You output ONLY valid JSON with keys: stance, confidence, reasoning.\n",
    "\n",
    "Example:\n",
    "Input: Expand healthcare access for all\n",
    "Output: {{\"stance\":\"Progressive\",\"confidence\":0.9,\"reasoning\":\"Universal healthcare is progressive policy\"}}\n",
    "\n",
    "Example:\n",
    "Input: Maintain current spending levels\n",
    "Output: {{\"stance\":\"Centrist\",\"confidence\":0.8,\"reasoning\":\"Status quo signals moderate position\"}}\n",
    "\n",
    "Now do the same:\n",
    "Input: {input}\n",
    "Output:\n",
    "\"\"\".format(input=text)\n",
    "\n",
    "data = json.loads(llm(prompt))\n",
    "print(\"✓ Successfully parsed JSON\")\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Provider JSON Mode / Schema (more reliable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for structured output\n",
    "schema = {\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"stance\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"Progressive\", \"Conservative\", \"Centrist\"]\n",
    "    },\n",
    "    \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n",
    "    \"reasoning\": {\"type\": \"string\"}\n",
    "  },\n",
    "  \"required\": [\"stance\", \"confidence\", \"reasoning\"],\n",
    "  \"additionalProperties\": False\n",
    "}\n",
    "\n",
    "text = \"Protect traditional family values\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a political analyst. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Analyze political stance: {text}\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},  # Force JSON mode\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "data = json.loads(response.choices[0].message.content)\n",
    "print(\"✓ JSON mode guarantees valid JSON\")\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 4: Function/Tool Calling (most structured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function schema with typed arguments\n",
    "tools = [{\n",
    "  \"type\": \"function\",\n",
    "  \"function\": {\n",
    "    \"name\": \"analyze_stance\",\n",
    "    \"description\": \"Return structured political stance analysis\",\n",
    "    \"parameters\": {\n",
    "      \"type\": \"object\",\n",
    "      \"properties\": {\n",
    "        \"stance\": {\n",
    "          \"type\": \"string\",\n",
    "          \"enum\": [\"Progressive\", \"Conservative\", \"Centrist\"]\n",
    "        },\n",
    "        \"confidence\": {\"type\": \"number\"},\n",
    "        \"reasoning\": {\"type\": \"string\"}\n",
    "      },\n",
    "      \"required\": [\"stance\", \"confidence\", \"reasoning\"]\n",
    "    }\n",
    "  }\n",
    "}]\n",
    "\n",
    "text = \"Expand social safety nets\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Analyze: {text}\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Extract structured arguments\n",
    "call = response.choices[0].message.tool_calls[0]\n",
    "args = json.loads(call.function.arguments)\n",
    "print(\"✓ Function calling provides strongest guarantees\")\n",
    "print(f\"Function called: {call.function.name}\")\n",
    "print(json.dumps(args, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({\n",
    "    \"Approach\": [\"Prompt-only\", \"Few-shot\", \"JSON mode\", \"Function calling\"],\n",
    "    \"Reliability\": [\"Low\", \"Medium\", \"High\", \"Highest\"],\n",
    "    \"Flexibility\": [\"High\", \"High\", \"Medium\", \"Low\"],\n",
    "    \"Support\": [\"Universal\", \"Universal\", \"Most APIs\", \"OpenAI, Anthropic, Google\"]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n✓ Recommendation: Start with JSON mode (Approach 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 3: Robust JSON Extraction\n",
    "\n",
    "Shows how to reliably extract JSON from models without native JSON mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear instructions for JSON-only output\n",
    "INSTRUCTIONS = (\n",
    "    'Return only a JSON object like this:\\n'\n",
    "    '{\"stance\":\"Progressive|Conservative|Centrist|null\",'\n",
    "    '\"confidence\":0-1,\"reasoning\":\"brief\"}\\n'\n",
    "    'Do not add any extra text.'\n",
    ")\n",
    "\n",
    "def get_labels(client, text, model=\"gpt-4\", max_retries=1):\n",
    "    \"\"\"\n",
    "    Robust JSON extraction with error handling and retry logic\n",
    "    \"\"\"\n",
    "    # 1) Ask for JSON only with low temperature\n",
    "    prompt = f'{INSTRUCTIONS}\\n\\nText: \"{text}\"'\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1  # Low temp for consistency\n",
    "    )\n",
    "    \n",
    "    output = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 2) Try to parse as JSON\n",
    "    try:\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"⚠ Parse failed on first attempt: {e}\")\n",
    "        print(f\"Raw output: {output}\\n\")\n",
    "        \n",
    "        if max_retries > 0:\n",
    "            # 3) One retry asking for just JSON again\n",
    "            fix_prompt = (\n",
    "                \"That was not valid JSON. Please send ONLY the JSON object, \"\n",
    "                \"nothing else. No explanations, no markdown fences.\"\n",
    "            )\n",
    "            \n",
    "            retry_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": output},\n",
    "                    {\"role\": \"user\", \"content\": fix_prompt}\n",
    "                ],\n",
    "                temperature=0.0  # Zero temp for retry\n",
    "            )\n",
    "            \n",
    "            retry_output = retry_response.choices[0].message.content.strip()\n",
    "            print(f\"Retry output: {retry_output}\\n\")\n",
    "            \n",
    "            try:\n",
    "                return json.loads(retry_output)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"✗ Parse failed after retry: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Test cases\n",
    "test_texts = [\n",
    "    \"We must expand Medicare to cover everyone\",\n",
    "    \"Cut taxes and reduce government spending\",\n",
    "    \"Maintain balanced approach to fiscal policy\"\n",
    "]\n",
    "\n",
    "print(\"Testing robust extraction with retry logic:\\n\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"Test {i}: {text}\")\n",
    "    try:\n",
    "        result = get_labels(client, text)\n",
    "        print(f\"✓ Success: {json.dumps(result, indent=2)}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 4: Batch Annotation\n",
    "\n",
    "Efficient batch processing with structured outputs and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "texts = [\n",
    "    \"We need stronger borders and immigration control\",\n",
    "    \"Healthcare is a human right for all\",\n",
    "    \"Balance the budget through moderate tax reform\",\n",
    "    \"Invest in renewable energy infrastructure\",\n",
    "    \"Cut regulations on small businesses\",\n",
    "    \"Expand access to affordable childcare\",\n",
    "    \"Maintain current defense spending levels\",\n",
    "    \"Protect voting rights and access\",\n",
    "    \"Reduce corporate tax rates\",\n",
    "    \"Fund public education and teacher salaries\"\n",
    "]\n",
    "\n",
    "# Template for consistent prompting\n",
    "JSON_TEMPLATE = \"\"\"Analyze this political text: {text}\n",
    "\n",
    "Return JSON with keys:\n",
    "- stance (Progressive/Conservative/Centrist)\n",
    "- confidence (0-1)\n",
    "- reasoning (brief explanation)\n",
    "- policy_domain (e.g., healthcare, economy, education)\"\"\"\n",
    "\n",
    "def annotate_text(text, model=\"gpt-4\", temperature=0):\n",
    "    \"\"\"Annotate a single text with structured output\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are a political analyst. Return valid JSON only.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": JSON_TEMPLATE.format(text=text)}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "def batch_annotate(texts, model=\"gpt-4\", temperature=0):\n",
    "    \"\"\"Annotate multiple texts\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Annotating {len(texts)} texts with {model}...\")\n",
    "    print(f\"Temperature: {temperature}\\n\")\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"[{i}/{len(texts)}] Processing: {text[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            annotation = annotate_text(text, model=model, temperature=temperature)\n",
    "            annotation['text'] = text\n",
    "            annotation['model'] = model\n",
    "            annotation['temperature'] = temperature\n",
    "            annotation['timestamp'] = datetime.now().isoformat()\n",
    "            annotation['success'] = True\n",
    "            annotation['error'] = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "            annotation = {\n",
    "                'text': text,\n",
    "                'model': model,\n",
    "                'temperature': temperature,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'stance': None,\n",
    "                'confidence': None,\n",
    "                'reasoning': None,\n",
    "                'policy_domain': None\n",
    "            }\n",
    "        \n",
    "        results.append(annotation)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ Completed: {df['success'].sum()}/{len(df)} successful\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run batch annotation\n",
    "df = batch_annotate(texts, model=\"gpt-4\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nSummary by stance:\")\n",
    "print(df['stance'].value_counts())\n",
    "\n",
    "print(\"\\nAverage confidence by stance:\")\n",
    "print(df.groupby('stance')['confidence'].mean().round(3))\n",
    "\n",
    "print(\"\\nSample annotations:\")\n",
    "print(df[['text', 'stance', 'confidence', 'policy_domain']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks\n",
    "print(\"\\nQuality Checks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for low confidence predictions\n",
    "low_confidence = df[df['confidence'] < 0.7]\n",
    "print(f\"\\nLow confidence annotations (< 0.7): {len(low_confidence)}\")\n",
    "if len(low_confidence) > 0:\n",
    "    print(low_confidence[['text', 'stance', 'confidence']])\n",
    "\n",
    "# Check for null values\n",
    "nulls = df[df['stance'].isna()]\n",
    "print(f\"\\nMissing stance labels: {len(nulls)}\")\n",
    "\n",
    "print(\"\\n✓ Batch annotation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 5: Mixture of Experts (Ensemble)\n",
    "\n",
    "Multi-model ensemble approach based on Kraft et al. (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stance_score(text, model=\"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Get ideological position score from a model\n",
    "    Returns: float from -1 (progressive) to +1 (conservative)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Rate this text on ideology from -1 (most progressive)\n",
    "to +1 (most conservative). Return only the number.\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return float(response.choices[0].message.content.strip())\n",
    "\n",
    "def ensemble_stance(text, models):\n",
    "    \"\"\"Aggregate stance estimates across multiple models\"\"\"\n",
    "    scores = []\n",
    "    individual = {}\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            score = get_stance_score(text, model=model)\n",
    "            scores.append(score)\n",
    "            individual[model] = score\n",
    "            print(f\"  {model:20}: {score:+.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {model:20}: Error - {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not scores:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"mean\": np.mean(scores),\n",
    "        \"median\": np.median(scores),\n",
    "        \"std\": np.std(scores),\n",
    "        \"min\": np.min(scores),\n",
    "        \"max\": np.max(scores),\n",
    "        \"individual\": individual,\n",
    "        \"n_models\": len(scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Single text with multiple models\n",
    "models = [\"gpt-4\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "text = \"We must protect traditional family values and limit government overreach\"\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Individual model scores:\")\n",
    "\n",
    "result = ensemble_stance(text, models)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nEnsemble results:\")\n",
    "    print(f\"  Mean:      {result['mean']:+.3f}\")\n",
    "    print(f\"  Median:    {result['median']:+.3f}\")\n",
    "    print(f\"  Std dev:   {result['std']:.3f}\")\n",
    "    print(f\"  Range:     [{result['min']:+.3f}, {result['max']:+.3f}]\")\n",
    "    print(f\"  Agreement: {'High' if result['std'] < 0.3 else 'Medium' if result['std'] < 0.6 else 'Low'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Batch analysis with ensemble\n",
    "tweets = [\n",
    "    \"Expand Medicare to cover everyone\",\n",
    "    \"Cut taxes and regulations on businesses\",\n",
    "    \"Protect voting rights and access\",\n",
    "    \"Secure the border and enforce immigration laws\",\n",
    "    \"Invest in public schools and teacher salaries\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"\\nAnalyzing {len(tweets)} texts with ensemble...\\n\")\n",
    "\n",
    "for i, tweet in enumerate(tweets, 1):\n",
    "    print(f\"[{i}/{len(tweets)}] {tweet}\")\n",
    "    ensemble = ensemble_stance(tweet, models)\n",
    "    \n",
    "    if ensemble:\n",
    "        results.append({\n",
    "            \"text\": tweet,\n",
    "            \"position\": ensemble[\"mean\"],\n",
    "            \"uncertainty\": ensemble[\"std\"],\n",
    "            \"n_models\": ensemble[\"n_models\"]\n",
    "        })\n",
    "    print()\n",
    "\n",
    "ensemble_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nPosition scores (negative = progressive, positive = conservative):\")\n",
    "print(ensemble_df[['text', 'position', 'uncertainty']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 6: Validation and Logging\n",
    "\n",
    "Best practices for reproducibility and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_with_logging(text, model=\"gpt-4-0613\", temperature=0, seed=42):\n",
    "    \"\"\"Annotate text with complete logging for reproducibility\"\"\"\n",
    "    prompt = f\"\"\"Analyze political stance: {text}\n",
    "\n",
    "Return JSON with stance, confidence, reasoning.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a political analyst. Return valid JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=temperature,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Parse result\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    # Create comprehensive log entry\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"text\": text,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"seed\": seed,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": result,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        },\n",
    "        \"finish_reason\": response.choices[0].finish_reason\n",
    "    }\n",
    "    \n",
    "    return result, log_entry\n",
    "\n",
    "text = \"Expand social safety nets and increase minimum wage\"\n",
    "result, log = annotate_with_logging(text)\n",
    "\n",
    "print(f\"✓ Annotated: {text}\")\n",
    "print(f\"  Stance: {result.get('stance')}\")\n",
    "print(f\"\\nLog entry (partial):\")\n",
    "print(json.dumps({k: log[k] for k in ['timestamp', 'model', 'temperature', 'seed']}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fingerprinting (Detect API Drift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fingerprint(model, test_prompts, temperature=0, seed=42):\n",
    "    \"\"\"Create fingerprint to detect if model behavior has changed\"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            seed=seed\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "    \n",
    "    # Hash concatenated responses\n",
    "    fingerprint = hashlib.sha256(\n",
    "        \"\".join(responses).encode()\n",
    "    ).hexdigest()\n",
    "    \n",
    "    return fingerprint\n",
    "\n",
    "# Create test set for fingerprinting\n",
    "test_prompts = [\n",
    "    \"Classify: 'Cut taxes for businesses' - Progressive/Conservative/Centrist\",\n",
    "    \"Classify: 'Expand healthcare coverage' - Progressive/Conservative/Centrist\",\n",
    "    \"Classify: 'Balanced budget amendment' - Progressive/Conservative/Centrist\"\n",
    "]\n",
    "\n",
    "fingerprint = model_fingerprint(\"gpt-4-0613\", test_prompts)\n",
    "print(f\"Model fingerprint: {fingerprint[:16]}...\")\n",
    "print(\"\\n✓ Save this fingerprint and check periodically for drift\")\n",
    "print(\"✓ If fingerprint changes, model behavior has changed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate human and LLM labels for validation\n",
    "human_labels = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])  # 0=Prog, 1=Centrist, 2=Cons\n",
    "llm_labels = np.array([0, 1, 1, 0, 1, 2, 0, 2, 2, 0])\n",
    "\n",
    "# 1. Human-LLM Agreement (Cohen's Kappa)\n",
    "kappa = cohen_kappa_score(human_labels, llm_labels)\n",
    "accuracy = accuracy_score(human_labels, llm_labels)\n",
    "\n",
    "print(\"Human-LLM Agreement:\\n\")\n",
    "print(f\"Cohen's κ: {kappa:.3f}\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "\n",
    "if kappa > 0.80:\n",
    "    print(\"✓ Substantial agreement\")\n",
    "elif kappa > 0.60:\n",
    "    print(\"⚠ Moderate agreement - consider refinement\")\n",
    "else:\n",
    "    print(\"✗ Low agreement - significant issues\")\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(human_labels, llm_labels)\n",
    "print(\"\\nConfusion Matrix (rows=human, cols=LLM):\")\n",
    "print(\"              Prog  Cent  Cons\")\n",
    "for i, label in enumerate([\"Progressive\", \"Centrist\", \"Conservative\"]):\n",
    "    print(f\"{label:12}  {cm[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promptbook Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptbook = {\n",
    "    \"task\": \"political_stance_classification\",\n",
    "    \"date_created\": \"2024-10-08\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"name\": \"gpt-4-0613\",\n",
    "            \"type\": \"api\",\n",
    "            \"provider\": \"openai\",\n",
    "            \"temperature\": 0,\n",
    "            \"seed\": 42,\n",
    "            \"response_format\": \"json\"\n",
    "        }\n",
    "    ],\n",
    "    \"prompt_template\": \"Analyze political stance: {text}\\n\\nReturn JSON with stance, confidence, reasoning.\",\n",
    "    \"output_schema\": {\n",
    "        \"stance\": [\"Progressive\", \"Conservative\", \"Centrist\"],\n",
    "        \"confidence\": \"float (0-1)\",\n",
    "        \"reasoning\": \"string\"\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"method\": \"human_comparison\",\n",
    "        \"sample_size\": 200,\n",
    "        \"cohen_kappa\": 0.78,\n",
    "        \"accuracy\": 0.82,\n",
    "        \"validation_date\": \"2024-10-08\"\n",
    "    },\n",
    "    \"fingerprint\": fingerprint,\n",
    "    \"notes\": \"Validated on US political tweets. Low confidence (<0.7) texts manually reviewed.\"\n",
    "}\n",
    "\n",
    "print(\"✓ Promptbook created\")\n",
    "print(\"\\nPromptbook includes:\")\n",
    "for key in promptbook.keys():\n",
    "    print(f\"  • {key}\")\n",
    "\n",
    "print(\"\\n\" + json.dumps(promptbook, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checklist = [\n",
    "    (\"☐ Pin model versions\", \"Use specific snapshots (gpt-4-0613, not gpt-4)\"),\n",
    "    (\"☐ Set temperature to 0\", \"For deterministic outputs\"),\n",
    "    (\"☐ Use seed parameter\", \"When supported by API\"),\n",
    "    (\"☐ Log everything\", \"Prompts, responses, settings, timestamps\"),\n",
    "    (\"☐ Create promptbook\", \"Document complete annotation pipeline\"),\n",
    "    (\"☐ Validate against humans\", \"Cohen's κ > 0.80 target\"),\n",
    "    (\"☐ Test-retest reliability\", \"Check consistency over time\"),\n",
    "    (\"☐ Model fingerprinting\", \"Detect API drift\"),\n",
    "    (\"☐ Share code & configs\", \"Enable exact replication\"),\n",
    "    (\"☐ Use open models when possible\", \"Fixed weights = perfect reproducibility\")\n",
    "]\n",
    "\n",
    "print(\"Reproducibility Checklist:\\n\")\n",
    "for item, description in checklist:\n",
    "    print(f\"{item:30} - {description}\")\n",
    "\n",
    "print(\"\\n✓ Following these practices enables credible, replicable research\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Keep prompts simple**: f-strings and templates are enough\n",
    "2. **Force structured output**: Use JSON mode (Approach 3) or function calling (Approach 4)\n",
    "3. **Log everything**: Prompts, responses, settings, timestamps\n",
    "4. **Validate**: Cohen's κ > 0.80 with human labels\n",
    "5. **Detect drift**: Model fingerprinting for API changes\n",
    "6. **Ensemble**: Multiple models > single model for robustness\n",
    "7. **Document**: Create promptbooks for replication\n",
    "\n",
    "## Recommended Workflow\n",
    "\n",
    "1. Start with JSON mode zero-shot on validation slice\n",
    "2. If needed, fine-tune open model (LoRA) with 100-1000 labels\n",
    "3. Add replication harness: fixed params, logs, regression test\n",
    "4. Report human-LLM κ, test-retest, and promptbook\n",
    "\n",
    "## Resources\n",
    "\n",
    "- Kraft et al. (2024): Mixture of Experts for Ideological Scaling\n",
    "- Alizadeh et al. (2024): Open-Source LLMs for Text Classification\n",
    "- Heseltine & Clemm von Hohenberg (2024): GPT-4 Accuracy on Political Texts\n",
    "- BPS Replication Guide (2025): Standards for LLM Reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

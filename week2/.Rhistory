# ============================================================
# Word2Vec (SGNS) on Jane Austen (janeaustenr)
# ============================================================
# --- one-time installs (uncomment if needed) ---
# install.packages(c("torch","janeaustenr","ggplot2","ggrepel","coro"))
# torch::install_torch()
library(torch)
library(janeaustenr)
library(ggplot2)
library(ggrepel)
library(coro)
set.seed(42)
# ---------------------------
# 1) Load & tokenize Austen
# ---------------------------
austen_txt  <- janeaustenr::austen_books()
austen_txt <- paste(austen_txt$text, collapse = " ")
# lowercase + basic tokenization (keep a–z only)
tokens <- tolower(austen_txt)
tokens <- gsub("[^a-z]+", " ", tokens)
tokens <- unlist(strsplit(tokens, "\\s+"))
tokens <- tokens[tokens != ""]
# ---------------------------
# 2) Build vocab & map to ids
# ---------------------------
min_count <- 5        # drop very rare words to shrink vocab
tab <- sort(table(tokens), decreasing = TRUE)
vocab <- names(tab[tab >= min_count])
itos <- c("<unk>", vocab)                      # index -> token
stoi <- setNames(seq_along(itos), itos)       # token -> index (1-based)
# stoi implicit: position in 'itos'
ids <- match(tokens, itos)   # NA if token not found
ids[is.na(ids)] <- 1         # 1 is "<unk>"
V <- length(itos)
cat("Vocab size (with <unk>):", V, "\n")
# 3) Build Skip-gram (center, context) pairs
# ---------------------------------------
window <- 4   # typical 2–5
pairs <- list()
N <- length(ids)
for (i in seq_len(N)) {
c_id <- ids[i]
left  <- max(1, i - window)
right <- min(N, i + window)
for (j in left:right) {
if (j == i) next
o_id <- ids[j]
pairs[[length(pairs) + 1L]] <- c(c_id, o_id)
}
}
pairs <- do.call(rbind, pairs)
colnames(pairs) <- c("center","context")
num_pairs <- nrow(pairs)
cat("Num training pairs:", format(num_pairs, big.mark=","), "\n")
pairs
# ---------------------------------------
# 4) Negative sampling distribution p(w) ∝ f(w)^0.75
# ---------------------------------------
counts <- numeric(V); names(counts) <- itos
counts[names(tab)] <- as.numeric(tab)
neg_probs <- counts ^ 0.75
neg_probs <- neg_probs / sum(neg_probs)
neg_probs_t <- torch_tensor(neg_probs, dtype = torch_float())
neg_probs
torch::install_torch()         # installs the Lantern runtime (CPU by default)
# ============================================================
# Word2Vec (SGNS) on Jane Austen (janeaustenr)
# ============================================================
# --- one-time installs (uncomment if needed) ---
# install.packages(c("torch","janeaustenr","ggplot2","ggrepel","coro"))
# torch::install_torch()
library(torch)
library(janeaustenr)
library(ggplot2)
library(ggrepel)
library(coro)
set.seed(42)
# ---------------------------
# 1) Load & tokenize Austen
# ---------------------------
austen_txt  <- janeaustenr::austen_books()
austen_txt <- paste(austen_txt$text, collapse = " ")
# lowercase + basic tokenization (keep a–z only)
tokens <- tolower(austen_txt)
tokens <- gsub("[^a-z]+", " ", tokens)
tokens <- unlist(strsplit(tokens, "\\s+"))
tokens <- tokens[tokens != ""]
# ---------------------------
# 2) Build vocab & map to ids
# ---------------------------
min_count <- 5        # drop very rare words to shrink vocab
tab <- sort(table(tokens), decreasing = TRUE)
vocab <- names(tab[tab >= min_count])
itos <- c("<unk>", vocab)                      # index -> token
stoi <- setNames(seq_along(itos), itos)       # token -> index (1-based)
# stoi implicit: position in 'itos'
ids <- match(tokens, itos)   # NA if token not found
ids[is.na(ids)] <- 1         # 1 is "<unk>"
V <- length(itos)
cat("Vocab size (with <unk>):", V, "\n")
# ---------------------------------------
# 3) Build Skip-gram (center, context) pairs
# ---------------------------------------
window <- 4   # typical 2–5
pairs <- list()
N <- length(ids)
for (i in seq_len(N)) {
c_id <- ids[i]
left  <- max(1, i - window)
right <- min(N, i + window)
for (j in left:right) {
if (j == i) next
o_id <- ids[j]
pairs[[length(pairs) + 1L]] <- c(c_id, o_id)
}
}
pairs <- do.call(rbind, pairs)
colnames(pairs) <- c("center","context")
num_pairs <- nrow(pairs)
cat("Num training pairs:", format(num_pairs, big.mark=","), "\n")
# ---------------------------------------
# 4) Negative sampling distribution p(w) ∝ f(w)^0.75
# ---------------------------------------
counts <- numeric(V); names(counts) <- itos
counts[names(tab)] <- as.numeric(tab)
neg_probs <- counts ^ 0.75
neg_probs <- neg_probs / sum(neg_probs)
neg_probs_t <- torch_tensor(neg_probs, dtype = torch_float())
# ---------------------------------------
# 5) Dataset & DataLoader
# ---------------------------------------
ds <- dataset(
name = "sgns_pairs",
initialize = function(pairs) self$pairs <- torch_tensor(pairs, dtype = torch_long()),
.getitem = function(i) self$pairs[i, ],
.length  = function() self$pairs$size()[1]
)(pairs)
batch_size <- 1024     # increase if you have RAM; SGNS is robust
dl <- dataloader(ds, batch_size = batch_size, shuffle = TRUE, num_workers = 0)
# ---------------------------------------
# 6) Model params + SGNS loss
# ---------------------------------------
d <- 100               # embedding dimension (100–300 is common)
K <- 5                 # negatives per positive example
lr <- 2e-3             # learning rate
epochs <- 5            # bump to 10–15 for better quality
E_in  <- nn_embedding(num_embeddings = V, embedding_dim = d)
E_out <- nn_embedding(num_embeddings = V, embedding_dim = d)
opt <- optim_adam(params = c(E_in$parameters, E_out$parameters), lr = lr)
sgns_step <- function(batch) {
center <- batch[ ,1]              # [B]
pos    <- batch[ ,2]              # [B]
B <- center$size()[1]
# draw negatives: [B, K]
neg <- torch_multinomial(neg_probs_t, num_samples = as.integer(B * K), replacement = TRUE)$
view(c(B, K))
# lookups
ec <- E_in(center)                # [B, d]
eo <- E_out(pos)                  # [B, d]
en <- E_out(neg)                  # [B, K, d]
# logits
pos_logit <- torch_sum(ec * eo, dim = 2)                   # [B]
neg_logit <- torch_einsum("bd,bkd->bk", ec, en)            # [B, K]
# SGNS loss
loss <- torch_mean(torch_softplus(-pos_logit)) +           # -log σ(pos)
torch_mean(torch_softplus(neg_logit))              # -Σ log σ(-neg)
loss
}
# ---------------------------------------
# 7) Training loop
# ---------------------------------------
for (epoch in 1:epochs) {
running <- 0; nb <- 0
coro::loop(for (batch in dl) {
opt$zero_grad()
loss <- sgns_step(batch)
loss$backward()
opt$step()
running <- running + as.numeric(loss$item()); nb <- nb + 1
})
cat(sprintf("epoch %02d | loss %.4f\n", epoch, running/nb))
}
# sanity checks on ids and pairs
stopifnot(all(!is.na(ids)))
stopifnot(all(ids >= 1 & ids <= V))
rng_pairs <- range(pairs)
cat("pairs range:", rng_pairs[1], "to", rng_pairs[2], " (V =", V, ")\n")
stopifnot(all(pairs >= 1 & pairs <= V))
# ---------------------------------------
# 6) Model params + SGNS loss (robust)
# ---------------------------------------
d <- 100
K <- 5
lr <- 2e-3
epochs <- 5
E_in  <- nn_embedding(num_embeddings = V, embedding_dim = d)
E_out <- nn_embedding(num_embeddings = V, embedding_dim = d)
opt <- optim_adam(params = c(E_in$parameters, E_out$parameters), lr = lr)
# OPTIONAL: drop training pairs that involve <unk> (id 1)
keep <- pairs[,1] != 1 & pairs[,2] != 1
pairs <- pairs[keep, , drop = FALSE]
ds <- dataset(
name = "sgns_pairs",
initialize = function(pairs) self$pairs <- torch_tensor(pairs, dtype = torch_long()),
.getitem = function(i) self$pairs[i, ],
.length  = function() self$pairs$size()[1]
)(pairs)
dl <- dataloader(ds, batch_size = 1024, shuffle = TRUE, num_workers = 0)
# Base-R negative sampler -> strictly 1..V
draw_negs_base <- function(batch_size, K, probs) {
matrix(
sample.int(length(probs), size = batch_size*K, replace = TRUE, prob = probs),
nrow = batch_size, byrow = TRUE
)
}
sgns_step <- function(batch) {
center <- batch[,1]$to(dtype = torch_long())
pos    <- batch[,2]$to(dtype = torch_long())
Vloc <- E_in$num_embeddings
# hard guards for indices
if (as.logical(torch_any(center < 1)$item()) || as.logical(torch_any(center > Vloc)$item()))
stop(sprintf("[center] min=%d max=%d V=%d",
as.integer(center$min()$item()), as.integer(center$max()$item()), Vloc))
if (as.logical(torch_any(pos < 1)$item()) || as.logical(torch_any(pos > Vloc)$item()))
stop(sprintf("[pos] min=%d max=%d V=%d",
as.integer(pos$min()$item()), as.integer(pos$max()$item()), Vloc))
B <- center$size()[1]
# negatives from base R (1..V), then to torch_long
neg_ids <- draw_negs_base(B, K, neg_probs)
neg <- torch_tensor(neg_ids, dtype = torch_long())
# another guard
if (as.logical(torch_any(neg < 1)$item()) || as.logical(torch_any(neg > Vloc)$item()))
stop(sprintf("[neg] min=%d max=%d V=%d",
as.integer(neg$min()$item()), as.integer(neg$max()$item()), Vloc))
ec <- E_in(center)                  # [B, d]
eo <- E_out(pos)                    # [B, d]
en <- E_out(neg)                    # [B, K, d]
pos_logit <- torch_sum(ec * eo, dim = 2)               # [B]
neg_logit <- torch_einsum("bd,bkd->bk", ec, en)        # [B, K]
torch_mean(torch_softplus(-pos_logit)) + torch_mean(torch_softplus(neg_logit))
}
# ---------------------------------------
# 7) Training loop (unchanged)
# ---------------------------------------
for (epoch in 1:epochs) {
running <- 0; nb <- 0
coro::loop(for (batch in dl) {
opt$zero_grad()
loss <- sgns_step(batch)
loss$backward()
opt$step()
running <- running + as.numeric(loss$item()); nb <- nb + 1
})
cat(sprintf("epoch %02d | loss %.4f\n", epoch, running/nb))
}
#!/usr/bin/env Rscript
library(tidyverse)
library(scales)
library(readr)
library(forcats)
library(tidytext)
# -------------------------------------------------
# 1) Paths
# -------------------------------------------------
DATA_ROOT  <- "data/output/test2_atp"
OPENAI_DIR <- NULL # file.path(DATA_ROOT, "openai")
PLOT_DIR <- "plots"
dir.create(PLOT_DIR, recursive = TRUE, showWarnings = FALSE)
# Items to use (order locked); reduce immigration is reversed into liberal scale
TOPICS_KEEP <- c("reduce immigration", "abortion", "same-sex marriage", "gun control")
# -------------------------------------------------
# 2) Read OpenRouter (exclude openai subdir) + OpenAI CSVs
# -------------------------------------------------
read_safe <- function(f) tryCatch(readr::read_csv(f, show_col_types = FALSE), error = function(e) NULL)
# Vectorized education parser
edu_from_text <- function(s) {
s <- tolower(ifelse(is.na(s), "", as.character(s)))
dplyr::case_when(
stringr::str_detect(s, "high school or less|less than high school|high school graduate") ~ "hs or less",
stringr::str_detect(s, "some college|associate degree|college degree|college graduate|postgraduate") ~ "some college plus",
TRUE ~ NA_character_
)
}
# OpenRouter
or_files <- list.files(
DATA_ROOT, pattern = "^openrouter_.*\\.csv$", full.names = TRUE, recursive = TRUE
) |>
(\(x) x[!grepl("/openai/", x)])()
or_df <- purrr::map_df(or_files, function(f) {
dat <- read_safe(f)
if (is.null(dat) || nrow(dat) == 0) return(tibble())
dat %>%
mutate(
provider   = if_else(str_detect(model, "/"), sub("/.*$", "", model), "openrouter"),
source     = "openrouter",
persona_id = condition,
condition  = edu_from_text(persona_text_preview),  # now vectorized
dimension  = "education"
) %>%
select(any_of(c(
"model","provider","dimension","ideology","condition",
"topic","run","likert","polarity","source",
"persona_id","persona_text_preview"
)))
})
if(!is.null(OPENAI_DIR)){
# OpenAI
oa_files <- list.files(OPENAI_DIR, pattern = "\\.csv$", full.names = TRUE, recursive = FALSE)
oa_df <- purrr::map_df(oa_files, function(f) {
dat <- read_safe(f)
if (is.null(dat) || nrow(dat) == 0) return(tibble())
dat %>%
mutate(
provider  = "openai",
source    = "openai",
persona_id = condition,
condition = edu_from_text(persona_text_preview),
dimension = "education"
) %>%
select(any_of(c(
"model","provider","dimension","ideology","condition",
"topic","run","likert","polarity","source",
"persona_id","persona_text_preview"
)))
})
}
raw <- bind_rows(or_df, oa_df) %>%
distinct()
#!/usr/bin/env Rscript
library(tidyverse)
library(scales)
library(readr)
library(forcats)
library(tidytext)
# -------------------------------------------------
# 1) Paths
# -------------------------------------------------
DATA_ROOT  <- "data/output/test2_atp"
OPENAI_DIR <- NULL # file.path(DATA_ROOT, "openai")
PLOT_DIR <- "plots"
dir.create(PLOT_DIR, recursive = TRUE, showWarnings = FALSE)
# Items to use (order locked); reduce immigration is reversed into liberal scale
TOPICS_KEEP <- c("reduce immigration", "abortion", "same-sex marriage", "gun control")
# -------------------------------------------------
# 2) Read OpenRouter (exclude openai subdir) + OpenAI CSVs
# -------------------------------------------------
read_safe <- function(f) tryCatch(readr::read_csv(f, show_col_types = FALSE), error = function(e) NULL)
# Vectorized education parser
edu_from_text <- function(s) {
s <- tolower(ifelse(is.na(s), "", as.character(s)))
dplyr::case_when(
stringr::str_detect(s, "high school or less|less than high school|high school graduate") ~ "hs or less",
stringr::str_detect(s, "some college|associate degree|college degree|college graduate|postgraduate") ~ "some college plus",
TRUE ~ NA_character_
)
}
# OpenRouter
or_files <- list.files(
DATA_ROOT, pattern = "^openrouter_.*\\.csv$", full.names = TRUE, recursive = TRUE
) |>
(\(x) x[!grepl("/openai/", x)])()
or_df <- purrr::map_df(or_files, function(f) {
dat <- read_safe(f)
if (is.null(dat) || nrow(dat) == 0) return(tibble())
dat %>%
mutate(
provider   = if_else(str_detect(model, "/"), sub("/.*$", "", model), "openrouter"),
source     = "openrouter",
persona_id = condition,
condition  = edu_from_text(persona_text_preview),  # now vectorized
dimension  = "education"
) %>%
select(any_of(c(
"model","provider","dimension","ideology","condition",
"topic","run","likert","polarity","source",
"persona_id","persona_text_preview"
)))
})
if(!is.null(OPENAI_DIR)){
# OpenAI
oa_files <- list.files(OPENAI_DIR, pattern = "\\.csv$", full.names = TRUE, recursive = FALSE)
oa_df <- purrr::map_df(oa_files, function(f) {
dat <- read_safe(f)
if (is.null(dat) || nrow(dat) == 0) return(tibble())
dat %>%
mutate(
provider  = "openai",
source    = "openai",
persona_id = condition,
condition = edu_from_text(persona_text_preview),
dimension = "education"
) %>%
select(any_of(c(
"model","provider","dimension","ideology","condition",
"topic","run","likert","polarity","source",
"persona_id","persona_text_preview"
)))
})
}
raw <- bind_rows(or_df, oa_df) %>%
distinct()
#!/usr/bin/env Rscript
library(tidyverse)
library(scales)
library(readr)
library(forcats)
library(tidytext)
# -------------------------------------------------
# 1) Paths
# -------------------------------------------------
DATA_ROOT  <- "data/output/test2_atp"
OPENAI_DIR <- NULL # file.path(DATA_ROOT, "openai")
PLOT_DIR <- "plots"
dir.create(PLOT_DIR, recursive = TRUE, showWarnings = FALSE)
# Items to use (order locked); reduce immigration is reversed into liberal scale
TOPICS_KEEP <- c("reduce immigration", "abortion", "same-sex marriage", "gun control")
# -------------------------------------------------
# 2) Read OpenRouter (exclude openai subdir) + OpenAI CSVs
# -------------------------------------------------
read_safe <- function(f) tryCatch(readr::read_csv(f, show_col_types = FALSE), error = function(e) NULL)
# Vectorized education parser
edu_from_text <- function(s) {
s <- tolower(ifelse(is.na(s), "", as.character(s)))
dplyr::case_when(
stringr::str_detect(s, "high school or less|less than high school|high school graduate") ~ "hs or less",
stringr::str_detect(s, "some college|associate degree|college degree|college graduate|postgraduate") ~ "some college plus",
TRUE ~ NA_character_
)
}
# OpenRouter
or_files <- list.files(
DATA_ROOT, pattern = "^openrouter_.*\\.csv$", full.names = TRUE, recursive = TRUE
) |>
(\(x) x[!grepl("/openai/", x)])()
or_files <- list.files(
DATA_ROOT, pattern = "^openrouter_.*\\.csv$", full.names = TRUE, recursive = TRUE
)
#!/usr/bin/env Rscript
library(tidyverse)
library(scales)
library(readr)
library(forcats)
library(tidytext)
# -------------------------------------------------
# 1) Paths
# -------------------------------------------------
DATA_ROOT  <- "data/output/test2_atp/"
OPENAI_DIR <- NULL # file.path(DATA_ROOT, "openai")
PLOT_DIR <- "plots"
dir.create(PLOT_DIR, recursive = TRUE, showWarnings = FALSE)
# Items to use (order locked); reduce immigration is reversed into liberal scale
TOPICS_KEEP <- c("reduce immigration", "abortion", "same-sex marriage", "gun control")
# -------------------------------------------------
# 2) Read OpenRouter (exclude openai subdir) + OpenAI CSVs
# -------------------------------------------------
read_safe <- function(f) tryCatch(readr::read_csv(f, show_col_types = FALSE), error = function(e) NULL)
# Vectorized education parser
edu_from_text <- function(s) {
s <- tolower(ifelse(is.na(s), "", as.character(s)))
dplyr::case_when(
stringr::str_detect(s, "high school or less|less than high school|high school graduate") ~ "hs or less",
stringr::str_detect(s, "some college|associate degree|college degree|college graduate|postgraduate") ~ "some college plus",
TRUE ~ NA_character_
)
}
# OpenRouter
or_files <- list.files(
DATA_ROOT, pattern = "^openrouter_.*\\.csv$", full.names = TRUE, recursive = TRUE
) |>
(\(x) x[!grepl("/openai/", x)])()
or_df <- purrr::map_df(or_files, function(f) {
dat <- read_safe(f)
if (is.null(dat) || nrow(dat) == 0) return(tibble())
dat %>%
mutate(
provider   = if_else(str_detect(model, "/"), sub("/.*$", "", model), "openrouter"),
source     = "openrouter",
persona_id = condition,
condition  = edu_from_text(persona_text_preview),  # now vectorized
dimension  = "education"
) %>%
select(any_of(c(
"model","provider","dimension","ideology","condition",
"topic","run","likert","polarity","source",
"persona_id","persona_text_preview"
)))
})
if(!is.null(OPENAI_DIR)){
# OpenAI
oa_files <- list.files(OPENAI_DIR, pattern = "\\.csv$", full.names = TRUE, recursive = FALSE)
oa_df <- purrr::map_df(oa_files, function(f) {
dat <- read_safe(f)
if (is.null(dat) || nrow(dat) == 0) return(tibble())
dat %>%
mutate(
provider  = "openai",
source    = "openai",
persona_id = condition,
condition = edu_from_text(persona_text_preview),
dimension = "education"
) %>%
select(any_of(c(
"model","provider","dimension","ideology","condition",
"topic","run","likert","polarity","source",
"persona_id","persona_text_preview"
)))
})
}
raw <- bind_rows(or_df, oa_df) %>%
distinct()
---
## Applications

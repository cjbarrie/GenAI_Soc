{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qualitative Coding with LLMs\n\nThis notebook demonstrates how to use Large Language Models for qualitative data analysis tasks like thematic coding, code condensation, and reflexive analysis.\n\n## Learning Objectives\n\n- Perform **inductive thematic analysis** with LLMs on open-ended text\n- Use **structured outputs** (JSON mode) for consistent coding results\n- Apply **code condensation** strategies to reduce redundant themes\n- Implement **reflexive coding** by having the LLM challenge its own interpretations\n- Handle **long documents** (interview transcripts) through text chunking\n- Compare thematic coding results from different LLM approaches\n\n## Setup\n\n### Running in Google Colab\n1. Upload this notebook to Google Colab\n2. Run the installation cell below\n3. You'll be prompted to enter your OpenAI API key\n\n### Running Locally\n1. Install requirements: `pip install openai scikit-learn pandas numpy`\n2. Set environment variable: `export OPENAI_API_KEY=\"your-key-here\"`\n3. Run notebook with Jupyter: `jupyter notebook week5_qualitative_coding_colab.ipynb`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -q \"openai>=1.40.0\" scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from sklearn.cluster import KMeans\n",
    "import getpass\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# For Colab: you'll be prompted to enter it\n",
    "# For local: set OPENAI_API_KEY environment variable\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()  # reads OPENAI_API_KEY from environment\n",
    "\n",
    "print(\"âœ“ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nSets up the environment for qualitative coding with LLMs:\n\n**Key libraries:**\n- **`openai`**: Official client for OpenAI API (GPT-4, GPT-3.5)\n- **`sklearn`**: Machine learning library (we'll use clustering for theme aggregation)\n- **`pandas`**: Data manipulation for tabular results\n- **`getpass`**: Secure API key input (won't display in output)\n\n**API Key setup:**\n- In Colab: You'll be prompted to paste your key\n- Locally: Set `export OPENAI_API_KEY=\"sk-...\"` before running\n- Security: Never hardcode keys or commit them to git\n\n**Why OpenAI vs other models:**\n- GPT-4o-mini: Best balance of cost and quality for coding (~$0.15 per 1M tokens)\n- Can switch to Anthropic Claude or open-source models with minor code changes\n\n**Expected output:** \"âœ“ Setup complete!\" means you're ready to make API calls.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Inductive Thematic Analysis\n",
    "\n",
    "In qualitative research, **inductive thematic analysis** means deriving themes directly from the data rather than applying pre-defined categories. We'll ask an LLM to:\n",
    "\n",
    "1. Read a collection of open-ended responses\n",
    "2. Identify 3-6 recurring themes\n",
    "3. Assign each text to one or more themes\n",
    "4. Provide justifications using quotes from the data\n",
    "\n",
    "We'll use **JSON mode** to get structured, parseable output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: short survey responses about community concerns\n",
    "texts = [\n",
    "    \"I worry about rising rent and housing costs in my city.\",\n",
    "    \"Public transit has improved, but buses are still unreliable.\",\n",
    "    \"I love my neighborhood's community garden and local markets.\",\n",
    "    \"Healthcare appointments take months to schedule.\",\n",
    "    \"Street lighting got better and I feel safer walking at night.\",\n",
    "    \"Childcare is unaffordable; I had to reduce my hours at work.\",\n",
    "    \"The new bike lanes are great, but drivers don't respect them.\",\n",
    "    \"Utility bills have gone up a lot this year.\",\n",
    "    \"Local library events helped me meet new neighbors.\",\n",
    "    \"The wait times at the public clinic are frustrating.\"\n",
    "]\n",
    "\n",
    "print(f\"Dataset: {len(texts)} responses about community life\\n\")\n",
    "for i, text in enumerate(texts[:3], 1):\n",
    "    print(f\"{i}. {text}\")\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nThis is the core of LLM-assisted inductive thematic analysis:\n\n**Prompt structure:**\n1. **System message:** Defines the LLM's role and output format\n   - \"Return valid JSON only\" ensures parseable output\n   - Specifies what keys to include (themes, assignments)\n2. **User message:** Contains the actual task and data\n   - Wrapped in JSON for clarity\n   - Specifies desired number of themes (3-6)\n\n**Key parameter: `response_format={\"type\": \"json_object\"}`**\n- Forces valid JSON output (won't return prose)\n- Still need to request JSON in the prompt\n- Available in GPT-4 and GPT-3.5-turbo models\n\n**Temperature choice: 0.3**\n- Lower than creative tasks (0.7-1.0)\n- Ensures more consistent theme identification\n- For maximum reproducibility, use 0.0 (but may be too rigid)\n\n**How to adapt this:**\n- Change `n_themes_range` to guide theme granularity\n- Add domain-specific instructions (e.g., \"focus on economic themes\")\n- Include codebook examples for few-shot learning\n\n**Expected output:** A JSON object with structured themes and text-to-theme assignments.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System instructions for the LLM\n",
    "system_instructions = \"\"\"You are a careful qualitative analyst.\n",
    "Return valid JSON only. Create 3â€“6 concise themes with clear names and\n",
    "1â€“2 sentence definitions. Then assign each input text an array of theme names\n",
    "(best matches), and include a short justification using quotes when possible.\n",
    "If uncertain, allow an empty array. Keys: themes, assignments.\"\"\"\n",
    "\n",
    "# User prompt with the task and data\n",
    "user_prompt = {\n",
    "    \"task\": \"Inductive theming of short survey responses\",\n",
    "    \"n_themes_range\": \"3-6\",\n",
    "    \"texts\": texts\n",
    "}\n",
    "\n",
    "# Make API call with JSON mode\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_instructions},\n",
    "        {\"role\": \"user\", \"content\": json.dumps(user_prompt)}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},  # Force JSON output\n",
    "    temperature=0.3  # Low temperature for more consistent coding\n",
    ")\n",
    "\n",
    "result = json.loads(response.choices[0].message.content)\n",
    "print(\"âœ“ Thematic analysis complete\")\n",
    "print(f\"Keys in response: {list(result.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nParses and displays the LLM's theme assignments - with robust error handling:\n\n**Why we need `get_text_index` function:**\n- LLMs may return results in different formats\n- Sometimes returns `\"idx\": 0`, sometimes `\"text\": \"full text here\"`\n- This function handles multiple possible response structures\n\n**Parsing strategies:**\n1. **Check for explicit index keys:** `idx`, `index`, `i`, `text_id`\n2. **Match by text content:** If no index, try to find the text in original list\n3. **Substring matching:** Handle partial quotes or truncated text\n\n**Note on variable key names:**\n- LLMs may return different key names (e.g., \"themes\" vs \"labels\" vs \"categories\")\n- The code handles this by checking multiple possible keys: `assignment.get(\"themes\", assignment.get(\"labels\", assignment.get(\"categories\", [])))`\n- This flexibility makes the code robust to LLM output variations\n\n**Why this matters:**\n- Makes code robust to LLM output variations\n- Different models or temperature settings may format differently\n- Always validate LLM outputs before trusting them\n\n**How to use:**\n- Run as-is for most cases\n- If your LLM uses different keys, add them to the function\n- For production, add logging to track which matching strategy succeeded\n\n**Expected output:** Human-readable list showing which texts got which theme labels, with justifications.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the themes identified by the LLM\n",
    "print(\"\\n=== IDENTIFIED THEMES ===\\n\")\n",
    "\n",
    "themes = result.get(\"themes\", [])\n",
    "for i, theme in enumerate(themes, 1):\n",
    "    name = theme.get(\"name\", \"Unnamed theme\")\n",
    "    definition = theme.get(\"definition\", \"No definition provided\")\n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   {definition}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display assignments of texts to themes\n",
    "def get_text_index(assignment, texts):\n",
    "    \"\"\"Extract text index from assignment, handling various response formats\"\"\"\n",
    "    # Try explicit index keys\n",
    "    for key in (\"idx\", \"index\", \"i\", \"text_id\"):\n",
    "        if key in assignment:\n",
    "            return int(assignment[key])\n",
    "    \n",
    "    # Try to match by text content\n",
    "    if \"text\" in assignment:\n",
    "        text_snippet = assignment[\"text\"]\n",
    "        # Exact match\n",
    "        if text_snippet in texts:\n",
    "            return texts.index(text_snippet)\n",
    "        # Substring match\n",
    "        for j, t in enumerate(texts):\n",
    "            if text_snippet[:30] in t:\n",
    "                return j\n",
    "    return None\n",
    "\n",
    "print(\"\\n=== THEME ASSIGNMENTS ===\\n\")\n",
    "\n",
    "assignments = result.get(\"assignments\", [])\n",
    "for assignment in assignments:\n",
    "    idx = get_text_index(assignment, texts)\n",
    "    \n",
    "    # Get theme names (handle different possible keys)\n",
    "    theme_names = assignment.get(\"themes\", \n",
    "                   assignment.get(\"labels\", \n",
    "                   assignment.get(\"categories\", [])))\n",
    "    \n",
    "    justification = assignment.get(\"justification\", \"\")\n",
    "    \n",
    "    if idx is not None:\n",
    "        print(f\"Text {idx}: {texts[idx][:60]}...\")\n",
    "        print(f\"  â†’ Themes: {', '.join(theme_names)}\")\n",
    "        if justification:\n",
    "            print(f\"  â†’ Why: {justification}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nDemonstrates **code condensation** - a key qualitative analysis technique:\n\n**What is code condensation:**\n- Start with many detailed codes (13 in this example)\n- Group them into fewer, broader themes (3-5)\n- Creates a hierarchical codebook structure\n\n**Why this matters:**\n- Mirrors iterative qualitative analysis workflow\n- Helps manage complexity in large datasets\n- Makes patterns more visible\n\n**How the prompt works:**\n1. Provides all initial codes as context\n2. Asks for 3-5 broader categories\n3. Requests mapping of initial â†’ condensed codes\n4. Wants definitions for each condensed theme\n\n**Temperature: 0.2 (very low)**\n- Code condensation should be consistent\n- We want logical groupings, not creative ones\n- Higher temperature might produce inconsistent hierarchies\n\n**How to use this:**\n- Start by coding 20-50 texts with detailed codes\n- Feed those codes to this condensation step\n- Iterate if the condensed themes don't feel right\n- Use human judgment to validate the groupings\n\n**Expected output:** 3-5 broader themes, each encompassing multiple initial codes, with clear definitions.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Code Condensation\n",
    "\n",
    "In iterative qualitative coding, you often start with many codes and then **condense** them into higher-level themes. Let's simulate this by:\n",
    "\n",
    "1. Creating an initial set of detailed codes\n",
    "2. Asking the LLM to group them into broader categories\n",
    "3. Producing a condensed codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate initial detailed codes from a first-pass analysis\n",
    "initial_codes = [\n",
    "    \"Rising rent concerns\",\n",
    "    \"Housing affordability crisis\",\n",
    "    \"Childcare costs\",\n",
    "    \"Utility bill increases\",\n",
    "    \"Bus unreliability\",\n",
    "    \"Bike lane infrastructure\",\n",
    "    \"Healthcare access delays\",\n",
    "    \"Public clinic wait times\",\n",
    "    \"Street lighting improvements\",\n",
    "    \"Neighborhood safety\",\n",
    "    \"Community garden engagement\",\n",
    "    \"Library social events\",\n",
    "    \"Local market connections\"\n",
    "]\n",
    "\n",
    "print(f\"Initial codes: {len(initial_codes)} detailed codes\\n\")\n",
    "for code in initial_codes:\n",
    "    print(f\"  â€¢ {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask LLM to condense codes into higher-level themes\n",
    "condensation_prompt = f\"\"\"You are a qualitative researcher performing code condensation.\n",
    "\n",
    "Given these {len(initial_codes)} initial codes from interview analysis:\n",
    "{json.dumps(initial_codes, indent=2)}\n",
    "\n",
    "Group them into 3-5 broader themes. For each theme:\n",
    "- Provide a clear theme name\n",
    "- List which initial codes it encompasses\n",
    "- Write a brief definition\n",
    "\n",
    "Return JSON with key 'condensed_themes' containing an array of theme objects.\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a qualitative researcher. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": condensation_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "condensed = json.loads(response.choices[0].message.content)\n",
    "print(\"âœ“ Code condensation complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display condensed themes\n",
    "print(\"=== CONDENSED THEMES ===\\n\")\n",
    "\n",
    "condensed_themes = condensed.get(\"condensed_themes\", [])\n",
    "for i, theme in enumerate(condensed_themes, 1):\n",
    "    name = theme.get(\"name\", \"Unnamed\")\n",
    "    definition = theme.get(\"definition\", \"\")\n",
    "    included_codes = theme.get(\"codes\", theme.get(\"initial_codes\", []))\n",
    "    \n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   Definition: {definition}\")\n",
    "    print(f\"   Encompasses: {', '.join(included_codes)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **reflexive coding** - having the LLM critique its own analysis:\n\n**What is reflexivity in qualitative research:**\n- Questioning your own interpretations\n- Acknowledging researcher positionality and bias\n- Considering alternative readings of data\n- Strengthening validity through self-critique\n\n**The two-step process:**\n1. **Initial coding:** LLM identifies themes (temperature 0.4)\n2. **Self-challenge:** LLM questions its own themes (temperature 0.5)\n\n**Why slightly higher temperature in step 2:**\n- Need creativity to generate alternative interpretations\n- Want to escape from initial framing\n- Still structured enough for analysis\n\n**What the challenge prompt asks:**\n- What biases might the LLM have made?\n- What alternative interpretations exist?\n- What evidence contradicts the themes?\n- What was overlooked?\n\n**Sociological value:**\n- Surfaces ambiguities in the data\n- Reveals multiple possible readings\n- Makes analysis more transparent and rigorous\n\n**How to use:**\n- Run on texts where interpretation is ambiguous\n- Use the challenges to refine your own thinking\n- Don't treat LLM challenges as \"truth\" - they're provocations\n- Combine with human reflexive memos\n\n**Expected output:** Critical reflections on initial coding, alternative themes, and a revised interpretation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Reflexive Coding (Self-Challenge)\n",
    "\n",
    "Good qualitative research involves **reflexivity**: questioning your own interpretations and biases. We can prompt the LLM to:\n",
    "\n",
    "1. Analyze text and produce initial codes\n",
    "2. Challenge its own coding decisions\n",
    "3. Consider alternative interpretations\n",
    "\n",
    "This helps surface ambiguities and encourages deeper analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **text chunking** for handling long documents (like interview transcripts):\n\n**Why chunking is necessary:**\n- LLMs have context windows (e.g., 128k tokens for GPT-4)\n- Long interviews may exceed this limit\n- Smaller chunks = more focused coding\n- Easier to manage API costs\n\n**The `chunk_text` function:**\n1. **Normalize input:** Handles both strings and lists\n2. **Clean whitespace:** Collapses multiple spaces to one\n3. **Split by words:** Chunks of ~200 words (adjustable)\n4. **Filter short chunks:** Minimum 40 characters to avoid tiny fragments\n\n**Parameters to adjust:**\n- **`max_words=200`:** Smaller = more granular coding, but more API calls\n  - Use 200-300 for detailed coding\n  - Use 500-1000 for broader themes\n- **`min_chars=40`:** Prevents tiny trailing chunks\n\n**Alternative chunking strategies:**\n- **Paragraph-based:** Split on `\\n\\n` (respects document structure)\n- **Sentence-based:** Use spaCy or NLTK sentence tokenizer\n- **Semantic:** Use embeddings to find natural break points\n- **Speaker turns:** For multi-party interviews\n\n**How to use:**\n- For 5,000-word interview: ~25 chunks at 200 words each\n- Each chunk gets coded separately (next cell)\n- Then aggregate themes across chunks (Part 5)\n\n**Expected output:** List of text segments, each small enough for focused LLM coding.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nApplies thematic coding to each chunk separately, then aggregates results:\n\n**The `code_chunks` function workflow:**\n1. Loop through each text chunk\n2. Send to LLM for theme identification (max 3 themes per chunk)\n3. Collect all themes in a flat list\n4. Print themes found in each chunk\n\n**Key parameter: `max_themes_per_chunk=3`**\n- Prevents theme explosion (too many themes)\n- Forces LLM to prioritize most salient themes\n- Adjust based on chunk size and content density\n\n**Why this approach:**\n- Each chunk gets focused attention\n- Avoids overwhelming the LLM with too much text\n- Natural for long interviews with multiple topics\n\n**Potential issues:**\n- **Duplicate themes across chunks:** \"affordability\" appears in chunks 1, 3, 5\n- **Inconsistent naming:** Chunk 1 says \"cost concerns\", chunk 3 says \"financial burden\"\n- Solution: Deduplication step (next cells)\n\n**Cost considerations:**\n- 25 chunks Ã— $0.0002 per call = ~$0.005 (very cheap)\n- Most cost is in prompt tokens (the chunk text)\n- For 100 interviews: budget ~$5-10\n\n**How to improve:**\n- Add few-shot examples in the prompt for consistency\n- Use lower temperature (0.1) for more uniform theme names\n- Include a preliminary codebook to guide naming\n\n**Expected output:** List of all themes across all chunks, with some duplication that needs resolution.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample interview excerpt for reflexive analysis\n",
    "interview_excerpt = \"\"\"I've lived in this neighborhood for fifteen years. \n",
    "When I first moved here, everyone knew each other. We'd have block parties, \n",
    "kids played outside together. Now? People keep to themselves. Everyone's \n",
    "always rushing somewhere. The new apartment buildings brought in a lot of \n",
    "young professionals who don't seem interested in community. But maybe that's \n",
    "just my generation talking. I don't know. Things change.\"\"\"\n",
    "\n",
    "print(\"Interview excerpt:\\n\")\n",
    "print(interview_excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initial coding\n",
    "initial_analysis_prompt = f\"\"\"Analyze this interview excerpt and identify 2-3 themes.\n",
    "\n",
    "Excerpt:\n",
    "{interview_excerpt}\n",
    "\n",
    "Return JSON with:\n",
    "- themes: array of theme names\n",
    "- interpretations: brief explanation of each theme\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a qualitative researcher. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": initial_analysis_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.4\n",
    ")\n",
    "\n",
    "initial_analysis = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"=== INITIAL ANALYSIS ===\\n\")\n",
    "for theme in initial_analysis.get(\"themes\", []):\n",
    "    print(f\"  â€¢ {theme}\")\n",
    "print()\n",
    "for interpretation in initial_analysis.get(\"interpretations\", []):\n",
    "    print(f\"  - {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nUses the LLM to **deduplicate and consolidate** similar themes:\n\n**Why LLMs are good at this:**\n- Can recognize semantic similarity (\"cost concerns\" â‰ˆ \"financial burden\")\n- Understand broader categories that encompass multiple themes\n- Generate clear definitions for consolidated themes\n\n**The deduplication prompt:**\n1. Shows all unique themes from chunk coding\n2. Asks LLM to merge similar/overlapping ones\n3. Requests 3-5 final distinct themes\n4. Wants mapping: which original themes â†’ which final theme\n\n**Temperature: 0.2 (low)**\n- Consolidation should be logical and consistent\n- Too high = creative but inconsistent groupings\n\n**How this differs from code condensation:**\n- **Code condensation:** Hierarchical grouping (initial â†’ broader)\n- **Deduplication:** Horizontal merging (similar themes â†’ single theme)\n\n**Alternative approaches:**\n- **Embedding similarity:** Use cosine similarity on theme embeddings to auto-cluster\n- **Manual review:** Export theme list and manually mark duplicates\n- **Hybrid:** LLM suggests merges, human approves\n\n**When to use:**\n- After coding 20+ chunks (enough variation to cause duplicates)\n- When you see obvious semantic overlap in theme names\n- Before final codebook creation\n\n**Expected output:** Clean, non-redundant final codebook with 3-5 themes, each with clear definition and list of merged original themes.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Challenge the initial coding\n",
    "challenge_prompt = f\"\"\"You previously identified these themes in an interview excerpt:\n",
    "{json.dumps(initial_analysis, indent=2)}\n",
    "\n",
    "Original excerpt:\n",
    "{interview_excerpt}\n",
    "\n",
    "Now, critically examine your own interpretation:\n",
    "1. What biases or assumptions might you have made?\n",
    "2. What alternative interpretations are possible?\n",
    "3. What evidence contradicts your themes?\n",
    "4. What did you overlook?\n",
    "\n",
    "Return JSON with:\n",
    "- challenges: array of critical reflections\n",
    "- alternative_themes: array of alternative theme names\n",
    "- revised_interpretation: your reconsidered analysis\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a reflexive qualitative researcher. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": challenge_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.5  # Slightly higher for creative challenges\n",
    ")\n",
    "\n",
    "reflexive_analysis = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n=== REFLEXIVE ANALYSIS ===\\n\")\n",
    "\n",
    "print(\"Critical reflections:\")\n",
    "for challenge in reflexive_analysis.get(\"challenges\", []):\n",
    "    print(f\"  â€¢ {challenge}\")\n",
    "\n",
    "print(\"\\nAlternative themes:\")\n",
    "for alt_theme in reflexive_analysis.get(\"alternative_themes\", []):\n",
    "    print(f\"  â€¢ {alt_theme}\")\n",
    "\n",
    "print(\"\\nRevised interpretation:\")\n",
    "print(f\"  {reflexive_analysis.get('revised_interpretation', '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nCreates an **interactive dialogue** where the LLM asks YOU questions about the analysis:\n\n**What is member checking/collaborative coding:**\n- Researcher presents analysis to participants\n- Participants validate or challenge interpretations\n- Iterative dialogue refines understanding\n\n**How this simulates that:**\n- LLM presents its themes\n- LLM asks open-ended questions about:\n  - Ambiguities in the text\n  - Missing context\n  - Alternative interpretations\n- Researcher answers (in practice, you'd feed answers back to LLM)\n\n**Temperature: 0.6 (moderate-high)**\n- Need creativity to formulate good questions\n- Want genuine probing, not just confirmation\n- Not coding task, so higher temp is fine\n\n**Practical workflow:**\n1. Run this cell to get LLM's questions\n2. Answer them based on your domain knowledge\n3. Feed answers back in a new prompt: \"Given these clarifications: [answers], revise your coding\"\n4. LLM produces refined themes\n\n**Why this matters:**\n- Makes LLM analysis more interactive\n- Surfaces what the LLM is uncertain about\n- Encourages researcher reflexivity\n- Combines LLM capabilities with human expertise\n\n**How to extend:**\n- Create a loop: Ask questions â†’ Get answers â†’ Revise â†’ Ask follow-ups\n- Log the dialogue for transparency in publications\n- Use for training human coders (shows what questions to ask)\n\n**Expected output:** 2-3 thoughtful questions that probe interpretation, not just factual gaps.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Handling Long Documents (Chunking)\n",
    "\n",
    "Interview transcripts can be very long. To handle them effectively:\n",
    "\n",
    "1. **Chunk** the text into manageable segments\n",
    "2. Code each segment separately\n",
    "3. Aggregate themes across all segments\n",
    "\n",
    "Here's a simple chunking strategy based on word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_words=200, min_chars=40):\n",
    "    \"\"\"\n",
    "    Split text into chunks of roughly max_words each.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text (string or list of strings)\n",
    "        max_words: Maximum words per chunk\n",
    "        min_chars: Minimum characters to keep a chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Normalize to string\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(str(t) for t in text)\n",
    "    else:\n",
    "        text = str(text)\n",
    "    \n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Split into words and chunk\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), max_words):\n",
    "        chunk = \" \".join(words[i:i+max_words]).strip()\n",
    "        if len(chunk) >= min_chars:\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test with a sample long text\n",
    "sample_long_text = \"\"\"Video games are a diverse medium. The audience who plays \n",
    "video games is similarly diverse. If you're trying to break into a specialist \n",
    "space, such as interactive fiction, there are very set definitions of what \n",
    "interactive fiction can be or can't be. Some people in the community believe \n",
    "interactive fiction is typing into a parser and nothing else. But there are \n",
    "people working to change that perception. Similar movements happen in other \n",
    "genres like roguelikes. Gaming is a big tent and we're seeing those voices \n",
    "being given more press as a testament to the maturity of our medium.\"\"\"\n",
    "\n",
    "chunks = chunk_text(sample_long_text, max_words=50)\n",
    "print(f\"Split text into {len(chunks)} chunks\\n\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"Chunk {i} ({len(chunk.split())} words):\")\n",
    "    print(f\"  {chunk[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's code each chunk separately\n",
    "def code_chunks(chunks, model=\"gpt-4o-mini\", max_themes_per_chunk=3):\n",
    "    \"\"\"\n",
    "    Apply thematic coding to each chunk and aggregate results.\n",
    "    \"\"\"\n",
    "    all_themes = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        prompt = f\"\"\"Identify up to {max_themes_per_chunk} themes in this text segment.\n",
    "        \n",
    "Segment:\n",
    "{chunk}\n",
    "\n",
    "Return JSON with:\n",
    "- themes: array of theme names\n",
    "- supporting_quotes: array of relevant quotes\n",
    "\"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a qualitative researcher. Return valid JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0.3\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        chunk_themes = result.get(\"themes\", [])\n",
    "        all_themes.extend(chunk_themes)\n",
    "        \n",
    "        print(f\"Chunk {i+1}: {chunk_themes}\")\n",
    "    \n",
    "    return all_themes\n",
    "\n",
    "print(\"=== CODING CHUNKS ===\\n\")\n",
    "all_themes = code_chunks(chunks)\n",
    "\n",
    "print(f\"\\nâœ“ Total themes identified across chunks: {len(all_themes)}\")\n",
    "print(f\"Unique themes: {len(set(all_themes))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Theme Aggregation and Deduplication\n",
    "\n",
    "When coding in chunks, you often get **duplicate or overlapping themes**. Let's aggregate and deduplicate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count theme frequency\n",
    "from collections import Counter\n",
    "\n",
    "theme_counts = Counter(all_themes)\n",
    "\n",
    "print(\"=== THEME FREQUENCY ===\\n\")\n",
    "for theme, count in theme_counts.most_common():\n",
    "    print(f\"  {theme}: {count}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LLM to deduplicate and merge similar themes\n",
    "dedup_prompt = f\"\"\"You identified these themes across multiple text segments:\n",
    "{json.dumps(list(theme_counts.keys()), indent=2)}\n",
    "\n",
    "Some themes may be duplicates or highly similar. Consolidate them into a \n",
    "final list of 3-5 distinct themes. For each:\n",
    "- Provide a clear theme name\n",
    "- List which original themes it merges\n",
    "- Give a 1-2 sentence definition\n",
    "\n",
    "Return JSON with key 'final_themes'.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a qualitative researcher. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": dedup_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "final_codebook = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"\\n=== FINAL CODEBOOK ===\\n\")\n",
    "for i, theme in enumerate(final_codebook.get(\"final_themes\", []), 1):\n",
    "    name = theme.get(\"name\", \"Unnamed\")\n",
    "    definition = theme.get(\"definition\", \"\")\n",
    "    merged = theme.get(\"merged_themes\", theme.get(\"merges\", []))\n",
    "    \n",
    "    print(f\"{i}. {name}\")\n",
    "    print(f\"   {definition}\")\n",
    "    if merged:\n",
    "        print(f\"   Merged from: {', '.join(merged)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Interactive Reflexive Dialogue\n",
    "\n",
    "One powerful feature of LLM-assisted coding is the ability to have a **dialogue** about the analysis. Let's create a function that allows the LLM to:\n",
    "\n",
    "1. Present its coding\n",
    "2. Ask for your opinion\n",
    "3. Incorporate your feedback\n",
    "\n",
    "This simulates collaborative coding or member checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dialogue where LLM asks for researcher input\n",
    "dialogue_prompt = f\"\"\"You've coded this interview excerpt:\n",
    "\n",
    "{interview_excerpt}\n",
    "\n",
    "Your themes: {initial_analysis.get('themes', [])}\n",
    "\n",
    "Now, formulate 2-3 questions to ask the researcher to validate or challenge \n",
    "your interpretation. These should be open-ended questions that:\n",
    "- Probe ambiguities in the text\n",
    "- Ask about context you might be missing\n",
    "- Invite alternative interpretations\n",
    "\n",
    "Return JSON with key 'questions'.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a reflexive qualitative researcher. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\", \"content\": dialogue_prompt}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},\n",
    "    temperature=0.6\n",
    ")\n",
    "\n",
    "dialogue = json.loads(response.choices[0].message.content)\n",
    "\n",
    "print(\"=== LLM QUESTIONS FOR RESEARCHER ===\\n\")\n",
    "for i, question in enumerate(dialogue.get(\"questions\", []), 1):\n",
    "    print(f\"{i}. {question}\\n\")\n",
    "\n",
    "print(\"\\nðŸ’¡ In practice, you would answer these questions and feed the responses\")\n",
    "print(\"   back to the LLM to refine the coding iteratively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**What this code does:**\n\nImplements **robust JSON extraction with retry logic** for when parsing fails:\n\n**The three-phase approach:**\n\n**Phase 1: Initial request**\n- Clear instructions: \"Return only a JSON object\"\n- Low temperature (0.1) for consistency\n- Specify exact schema in prompt\n\n**Phase 2: Parse attempt**\n- Try `json.loads()` on response\n- If successful â†’ return result\n- If fails â†’ proceed to Phase 3\n\n**Phase 3: Retry with correction**\n- Send original prompt + failed response + correction instruction\n- Use temperature 0.0 (most deterministic)\n- Try parsing again\n- If still fails â†’ raise exception for manual review\n\n**Key features:**\n- **`max_retries` parameter:** Control how many attempts (1 is usually enough)\n- **Error logging:** Print failed output for debugging\n- **Gradual temperature reduction:** 0.1 â†’ 0.0 increases determinism\n\n**Note about `get_labels_robust`:**\n- This function is defined here for demonstration purposes to show robust JSON extraction with retry logic\n- However, the actual batch annotation code in this notebook uses a simpler approach with JSON mode API\n- JSON mode API (shown earlier) is more reliable and doesn't require this retry logic\n- This function is useful when working with models that don't support JSON mode or for understanding error handling\n\n**When to use retry logic:**\n- Using Approach 1 or 2 (not JSON mode)\n- Critical annotations (can't skip failures)\n- Debugging schema issues\n\n**When NOT needed:**\n- Using JSON mode or function calling (already reliable)\n- Batch processing (skip failures, review later)\n\n**Success rates:**\n- Without retry: ~85% (prompt-only) to ~95% (few-shot)\n- With retry: ~98%\n- Remaining 2%: Usually schema issues or model limitations\n\n**Best practice:** Use JSON mode (Approach 3) to avoid needing this complexity."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
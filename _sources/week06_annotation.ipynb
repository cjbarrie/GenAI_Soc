{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Structured Text Annotation with LLMs\n\nThis notebook demonstrates how to reliably extract structured outputs from LLMs for text annotation tasks in social science research.\n\n## Learning Objectives\n\n- Master **prompt formatting** strategies (f-strings, templates, few-shot, chain-of-thought)\n- Understand **three approaches** to structured outputs (JSON)\n  1. **Prompt for JSON** (simplest, least reliable)\n  2. **JSON Mode API** (recommended default)\n  3. **Function Calling** (most structured, type-safe)\n- Implement **robust JSON extraction** with error handling and retries\n- Perform **batch annotation** with quality checks and logging\n- Apply **mixture of experts** (ensemble) approaches for increased reliability\n- Follow **replication best practices** (validation, logging, fingerprinting)\n\n## Setup\n\n### Running in Google Colab\n1. Upload this notebook to Google Colab\n2. Run the installation cell below\n3. You'll be prompted to enter your OpenAI API key\n\n### Running Locally\n1. Install requirements: `pip install openai pandas scikit-learn numpy`\n2. Set environment variable: `export OPENAI_API_KEY=\"your-key-here\"`\n3. Run notebook with Jupyter: `jupyter notebook week6_structured_annotation_colab.ipynb`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install -q \"openai>=1.40.0\" pandas scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "import re\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, confusion_matrix\n",
    "import getpass\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# For Colab: you'll be prompted to enter it\n",
    "# For local: set OPENAI_API_KEY environment variable\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API key: \")\n",
    "\n",
    "client = OpenAI()  # reads OPENAI_API_KEY from environment\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nSets up the complete environment for production-grade LLM annotation:\n\n**Key libraries:**\n- **`openai`**: API access to GPT models\n- **`pandas`**: Tabular data manipulation (annotation results)\n- **`scikit-learn`**: Validation metrics (Cohen's kappa, confusion matrix)\n- **`hashlib`**: Model fingerprinting to detect API drift\n- **`datetime`**: Timestamping for reproducibility\n\n**Why this setup is more comprehensive than previous notebooks:**\n- Includes validation metrics (Cohen's kappa)\n- Supports model fingerprinting (detect when API changes)\n- Designed for research-grade reproducibility\n\n**When to use each metric:**\n- **Cohen's kappa**: Inter-rater reliability (target: κ > 0.80)\n- **Confusion matrix**: Where disagreements occur\n- **Fingerprinting**: Detect if model behavior changes over time\n\n**Security reminder:** Use `getpass` for API keys, never hardcode them.\n\n**Expected output:** \"✓ Setup complete!\" - you're ready for structured annotation workflows.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Prompt Formatting Strategies\n",
    "\n",
    "Before we get structured outputs, let's review simple ways to format prompts for text annotation tasks.\n",
    "\n",
    "### 1A. Simple f-string Prompting\n",
    "\n",
    "The most basic approach: use Python f-strings to insert text into a prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nDemonstrates the **simplest** prompting approach using Python f-strings:\n\n**How f-strings work:**\n- `f\"string with {variable}\"` inserts variable values directly\n- Simple, readable, familiar to Python developers\n\n**Key parameter: temperature=0**\n- For annotation tasks, use temperature 0 (deterministic)\n- Same input = same output (critical for reproducibility)\n- Higher temperature (0.7+) is for creative tasks only\n\n**Limitations of this approach:**\n- No structured output (returns free text)\n- Hard to parse programmatically\n- Model might add extra explanation\n- Not suitable for batch processing\n\n**When to use:**\n- Quick prototyping\n- Interactive exploration\n- Single-shot annotations where you'll read the output\n\n**For production:** Use structured outputs (Approach 2 or 3 later in this notebook)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample political texts for annotation\n",
    "texts = [\n",
    "    \"We must invest in renewable energy now!\",\n",
    "    \"Cut taxes and reduce business regulations\",\n",
    "    \"Healthcare is a human right for all citizens\",\n",
    "    \"Maintain current spending levels and balanced budget\"\n",
    "]\n",
    "\n",
    "# Simple f-string approach\n",
    "text = texts[0]\n",
    "prompt = f\"\"\"Classify the political stance of this text as:\n",
    "- Progressive\n",
    "- Conservative\n",
    "- Centrist\n",
    "\n",
    "Text: {text}\n",
    "Stance:\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0  # Deterministic for annotation\n",
    ")\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Stance: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImproves on f-strings by using **reusable templates** with `.format()`:\n\n**Why templates are better:**\n- Define prompt once, reuse for all texts\n- Ensures consistency across annotations\n- Easy to modify prompt for entire batch\n- Supports batch processing naturally\n\n**How `.format()` works:**\n- Template contains `{variable_name}` placeholders\n- `.format(variable_name=value)` fills them in\n- More explicit than f-strings (clearer what's being inserted)\n\n**Best practices for template prompts:**\n1. Keep instructions consistent across all texts\n2. Clearly define categories (Progressive/Conservative/Centrist)\n3. Place variable at the end (reduces interference)\n4. Use imperative tone (\"Classify the...\") not questions\n\n**Still limited:**\n- Output is free text, not structured\n- Need parsing logic to extract stance\n- Model might be verbose or inconsistent\n\n**Next steps:** Add few-shot examples (next cell) or use JSON mode (Approach 3)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1B. Reusable Template with .format()\n",
    "\n",
    "For batch processing, create a template you can reuse across multiple texts."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImproves on f-strings by using **reusable templates** with `.format()`:\n\n**Why templates are better:**\n- Define prompt once, reuse for all texts\n- Ensures consistency across annotations\n- Easy to modify prompt for entire batch\n- Supports batch processing naturally\n\n**How `.format()` works:**\n- Template contains `{variable_name}` placeholders\n- `.format(variable_name=value)` fills them in\n- More explicit than f-strings (clearer what's being inserted)\n\n**Best practices for template prompts:**\n1. Keep instructions consistent across all texts\n2. Clearly define categories (Progressive/Conservative/Centrist)\n3. Place variable at the end (reduces interference)\n4. Use imperative tone (\"Classify the...\") not questions\n\n**Still limited:**\n- Output is free text, not structured\n- Need parsing logic to extract stance\n- Model might be verbose or inconsistent\n\n**Next steps:** Add few-shot examples or use JSON mode (Part 2)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nDemonstrates **few-shot prompting** - providing examples to guide the model:\n\n**What is few-shot learning:**\n- Show model 2-5 examples of input → output pairs\n- Model learns the pattern and applies it to new inputs\n- No fine-tuning required (examples in prompt only)\n\n**Why few-shot helps:**\n- **Clarifies format:** Shows exact output style you want\n- **Reduces ambiguity:** Examples demonstrate edge cases\n- **Improves consistency:** Model mimics example structure\n- **Domain adaptation:** Examples can include domain jargon\n\n**Structure of few-shot prompts:**\n1. Task description (brief)\n2. Examples (2-5 is usually enough)\n   - Show diverse cases (not all similar)\n   - Include format you want (here: \"Text → Label\")\n3. New input to classify\n\n**How many examples to use:**\n- **0-shot (zero-shot):** No examples (what we did before)\n- **Few-shot (2-5):** Most common, good balance\n- **Many-shot (10-100):** For complex tasks or narrow domains\n\n**When few-shot is essential:**\n- Narrow domain (not in training data)\n- Specific format required\n- Ambiguous category boundaries\n- Model is undershooting or overshooting\n\n**Cost consideration:** Examples add tokens → higher cost. But often worth it for quality.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template approach for consistency\n",
    "STANCE_TEMPLATE = \"\"\"Classify the political stance of this text as:\n",
    "- Progressive\n",
    "- Conservative\n",
    "- Centrist\n",
    "\n",
    "Text: {text}\n",
    "Stance:\"\"\"\n",
    "\n",
    "results = []\n",
    "\n",
    "for text in texts:\n",
    "    prompt = STANCE_TEMPLATE.format(text=text)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content.strip()\n",
    "    results.append({\"text\": text, \"stance\": result})\n",
    "    print(f\"• {result}: {text}\")\n",
    "\n",
    "print(f\"\\n✓ Annotated {len(results)} texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **chain-of-thought (CoT) prompting** - asking the model to reason step-by-step:\n\n**What is chain-of-thought:**\n- Ask model to show its reasoning before giving final answer\n- Originated in Wei et al. (2022) \"Chain-of-Thought Prompting Elicits Reasoning\"\n- Dramatically improves performance on reasoning tasks\n\n**Why CoT helps:**\n- **Better accuracy:** Especially on multi-step problems\n- **Transparency:** Can see model's logic\n- **Debugging:** Identify where reasoning goes wrong\n- **Trust:** Justifications build confidence in annotations\n\n**Temperature: 0.3 (slightly higher)**\n- CoT needs some creativity for explanations\n- But still low enough for consistency\n- Trade-off between rigid and random\n\n**When to use CoT:**\n- Complex classification (multiple factors to consider)\n- Need justifications for auditing\n- Debugging misclassifications\n- Training human annotators (shows reasoning process)\n\n**When NOT to use CoT:**\n- Simple tasks (adds unnecessary tokens/cost)\n- Speed critical (CoT is slower)\n- Don't need explanations\n\n**Cost consideration:** CoT generates more tokens (reasoning + answer), so ~2-3x more expensive than direct classification.\n\n**Alternative:** Use JSON mode (Approach 3) to get structured reasoning + classification",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1C. Few-Shot Prompting\n",
    "\n",
    "Provide **examples** in the prompt to guide the model's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-shot template with examples\n",
    "FEW_SHOT_TEMPLATE = \"\"\"Classify political stance as Progressive, Conservative, or Centrist.\n",
    "\n",
    "Examples:\n",
    "Text: \"Cut taxes and reduce regulations\" → Conservative\n",
    "Text: \"Expand healthcare access for all\" → Progressive\n",
    "Text: \"Maintain current spending levels\" → Centrist\n",
    "\n",
    "Text: {text} →\"\"\"\n",
    "\n",
    "text = \"Protect traditional family values and limit government overreach\"\n",
    "prompt = FEW_SHOT_TEMPLATE.format(text=text)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Predicted: {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Key advantages over Approach 1 (prompt-only):**\n- **Guaranteed valid JSON:** No \"Here's the JSON:\" or markdown fences\n- **No retry logic needed:** Works first time\n- **Same cost:** No extra charge\n- **Simpler code:** No complex parsing\n\n**Requirements:**\n- Model must support JSON mode (GPT-4, GPT-3.5-turbo, GPT-4o do)\n- Must mention \"JSON\" in prompt (system or user message)\n\n**When to use:**\n- **This is the recommended default approach**\n- Most annotation tasks (sentiment, topics, stance, etc.)\n- When you want flexible schema (any JSON structure)\n\n**Limitation:**\n- No type validation (can't enforce \"confidence must be 0-1\")\n- For strict typing, use Approach 3",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D. Chain-of-Thought Prompting\n",
    "\n",
    "Ask the model to **explain its reasoning** step-by-step before giving an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\n**Approach 3 (JSON Mode)** - The **recommended approach** for most annotation tasks:\n\n**How it works:**\n- Set `response_format={\"type\": \"json_object\"}` in API call\n- API **guarantees** valid JSON output\n- Model cannot return prose or malformed JSON\n\n**Key advantages:**\n- **99.9%+ reliability:** Valid JSON guaranteed\n- **No examples needed:** Saves tokens/cost\n- **Simple to use:** Just one parameter\n- **Fast:** No retry logic needed\n\n**Requirements:**\n1. Must mention \"JSON\" in prompt (system or user message)\n2. Model must support it (GPT-4, GPT-3.5-turbo, GPT-4o, etc.)\n3. Temperature can be anything (0 for consistency)\n\n**How to structure the request:**\n```python\nmessages=[\n    {\"role\": \"system\", \"content\": \"You are X. Return valid JSON only.\"},\n    {\"role\": \"user\", \"content\": \"Task: ... Return JSON with keys: ...\"}\n]\nresponse_format={\"type\": \"json_object\"}\n```\n\n**Compared to other approaches:**\n- **vs Prompt-only:** Much more reliable (99% vs 70%)\n- **vs Few-shot:** Simpler and cheaper (no examples needed)\n- **vs Function calling:** More flexible schema (next cell)\n\n**When to use JSON mode:**\n- Most annotation tasks (sentiment, topics, stance, etc.)\n- When you want flexibility in schema\n- When you don't need enum validation\n\n**When to use Function calling instead:**\n- Need strict type checking (enums, number ranges)\n- Complex nested schemas\n- Want IDE autocomplete on schema\n\n**Cost:** Same as normal API calls - no extra charge for JSON mode.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "COT_TEMPLATE = \"\"\"Classify the stance and explain your reasoning.\n\nText: {text}\n\nThink step-by-step:\n1. What policy domain is this?\n2. What values does it express?\n3. What stance does this suggest?\n\nReasoning:\nStance:\"\"\"\n\ntext = \"Invest heavily in public education and teacher salaries\"\nprompt = COT_TEMPLATE.format(text=text)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": prompt}],\n    temperature=0.3  # Slightly higher for reasoning\n)\n\nprint(f\"Text: {text}\\n\")\nprint(f\"Chain-of-Thought Response:\\n{response.choices[0].message.content}\")\nprint(\"\\n✓ CoT provides reasoning steps before final classification\")"
  },
  {
   "cell_type": "markdown",
   "source": "**Key advantages over Approach 2 (JSON mode):**\n- **Type validation:** `\"type\": \"number\"` ensures numeric values  \n- **Enum constraints:** `\"enum\": [\"A\", \"B\", \"C\"]` restricts to specific values\n- **Required fields:** `\"required\": [...]` ensures all fields present\n- **Nested structures:** Complex object hierarchies with validation\n\n**The schema format:**\n```python\n{\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"analyze_stance\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"stance\": {\"type\": \"string\", \"enum\": [\"Progressive\", \"Conservative\", \"Centrist\"]},\n                \"confidence\": {\"type\": \"number\"},  # Must be a number\n                \"reasoning\": {\"type\": \"string\"}\n            },\n            \"required\": [\"stance\", \"confidence\", \"reasoning\"]\n        }\n    }\n}\n```\n\n**When to use:**\n- Production systems needing validation guarantees\n- Strict category sets (must be one of 3 options, not free text)\n- Type safety important (confidence must be number, not string)\n- Complex nested schemas\n\n**vs JSON mode:**\n- **JSON mode:** Flexible, simpler, good default\n- **Function calling:** Stricter, more setup, better for production\n\n**Cost:** Same as JSON mode - no extra charge",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Part 2: Three Approaches to Getting Structured Outputs (JSON)\n\nWhen you want the LLM to return structured data (like JSON with specific keys), you have **three main approaches**. Each has different reliability and complexity.\n\n**Why structured outputs matter:**\n- Easy to parse programmatically (no regex needed)\n- Can extract multiple fields (label, confidence, reasoning)\n- Essential for batch annotation pipelines\n- Enables downstream analysis\n\nLet's compare the three approaches from simplest to most reliable."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Approach 1: Prompt for JSON (Simplest, Least Reliable)\n\n**How it works:** Just ask the LLM to return JSON in your prompt. No special API parameters.\n\n**What you do:**\n- Include \"Return JSON\" in your prompt\n- Hope the model follows instructions\n- Try to parse the response with `json.loads()`\n\n**Success rate:** ~60-80% (varies by model and complexity)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Approach 2: JSON Mode API Parameter (Recommended for Most Tasks)\n\n**How it works:** Use `response_format={\"type\": \"json_object\"}` in your API call. The API **guarantees** valid JSON.\n\n**What you do:**\n1. Add `response_format={\"type\": \"json_object\"}` parameter\n2. Mention \"JSON\" somewhere in your prompt\n3. Parse the response - it will always be valid JSON\n\n**Success rate:** ~99.9% (valid JSON guaranteed by API)"
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **robust JSON extraction with retry logic** for when parsing fails:\n\n**The three-phase approach:**\n\n**Phase 1: Initial request**\n- Clear instructions: \"Return only a JSON object\"\n- Low temperature (0.1) for consistency\n- Specify exact schema in prompt\n\n**Phase 2: Parse attempt**\n- Try `json.loads()` on response\n- If successful → return result\n- If fails → proceed to Phase 3\n\n**Phase 3: Retry with correction**\n- Send original prompt + failed response + correction instruction\n- Use temperature 0.0 (most deterministic)\n- Try parsing again\n- If still fails → raise exception for manual review\n\n**Key features:**\n- **`max_retries` parameter:** Control how many attempts (1 is usually enough)\n- **Error logging:** Print failed output for debugging\n- **Gradual temperature reduction:** 0.1 → 0.0 increases determinism\n\n**When to use retry logic:**\n- Using Approach 1 or 2 (not JSON mode)\n- Critical annotations (can't skip failures)\n- Debugging schema issues\n\n**When NOT needed:**\n- Using JSON mode or function calling (already reliable)\n- Batch processing (skip failures, review later)\n\n**Success rates:**\n- Without retry: ~85% (prompt-only) to ~95% (few-shot)\n- With retry: ~98%\n- Remaining 2%: Usually schema issues or model limitations\n\n**Best practice:** Use JSON mode (Approach 3) to avoid needing this complexity.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Cut taxes and reduce regulations\"\n",
    "prompt = \"\"\"You output ONLY valid JSON with keys: stance, confidence, reasoning.\n",
    "\n",
    "Example:\n",
    "Input: Expand healthcare access for all\n",
    "Output: {{\"stance\":\"Progressive\",\"confidence\":0.9,\"reasoning\":\"Universal healthcare is progressive policy\"}}\n",
    "\n",
    "Example:\n",
    "Input: Maintain current spending levels\n",
    "Output: {{\"stance\":\"Centrist\",\"confidence\":0.8,\"reasoning\":\"Status quo signals moderate position\"}}\n",
    "\n",
    "Now do the same:\n",
    "Input: {input}\n",
    "Output:\n",
    "\"\"\".format(input=text)\n",
    "\n",
    "output = llm_call(prompt)\n",
    "print(\"Raw output:\")\n",
    "print(output)\n",
    "\n",
    "print(\"\\nParsing JSON...\")\n",
    "data = json.loads(output)\n",
    "print(\"✓ Successfully parsed!\")\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Provider JSON Mode (More Reliable)\n",
    "\n",
    "Use the API's **JSON mode** to **force** valid JSON output. This is the recommended approach for most annotation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Why this approach often fails:**\n\nCommon problems:\n1. **Extra text:** \"Here's the JSON: {...}\"\n2. **Markdown fences:** \\`\\`\\`json {...} \\`\\`\\`\n3. **Prose instead:** \"The stance is Progressive because...\"\n4. **Malformed JSON:** Trailing commas, unquoted keys\n\n**When to use:**\n- Quick prototyping only\n- Not recommended for production\n\n**Better alternatives:** Approach 2 (JSON mode API parameter) or Approach 3 (function calling)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **production-grade batch annotation** with comprehensive logging and error handling:\n\n**The `annotate_text` function:**\n- Single-text annotation with JSON mode\n- Returns parsed dictionary\n- Extended schema: stance, confidence, reasoning, **policy_domain**\n\n**The `batch_annotate` function workflow:**\n1. Loop through all texts with progress indicator\n2. Try to annotate each text\n3. On success: Add metadata (model, timestamp, success=True)\n4. On failure: Log error, mark success=False, set fields to None\n5. Return pandas DataFrame for analysis\n\n**Key metadata fields:**\n- **`model`:** Track which model annotated (for comparison)\n- **`timestamp`:** When annotation happened (ISO format)\n- **`success`:** Boolean flag for filtering\n- **`error`:** Error message if failed\n\n**Why comprehensive logging matters:**\n- **Reproducibility:** Can trace back to exact API call\n- **Debugging:** Identify systematic failures\n- **Cost tracking:** Know how many API calls made\n- **Validation:** Compare annotations across time\n\n**Quality summary:**\n- Print success rate at end\n- Identify low-confidence annotations\n- Flag failures for manual review\n\n**How to use:**\n```python\ndf = batch_annotate(texts, model=\"gpt-4o-mini\")\n# Filter successful\nsuccessful = df[df['success']]\n# Check low confidence\nreview = df[df['confidence'] < 0.7]\n```\n\n**Best practices:**\n- Always wrap API calls in try/except\n- Log everything (model, time, prompt, response)\n- Return structured DataFrames (easier analysis)\n- Calculate success rate and quality metrics",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Protect traditional family values\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"You are a political analyst. Return valid JSON only.\"},\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": f\"Analyze political stance: {text}\\n\\nReturn JSON with stance, confidence, reasoning.\"}\n",
    "    ],\n",
    "    response_format={\"type\": \"json_object\"},  # Force JSON mode\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "data = json.loads(response.choices[0].message.content)\n",
    "print(\"✓ JSON mode guarantees valid JSON\")\n",
    "print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function schema with typed arguments\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"analyze_stance\",\n",
    "        \"description\": \"Return structured political stance analysis\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"stance\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"Progressive\", \"Conservative\", \"Centrist\"]\n",
    "                },\n",
    "                \"confidence\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"Confidence score from 0 to 1\"\n",
    "                },\n",
    "                \"reasoning\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Brief explanation of the classification\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"stance\", \"confidence\", \"reasoning\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "text = \"Expand social safety nets\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Analyze: {text}\"}],\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Extract structured arguments\n",
    "call = response.choices[0].message.tool_calls[0]\n",
    "args = json.loads(call.function.arguments)\n",
    "\n",
    "print(\"✓ Function calling provides strongest guarantees\")\n",
    "print(f\"Function called: {call.function.name}\")\n",
    "print(json.dumps(args, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **mixture of experts (MoE)** - using multiple models and aggregating predictions:\n\n**Theoretical foundation:**\n- Kraft et al. (2024): \"Mixture-of-Experts Approach to LLM-Based Political Ideology Scaling\"\n- Ensemble methods reduce variance and bias\n- Multiple perspectives increase reliability\n\n**The approach:**\n1. Get ideological score from each model (-1 to +1)\n2. Aggregate using mean, median, std\n3. Use std as uncertainty measure\n\n**The `get_stance_score` function:**\n- Asks for numeric score (not categorical)\n- -1 = most progressive\n- +1 = most conservative\n- 0 = centrist\n- Returns float for aggregation\n\n**The `ensemble_stance` function:**\n- Calls multiple models\n- Handles failures gracefully (skip failed models)\n- Computes aggregate statistics\n- Returns uncertainty metrics\n\n**Key metrics:**\n- **Mean:** Central tendency\n- **Median:** Robust to outliers\n- **Std:** Agreement/uncertainty\n  - Low std (<0.3) = high agreement\n  - High std (>0.6) = low agreement = review needed\n\n**When to use MoE:**\n- High-stakes decisions (publication-critical)\n- Ambiguous/contested texts\n- Want confidence intervals\n- Have budget for multiple API calls\n\n**Cost consideration:**\n- N models × cost per call\n- 3 models = 3× cost (but usually worth it for quality)\n\n**Alternative aggregation:**\n- Majority vote (for categorical)\n- Weighted average (weight by model quality)\n- Bayesian model combination",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Comparison Summary"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "comparison_df = pd.DataFrame({\n    \"Approach\": [\n        \"1. Prompt for JSON\",\n        \"2. JSON Mode (API)\",\n        \"3. Function Calling\"\n    ],\n    \"Reliability\": [\n        \"60-80%\",\n        \"99.9%\",\n        \"99.9%\"\n    ],\n    \"Type Safety\": [\n        \"None\",\n        \"None\",\n        \"Full\"\n    ],\n    \"Complexity\": [\n        \"Lowest\",\n        \"Low\",\n        \"Medium\"\n    ],\n    \"When to Use\": [\n        \"Prototyping only\",\n        \"Most tasks (default)\",\n        \"Production systems\"\n    ]\n})\n\nprint(comparison_df.to_string(index=False))\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"\\n💡 **Recommendation for novices:**\")\nprint(\"   Start with Approach 2 (JSON Mode) - reliable and simple\")\nprint(\"\\n💡 **For production systems:**\")\nprint(\"   Use Approach 3 (Function Calling) if you need type validation\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Robust JSON Extraction\n",
    "\n",
    "Sometimes JSON parsing fails. Here's how to handle errors gracefully with **retry logic**."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **comprehensive logging** for full reproducibility of LLM annotations:\n\n**Why complete logging is essential:**\n- **Reproducibility:** Others can replicate your exact setup\n- **Debugging:** Trace errors back to source\n- **Auditing:** See exactly what model was asked\n- **Cost tracking:** Monitor token usage\n- **Drift detection:** Compare results over time\n\n**What to log:**\n1. **Timestamp:** When annotation happened (ISO 8601 format)\n2. **Input:** Original text\n3. **Model:** Exact version (e.g., \"gpt-4-0613\", not just \"gpt-4\")\n4. **Parameters:** Temperature, seed, top_p, etc.\n5. **Prompt:** Exact prompt sent (system + user)\n6. **Response:** Complete model output\n7. **Usage:** Token counts (prompt, completion, total)\n8. **Metadata:** Finish reason, API version\n\n**The `seed` parameter:**\n- Available in some OpenAI models (GPT-4, GPT-4o)\n- Makes sampling deterministic\n- Same (model + prompt + seed + temp) = same output\n- Critical for reproducibility\n\n**Best practices:**\n- Pin model version: Use \"gpt-4-0613\" not \"gpt-4\"\n- Always log seed and temperature\n- Store logs as JSONL (one JSON object per line)\n- Include git commit hash if code changes\n- Log API response headers (rate limits, etc.)\n\n**How to use logs:**\n- Debug failures by inspecting prompt\n- Calculate cost from token counts\n- Verify reproducibility by re-running with same params\n- Track model drift by comparing same prompts over time\n\n**Storage recommendation:** Save to file, not just print",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear instructions for JSON-only output\n",
    "INSTRUCTIONS = (\n",
    "    'Return only a JSON object like this:\\n'\n",
    "    '{\"stance\":\"Progressive|Conservative|Centrist|null\",'\n",
    "    '\"confidence\":0-1,\"reasoning\":\"brief\"}\\n'\n",
    "    'Do not add any extra text.'\n",
    ")\n",
    "\n",
    "def get_labels_robust(text, model=\"gpt-4o-mini\", max_retries=1):\n",
    "    \"\"\"\n",
    "    Robust JSON extraction with error handling and retry logic.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to annotate\n",
    "        model: Model name\n",
    "        max_retries: Number of retry attempts on parse failure\n",
    "    \n",
    "    Returns:\n",
    "        dict: Parsed JSON result\n",
    "    \"\"\"\n",
    "    # 1) Initial request with low temperature\n",
    "    prompt = f'{INSTRUCTIONS}\\n\\nText: \"{text}\"'\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.1  # Low temp for consistency\n",
    "    )\n",
    "    \n",
    "    output = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 2) Try to parse as JSON\n",
    "    try:\n",
    "        return json.loads(output)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"⚠ Parse failed: {e}\")\n",
    "        print(f\"Raw output: {output[:100]}...\\n\")\n",
    "        \n",
    "        if max_retries > 0:\n",
    "            # 3) Retry with correction prompt\n",
    "            fix_prompt = (\n",
    "                \"That was not valid JSON. Please send ONLY the JSON object, \"\n",
    "                \"nothing else. No explanations, no markdown fences.\"\n",
    "            )\n",
    "            \n",
    "            retry_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                    {\"role\": \"assistant\", \"content\": output},\n",
    "                    {\"role\": \"user\", \"content\": fix_prompt}\n",
    "                ],\n",
    "                temperature=0.0  # Zero temp for retry\n",
    "            )\n",
    "            \n",
    "            retry_output = retry_response.choices[0].message.content.strip()\n",
    "            print(f\"Retry output: {retry_output[:100]}...\\n\")\n",
    "            \n",
    "            try:\n",
    "                return json.loads(retry_output)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"✗ Parse failed after retry\")\n",
    "                raise\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Test cases\n",
    "test_texts = [\n",
    "    \"We must expand Medicare to cover everyone\",\n",
    "    \"Cut taxes and reduce government spending\",\n",
    "    \"Maintain balanced approach to fiscal policy\"\n",
    "]\n",
    "\n",
    "print(\"Testing robust extraction with retry logic:\\n\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"Test {i}: {text}\")\n",
    "    try:\n",
    "        result = get_labels_robust(text)\n",
    "        print(f\"✓ Success: {result['stance']} (confidence: {result['confidence']})\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nImplements **model fingerprinting** to detect when API behavior changes (API drift):\n\n**What is API drift:**\n- Model providers update models without warning\n- \"gpt-4\" might point to different weights next month\n- Causes non-reproducibility issues\n- Your results today ≠ results tomorrow\n\n**How fingerprinting works:**\n1. Create a small test set (5-10 prompts)\n2. Run them through model with fixed seed and temperature\n3. Hash the concatenated responses (SHA256)\n4. Save this fingerprint\n5. Periodically re-run and compare hashes\n\n**The `model_fingerprint` function:**\n- Takes model name and test prompts\n- Uses temperature=0 and seed for determinism\n- Concatenates all responses\n- Computes SHA256 hash (deterministic)\n- Returns 64-character hex string\n\n**When fingerprint changes:**\n- Model has been updated (weights changed)\n- API behavior changed\n- Your previous results may not be replicable\n- Need to re-run annotations or document drift\n\n**Best practices:**\n1. **Create fingerprint at start:** Before annotating corpus\n2. **Check periodically:** Weekly or monthly\n3. **Store with results:** Save fingerprint with annotations\n4. **Document changes:** Note when drift detected\n5. **Use versioned models:** \"gpt-4-0613\" vs \"gpt-4\"\n\n**How often does drift happen:**\n- Unversioned models (\"gpt-4\"): Monthly\n- Versioned models (\"gpt-4-0613\"): Rarely (usually stable)\n- Open-source models: Never (fixed weights)\n\n**Recommendation:** Always use versioned models for research",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Batch Annotation\n",
    "\n",
    "Annotating multiple texts efficiently with **logging** and **quality checks**."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nMeasures **agreement between human and LLM annotations** using standard metrics:\n\n**Why validation is critical:**\n- LLMs can be systematically biased\n- Need to know if LLM matches human judgment\n- Required for publication in most venues\n- Establishes annotation quality\n\n**Cohen's Kappa (κ):**\n- Measures inter-rater reliability (agreement corrected for chance)\n- Range: -1 to +1 (usually 0 to 1)\n- **Interpretation:**\n  - κ > 0.80: Substantial agreement (excellent)\n  - κ > 0.60: Moderate agreement (acceptable)\n  - κ < 0.60: Questionable (needs work)\n- Formula accounts for chance agreement\n\n**Accuracy:**\n- Simple: % of matching labels\n- Doesn't account for chance\n- Can be misleading with imbalanced classes\n\n**Confusion Matrix:**\n- Shows where disagreements occur\n- Rows = human labels\n- Cols = LLM labels\n- Diagonal = agreements\n- Off-diagonal = disagreements\n\n**How to use in practice:**\n1. **Validation set:** Human-annotate 100-200 texts\n2. **LLM annotation:** Annotate same texts with LLM\n3. **Calculate κ:** Use `cohen_kappa_score()`\n4. **Analyze errors:** Check confusion matrix\n5. **Iterate:** If κ < 0.80, refine prompt and repeat\n\n**Target thresholds for publication:**\n- Minimum: κ > 0.70\n- Good: κ > 0.80\n- Excellent: κ > 0.90\n\n**Cost saving:** Once validated, can annotate full corpus with LLM",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus for batch annotation\n",
    "corpus = [\n",
    "    \"We need stronger borders and immigration control\",\n",
    "    \"Healthcare is a human right for all\",\n",
    "    \"Balance the budget through moderate tax reform\",\n",
    "    \"Invest in renewable energy infrastructure\",\n",
    "    \"Cut regulations on small businesses\",\n",
    "    \"Expand access to affordable childcare\",\n",
    "    \"Maintain current defense spending levels\",\n",
    "    \"Protect voting rights and access\",\n",
    "    \"Reduce corporate tax rates\",\n",
    "    \"Fund public education and teacher salaries\"\n",
    "]\n",
    "\n",
    "print(f\"Corpus: {len(corpus)} political statements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**What this code does:**\n\nCreates a **promptbook** - a comprehensive documentation artifact for reproducible LLM research:\n\n**What is a promptbook:**\n- Single JSON file documenting entire annotation pipeline\n- Analogous to lab notebook in experimental research\n- Enables exact replication by other researchers\n- Required by some journals (e.g., Nature family)\n\n**What to include:**\n1. **Task description:** What you're annotating\n2. **Model details:** Exact version, provider, parameters\n3. **Prompts:** Full system and user messages\n4. **Output schema:** Structure of responses\n5. **Validation:** Human agreement metrics (Cohen's κ)\n6. **Fingerprint:** Hash for detecting drift\n7. **Metadata:** Date, version, notes\n\n**Why this matters for reproducibility:**\n- Someone with your promptbook can replicate exactly\n- Shows transparency about model and prompts\n- Documents validation against human labels\n- Tracks when model behavior changes (fingerprint)\n\n**Best practices:**\n1. **Version control:** Increment version when prompts change\n2. **Git integration:** Include git commit hash\n3. **Store with data:** Save alongside annotations\n4. **Share openly:** Publish with paper (supplementary materials)\n5. **Update regularly:** New fingerprint each month\n\n**What to do with promptbook:**\n- Include in paper's methods section\n- Upload to OSF/Dataverse with data\n- Reference in computational appendix\n- Use for internal documentation\n\n**Publication requirements:**\n- Many journals now require computational reproducibility\n- Promptbook satisfies most requirements\n- Some journals have templates (adapt this structure)\n\n**This establishes research-grade annotation practices**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_text(text, model=\"gpt-4o-mini\", temperature=0):\n",
    "    \"\"\"Annotate a single text with structured output\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\",\n",
    "             \"content\": \"You are a political analyst. Return valid JSON only.\"},\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": f\"\"\"Analyze this political text: {text}\n",
    "\n",
    "Return JSON with keys:\n",
    "- stance (Progressive/Conservative/Centrist)\n",
    "- confidence (0-1)\n",
    "- reasoning (brief explanation)\n",
    "- policy_domain (e.g., healthcare, economy, education)\"\"\"}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return json.loads(response.choices[0].message.content)\n",
    "\n",
    "def batch_annotate(texts, model=\"gpt-4o-mini\", temperature=0):\n",
    "    \"\"\"Annotate multiple texts with error handling and logging\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"Annotating {len(texts)} texts with {model}...\\n\")\n",
    "    \n",
    "    for i, text in enumerate(texts, 1):\n",
    "        print(f\"[{i}/{len(texts)}] {text[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            annotation = annotate_text(text, model=model, temperature=temperature)\n",
    "            annotation['text'] = text\n",
    "            annotation['model'] = model\n",
    "            annotation['timestamp'] = datetime.now().isoformat()\n",
    "            annotation['success'] = True\n",
    "            annotation['error'] = None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "            annotation = {\n",
    "                'text': text,\n",
    "                'model': model,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'stance': None,\n",
    "                'confidence': None,\n",
    "                'reasoning': None,\n",
    "                'policy_domain': None\n",
    "            }\n",
    "        \n",
    "        results.append(annotation)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    print(f\"\\n✓ Completed: {df['success'].sum()}/{len(df)} successful\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run batch annotation\n",
    "df = batch_annotate(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n=== ANNOTATION RESULTS ===\\n\")\n",
    "\n",
    "print(\"Summary by stance:\")\n",
    "print(df['stance'].value_counts())\n",
    "\n",
    "print(\"\\nAverage confidence by stance:\")\n",
    "print(df.groupby('stance')['confidence'].mean().round(3))\n",
    "\n",
    "print(\"\\nSample annotations:\")\n",
    "display_df = df[['text', 'stance', 'confidence', 'policy_domain']].head(5)\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality checks\n",
    "print(\"\\n=== QUALITY CHECKS ===\\n\")\n",
    "\n",
    "# Check for low confidence predictions\n",
    "low_confidence = df[df['confidence'] < 0.7]\n",
    "print(f\"Low confidence annotations (< 0.7): {len(low_confidence)}\")\n",
    "if len(low_confidence) > 0:\n",
    "    print(low_confidence[['text', 'stance', 'confidence']].to_string(index=False))\n",
    "\n",
    "# Check for failures\n",
    "failures = df[~df['success']]\n",
    "print(f\"\\nFailed annotations: {len(failures)}\")\n",
    "if len(failures) > 0:\n",
    "    print(failures[['text', 'error']].to_string(index=False))\n",
    "\n",
    "print(\"\\n✓ Quality checks complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Mixture of Experts (Ensemble)\n",
    "\n",
    "Using **multiple models** and aggregating their predictions can increase reliability. This is based on Kraft et al. (2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stance_score(text, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Get ideological position score from a model.\n",
    "    Returns: float from -1 (most progressive) to +1 (most conservative)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Rate this text on ideology from -1 (most progressive)\n",
    "to +1 (most conservative). Return only the number.\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return float(response.choices[0].message.content.strip())\n",
    "\n",
    "def ensemble_stance(text, models):\n",
    "    \"\"\"Aggregate stance estimates across multiple models\"\"\"\n",
    "    scores = []\n",
    "    individual = {}\n",
    "    \n",
    "    for model in models:\n",
    "        try:\n",
    "            score = get_stance_score(text, model=model)\n",
    "            scores.append(score)\n",
    "            individual[model] = score\n",
    "            print(f\"  {model:20}: {score:+.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {model:20}: Error - {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not scores:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"mean\": np.mean(scores),\n",
    "        \"median\": np.median(scores),\n",
    "        \"std\": np.std(scores),\n",
    "        \"min\": np.min(scores),\n",
    "        \"max\": np.max(scores),\n",
    "        \"individual\": individual,\n",
    "        \"n_models\": len(scores)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Using a continuous scale instead of categorical labels:**\n\nFor more nuanced analysis, we can ask models to rate ideology on a **continuous scale** rather than discrete categories. This approach:\n\n- Captures gradations between positions (e.g., -0.45 vs -0.90 both progressive, but different intensities)\n- Enables more sophisticated aggregation (mean, median, standard deviation)\n- Allows measurement of **uncertainty** via ensemble variance\n- Follows Kraft et al. (2024) methodology for ideological scaling\n\n**The scale:**\n- **-1**: Most progressive position\n- **0**: Centrist/neutral\n- **+1**: Most conservative position\n\nThis continuous representation is particularly useful for mixture-of-experts approaches where we aggregate scores across multiple models.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single text with multiple models\n",
    "models = [\"gpt-4o-mini\", \"gpt-3.5-turbo\"]\n",
    "\n",
    "text = \"We must protect traditional family values and limit government overreach\"\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Individual model scores:\")\n",
    "\n",
    "result = ensemble_stance(text, models)\n",
    "\n",
    "if result:\n",
    "    print(f\"\\nEnsemble results:\")\n",
    "    print(f\"  Mean:      {result['mean']:+.3f}\")\n",
    "    print(f\"  Median:    {result['median']:+.3f}\")\n",
    "    print(f\"  Std dev:   {result['std']:.3f}\")\n",
    "    print(f\"  Range:     [{result['min']:+.3f}, {result['max']:+.3f}]\")\n",
    "    \n",
    "    agreement = \"High\" if result['std'] < 0.3 else \"Medium\" if result['std'] < 0.6 else \"Low\"\n",
    "    print(f\"  Agreement: {agreement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch analysis with ensemble\n",
    "sample_texts = [\n",
    "    \"Expand Medicare to cover everyone\",\n",
    "    \"Cut taxes and regulations on businesses\",\n",
    "    \"Protect voting rights and access\",\n",
    "    \"Secure the border and enforce immigration laws\",\n",
    "    \"Invest in public schools and teacher salaries\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(f\"Analyzing {len(sample_texts)} texts with ensemble...\\n\")\n",
    "\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"[{i}/{len(sample_texts)}] {text}\")\n",
    "    ensemble = ensemble_stance(text, models)\n",
    "    \n",
    "    if ensemble:\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"position\": ensemble[\"mean\"],\n",
    "            \"uncertainty\": ensemble[\"std\"],\n",
    "            \"n_models\": ensemble[\"n_models\"]\n",
    "        })\n",
    "    print()\n",
    "\n",
    "ensemble_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\nPosition scores (negative = progressive, positive = conservative):\")\n",
    "print(ensemble_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Validation and Replication\n",
    "\n",
    "Best practices for **reproducible** and **validated** LLM annotation.\n",
    "\n",
    "### 6A. Comprehensive Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_with_logging(text, model=\"gpt-4-0613\", temperature=0, seed=42):\n",
    "    \"\"\"Annotate text with complete logging for reproducibility\"\"\"\n",
    "    prompt = f\"\"\"Analyze political stance: {text}\n",
    "\n",
    "Return JSON with stance, confidence, reasoning.\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a political analyst. Return valid JSON only.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=temperature,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Parse result\n",
    "    result = json.loads(response.choices[0].message.content)\n",
    "    \n",
    "    # Create comprehensive log entry\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"text\": text,\n",
    "        \"model\": model,\n",
    "        \"temperature\": temperature,\n",
    "        \"seed\": seed,\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": result,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "            \"completion_tokens\": response.usage.completion_tokens,\n",
    "            \"total_tokens\": response.usage.total_tokens\n",
    "        },\n",
    "        \"finish_reason\": response.choices[0].finish_reason\n",
    "    }\n",
    "    \n",
    "    return result, log_entry\n",
    "\n",
    "text = \"Expand social safety nets and increase minimum wage\"\n",
    "result, log = annotate_with_logging(text)\n",
    "\n",
    "print(f\"✓ Annotated: {text}\")\n",
    "print(f\"  Stance: {result.get('stance')}\")\n",
    "print(f\"\\nLog entry includes: {list(log.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6B. Model Fingerprinting (Detect API Drift)\n",
    "\n",
    "Create a **fingerprint** of model behavior to detect when the API changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fingerprint(model, test_prompts, temperature=0, seed=42):\n",
    "    \"\"\"\n",
    "    Create fingerprint to detect if model behavior has changed.\n",
    "    Returns: SHA256 hash of concatenated responses\n",
    "    \"\"\"\n",
    "    responses = []\n",
    "    \n",
    "    for prompt in test_prompts:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            seed=seed\n",
    "        )\n",
    "        responses.append(response.choices[0].message.content)\n",
    "    \n",
    "    # Hash concatenated responses\n",
    "    fingerprint = hashlib.sha256(\n",
    "        \"\".join(responses).encode()\n",
    "    ).hexdigest()\n",
    "    \n",
    "    return fingerprint\n",
    "\n",
    "# Create test set for fingerprinting\n",
    "test_prompts = [\n",
    "    \"Classify: 'Cut taxes for businesses' - Progressive/Conservative/Centrist\",\n",
    "    \"Classify: 'Expand healthcare coverage' - Progressive/Conservative/Centrist\",\n",
    "    \"Classify: 'Balanced budget amendment' - Progressive/Conservative/Centrist\"\n",
    "]\n",
    "\n",
    "fingerprint = model_fingerprint(\"gpt-4o-mini\", test_prompts)\n",
    "print(f\"Model fingerprint: {fingerprint[:16]}...\")\n",
    "print(\"\\n✓ Save this fingerprint and check periodically for drift\")\n",
    "print(\"✓ If fingerprint changes, model behavior has changed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6C. Validation with Human Labels\n",
    "\n",
    "Measure agreement between LLM and human annotations using **Cohen's kappa**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate human and LLM labels for validation\n",
    "# 0 = Progressive, 1 = Centrist, 2 = Conservative\n",
    "human_labels = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0])\n",
    "llm_labels = np.array([0, 1, 1, 0, 1, 2, 0, 2, 2, 0])\n",
    "\n",
    "# Calculate agreement metrics\n",
    "kappa = cohen_kappa_score(human_labels, llm_labels)\n",
    "accuracy = accuracy_score(human_labels, llm_labels)\n",
    "\n",
    "print(\"Human-LLM Agreement:\\n\")\n",
    "print(f\"Cohen's κ: {kappa:.3f}\")\n",
    "print(f\"Accuracy:  {accuracy:.3f}\")\n",
    "\n",
    "if kappa > 0.80:\n",
    "    print(\"✓ Substantial agreement\")\n",
    "elif kappa > 0.60:\n",
    "    print(\"⚠ Moderate agreement - consider refinement\")\n",
    "else:\n",
    "    print(\"✗ Low agreement - significant issues\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(human_labels, llm_labels)\n",
    "print(\"\\nConfusion Matrix (rows=human, cols=LLM):\")\n",
    "print(\"              Prog  Cent  Cons\")\n",
    "for i, label in enumerate([\"Progressive\", \"Centrist\", \"Conservative\"]):\n",
    "    print(f\"{label:12}  {cm[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6D. Promptbook Documentation\n",
    "\n",
    "A **promptbook** documents your entire annotation pipeline for replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptbook = {\n",
    "    \"task\": \"political_stance_classification\",\n",
    "    \"date_created\": datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "    \"version\": \"1.0\",\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"name\": \"gpt-4o-mini\",\n",
    "            \"type\": \"api\",\n",
    "            \"provider\": \"openai\",\n",
    "            \"temperature\": 0,\n",
    "            \"seed\": 42,\n",
    "            \"response_format\": \"json\"\n",
    "        }\n",
    "    ],\n",
    "    \"prompt_template\": \"Analyze political stance: {text}\\n\\nReturn JSON with stance, confidence, reasoning.\",\n",
    "    \"output_schema\": {\n",
    "        \"stance\": [\"Progressive\", \"Conservative\", \"Centrist\"],\n",
    "        \"confidence\": \"float (0-1)\",\n",
    "        \"reasoning\": \"string\"\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"method\": \"human_comparison\",\n",
    "        \"sample_size\": 200,\n",
    "        \"cohen_kappa\": 0.78,\n",
    "        \"accuracy\": 0.82,\n",
    "        \"validation_date\": datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    },\n",
    "    \"fingerprint\": fingerprint,\n",
    "    \"notes\": \"Validated on US political tweets. Low confidence (<0.7) texts manually reviewed.\"\n",
    "}\n",
    "\n",
    "print(\"✓ Promptbook created\")\n",
    "print(\"\\nPromptbook includes:\")\n",
    "for key in promptbook.keys():\n",
    "    print(f\"  • {key}\")\n",
    "\n",
    "print(\"\\nPromptbook (excerpt):\")\n",
    "print(json.dumps(promptbook, indent=2)[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook demonstrated key techniques for reliable, reproducible LLM annotation:\n",
    "\n",
    "### 1. Prompt Formatting\n",
    "- **f-strings**: Simple variable insertion\n",
    "- **Templates**: Reusable format strings with `.format()`\n",
    "- **Few-shot**: Provide examples to guide behavior\n",
    "- **Chain-of-thought**: Ask for step-by-step reasoning\n",
    "\n",
    "### 2. Structured Outputs (Four Approaches)\n",
    "- **Prompt-only**: Ask for JSON (least reliable)\n",
    "- **Few-shot with schema**: Show examples (better)\n",
    "- **JSON mode**: Force valid JSON with API parameter (recommended)\n",
    "- **Function calling**: Typed schemas (most structured)\n",
    "\n",
    "### 3. Robust Extraction\n",
    "- Error handling with try/except\n",
    "- Retry logic for parse failures\n",
    "- Low temperature for consistency\n",
    "\n",
    "### 4. Batch Annotation\n",
    "- Efficient processing of multiple texts\n",
    "- Logging timestamps and metadata\n",
    "- Quality checks (low confidence, failures)\n",
    "\n",
    "### 5. Mixture of Experts\n",
    "- Aggregate predictions from multiple models\n",
    "- Measure uncertainty (standard deviation)\n",
    "- Identify high/low agreement cases\n",
    "\n",
    "### 6. Validation & Replication\n",
    "- **Comprehensive logging**: All parameters and outputs\n",
    "- **Model fingerprinting**: Detect API drift\n",
    "- **Human validation**: Cohen's κ > 0.80 target\n",
    "- **Promptbook**: Document entire pipeline\n",
    "\n",
    "## Best Practices Checklist\n",
    "\n",
    "- ☐ Pin model versions (use specific snapshots like `gpt-4-0613`)\n",
    "- ☐ Set temperature to 0 for deterministic outputs\n",
    "- ☐ Use seed parameter when supported\n",
    "- ☐ Log everything (prompts, responses, settings, timestamps)\n",
    "- ☐ Create promptbook for documentation\n",
    "- ☐ Validate against human labels (Cohen's κ > 0.80)\n",
    "- ☐ Test-retest reliability (check consistency over time)\n",
    "- ☐ Model fingerprinting (detect API drift)\n",
    "- ☐ Share code and configs for replication\n",
    "- ☐ Consider open models for perfect reproducibility\n",
    "\n",
    "## Recommended Workflow\n",
    "\n",
    "1. **Start simple**: JSON mode zero-shot on validation sample\n",
    "2. **If needed**: Fine-tune open model (LoRA) with 100-1000 labels\n",
    "3. **Add replication harness**: Fixed params, logs, regression tests\n",
    "4. **Report validation**: Human-LLM κ, test-retest, promptbook\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- Kraft et al. (2024): \"Mixture of Experts for Ideological Scaling\"\n",
    "- Alizadeh et al. (2024): \"Open-Source LLMs for Text Classification\"\n",
    "- Heseltine & Clemm von Hohenberg (2024): \"GPT-4 Accuracy on Political Texts\"\n",
    "- Ziems et al. (2024): \"Can Large Language Models Transform Computational Social Science?\"\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. **Compare approaches**: Annotate the same 20 texts with all 4 structured output approaches. Which is most reliable?\n",
    "2. **Measure test-retest**: Run the same annotation twice with identical settings. Calculate Cohen's κ between runs.\n",
    "3. **Build ensemble**: Use 3+ models and compare ensemble vs. individual model performance.\n",
    "4. **Create promptbook**: Document a complete annotation pipeline for your research domain.\n",
    "5. **Validate**: Annotate 100 texts yourself, then with LLM. Calculate agreement metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}